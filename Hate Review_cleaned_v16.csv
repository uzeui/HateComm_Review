id,repeat check,reference,Title of the paper,Journal,Researcher location,Summary of the Paper,reaffirm,citations,Publication date,Fill-in date,Accessibility,Filter dataset,Construct definition,Sub-category,Sub-category_1_TEXT,availability of data,format of dataset,format of dataset_3_TEXT,how to access,verification of the way to access,reference of dataset,Number of datasets in use,Number of new datasets,Data source,topical focus,Language,Time span of data collection,data production,Country/ region span,Events,"Data collection procedure
",anonymity of data,anonymity data_1_TEXT,Annotated type,annotation schema,schema,Sampling available,sampling strategy,annotation number,train,test,Annotators,guideline1,Guidelines,Incentives 1,Incentives 2,General or specific,Reported,Targeted or non-targeted,Conceptualization,neutral,Identity-directed,Identity-directed_1_TEXT,Identity-directed_summary,Affiliation-directed,Affiliation-directed_1_TEXT,Person-directed,Person-directed_1_TEXT,about or to a person,counter speech,Content producer
1,No,"(Aldera et al., 2021)",Exploratory Data Analysis and Classification of a New Arabic Online Extremism Dataset,IEEE Access,Saudi Arabia,"In this paper, the authors present a dataset compiled for this purpose and discuss the classification methods that can be used for extremism detection. The manually annotated Arabic Twitter dataset consists of 89,816 tweets published between 2011 and 2021. Using guidelines, three expert annotators labelled the tweets as extremist or non-extremist. Exploratory data analysis was performed to understand the dataset’s features. Classification algorithms were used with the dataset, including logistic regression, support vector machine, multinominal naïve Bayes, random forest, and BERT. Among the traditional machine learning models, support vector machine with term frequency-inverse document frequency features achieved the highest accuracy (0.9729). However, BERT outperformed the traditional models with an accuracy of 0.9749. The dataset is publicly available at https://dx.doi.org/ 10.21227/g9c0-1t21. ",Yes,1,03.12.2021,02.09.2022,Yes,Yes,"Extremism in social media (Extremism, Non-extremism); no definition regarding extremism was given but can be deduced from the annotation guidelines",No,,Yes,Website,,https://ieee-dataport.org/documents/annotated-arabic-extremism-tweets,Yes,Arabic Twitter dataset for online extremism detection,1,1,Twitter,Extremism,Arabic,N/A,XX.05.2011 - XX.03.2021,N/A,N/A,"Twitter Streaming API and Search API;
 Used query 'Data [] = Search {Search_Term, longitude, latitude, lang}', which returned the text of tweets, along with user information (e.g. username, user location, number of friends, number of followers, number of likes, and user description.
Arabic search terms are provided but cannot be copied and pasted here",No,,manual,Yes,"The author oped for manual annotation with three experts by checking each tweets and considering the occurrence and meaning of the word and the context of the tweet. Annotation guideline is available in the paper as below:
Annotation Guidelines. After reading the tweet, please classify if as 'extreme' or 'not-extreme' using the following table focusing on the context of its text:
Extremist (T):
Classify a tweet as extremist if it has one of the following traits:
1. Concept of burglary 2. Intellectual unilateralism 3. The 'grievance' fallacy 4. The Islamophobia fallacy 5. Dismantling and building chaos 
The most impportant 'ideas' and 'sayings' of extremist discourse are:
1. Atonement for others who are different 2. Advocating and inciting violence 3. Reliance on references to extremist groups 4. The logic of fatal divisions 5. The idea of a single model

Non-extremist (F)
Classify a tweet as non-extremist if it does not contain the features mentioned above even if it contains religious, political, ethnic, or social themes or if its topic is unrelated to extremism (e.g. sports or fashion).
",Not discussed,,"89,816",N/A,N/A,Expert annotators,Yes,"The guideline provides criteria to classify the tweet as extremists, which are: 1. Concept of burglary 2. Intellectual unilateralism 3. The 'grievance' fallacy 4. The Islamophobia fallacy 5. Dismantling and building chaos",Not discussed,,Specific,"religion, political, other",Non-targeted,N/A,Yes,Yes,"Extremism, Religion, Political","Religion, Political, Other",No,,No,,,No,Human
2,No,"(Relia et al., 2019)","Race, Ethnicity and National Origin-Based Discrimination in Social Media and Hate Crimes across 100 U.S. Cities",In Proceedings of the International AAAI Conference on Web and Social Media,USA,"The paper studies malicious online content of hate speech, specifically related to race, ethnicity and national-origin based discrimination in social media, alongside hate crimes motivated by those characteristics in 100 cities across the United States. The paper developed a spatially-diverse dataset and classification pipeline to study targeted and self-narration of discrimination on social media. The paper investigates the association between the proportion of targeted discrimination and the number of hate crimes. Also, the authors study linguistic features of discrimination Tweets in relation to hate crimes by city. ",Yes,47,06.07.2019,02.09.2022,Yes,Yes,"“Race, ethnicity or national-origin based discrimination (hereafter referred to as “discrimination”) is a type of hate speech that systemically and unfairly assigns value based on race, ethnicity, or national-origin and affects the daily realities of many communities” (Relia et al., p. 417)",Yes,"Race, ethnicity or national-origin based discrimination ",No,,,,,Training data,3,1,Twitter,"Race, ethnicity and national-origin based discrimination",English,N/A,01.01.2011 - 31.12.2016,USA,N/A,Twitter Streaming API; Chose 100 cities of the US using the 'place' attribute of the Twitter,No,,manual,Yes,"The paper used crowdsourcing service 'Figure Eight' to label the training data. They provided the criteria for labelling the tweet as 'discrimination'. It was 'Tweet against a person, property, or society which is motivated, in whole or in part, by bias against race, ethnicity or national origin'. They performed initial trial experiment to confirm the clarity of instruction. The authors also indicated to annotators regarding the characteristic of the task that involves discrimination. Each tweets were labelled by at least two independent annotators. The authors also ensured annotators' accuracy to be minimum 80% according to five test tasks.
The label result was chosen based on the confidence of each labels. The confidence score was calculated based on the level of agreement between multiple contributors, which was weighted by trust scores of Figure Eight. ",No,,17000,17000,,Figure Eight workers,Yes,"“Tweet against a person, property, or society which is motivated, in whole or in part, by bias against race, ethnicity or national origin to workers for annotation.” (Relia et al., p. 420)",Yes,It is not mentioned but I assumed Figure Eight works via financial incentives,Specific,"race, nationality",Targeted,"race, nationality",Yes,Yes,"Race, Ethnicity, National origin","Race, Nationality",No,,No,,,No,Human
4,No,"(Nangia et al., 2020)",CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,arXiv preprint,USA,"To measure some forms of social bias in language models against protected demographic groups in the US, the authors introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. They find that all three of the widelyused MLMs they evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. ",Yes,98,30.09.2020,05.09.2022,Yes,Yes,"""We introduce Crowdsourced Stereotype Pairs
(CrowS-Pairs), a challenge set for measuring the
degree to which nine types of social bias are
present in language models. … CrowS-Pairs covers a broad-coverage set of
nine bias types: race, gender/gender identity, sexual orientation, religion, age, nationality, disability,
physical appearance, and socioeconomic status.""",Yes,"“race/color, gender/gender identity or expression, socioeconomic status/occupation, nationality, religion, age, sexual orientation, physical appearance, and disability” ([Nangia et al., 2020, p. 3)",Yes,Github,,"Link not presented in the paper, but it was easily discoverable by searching 'Crows-Pairs' in the search engine: 'https://github.com/nyu-mll/crows-pairs/",Yes,CrowS-Pairs,1,1,Written by MTurk workers,Social Bias,English,N/A,N/A,Workers were required to be in the United States,Not mentioned,"The authors collected and validated the data using Amazon Mechanical Turk (MTurk). The crowd-sourced workers were asked to write two minimally distant sentences. One sentence about a disadvantaged group that exhibits a clear stereotype or violates a stereotype about a group. Then, the workers were asked to copy the first sentence and make minimal edits by replacing target group to a contrasting advantaged group. Then they were asked to label their written sentences as stereotypical or anti-stereotypical. Lastly, the workers had to label the example with the best fitting bias category, out of nine pre-defined categories of bias: race/-color, gender/gender identity or expression, socioeconomic status/occupation, nationality, religion, age, sexual orientation, physical appearance, and disability",Not discussed,,manual,Not discussed,,Not discussed,,1508,N/A,N/A,The workers were required to be in the US,Not discussed,,Yes,The authors rewarded a $1 bonus to workers for each set of 4 examples about 4 different bias types to encourage crowdworkers to write sentences about a diverse set of bias types.,Specific,"race, gender, class, nationality, religion, age, sexuality, body, disability",Targeted,"race, gender, sexuality, religion, age, nationality, disability, body, class",Yes,Yes,"Race, Gender, Class, Nationality, Religion, Age, Sexuality, Body, Disability","race, gender, class, nationality, religion, age, sexuality, body, disability",No,,No,,,Yes,Human
5,No,"(Iqbal, Chun and Keshtkar, 2020)",Using Computational Linguistics to Extract Semantic Patterns from Trolling Data,2020 IEEE 14th International Conference on Semantic Computing (ICSC),USA,"The goal of this study is to use state-of-the-art word embedding, computational linguistics, and semantic sentiment extraction techniques to find patterns for the trolling contents. To do so, the authors applied word embedding to explore patterns in the tweet context. They also apply part of speech extraction and analysis, n-grams and word cloud analysis from different tweet categories. Finally, they apply the SentiStrength approach to explore the sentiment rooted in the semantics of the tweets. Their dataset contains 34,000 tweets. The data are categorized as LeftTroll and RightTroll. They applied different feature extraction techniques to explore the context of trolls with respect to left and right trolls and findings results are promising.",Yes,1,05.02.2020 ,05.09.2022,Yes,Yes,Trolling,No,,No,,,N/A,No,Dataset consists of LeftTroll and RightTroll,1,1,Twitter,Trolling,English,N/A,N/A,N/A,Not mentioned,N/A,Not discussed,,N/A,Not discussed,,Not discussed,,34818,N/A,N/A,Not discussed,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Non-human
6,No,"(Weld et al., 2021)",CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection,ACL-IJCNLP 2021,Australia,"The paper presents a new dataset for in-game toxic language detection for joint intent classificaiton and slot filling analysis. “The dataset consists of 45K utterances from 12K conversations from the chat logs of 1.9K completed Dota 2 matches. The paper proposes a framework for dual semantic-level toxicity, which handles utterance and token-level patterns, alongside contextual chatting history. ",Yes,4,11.06.2021,05.09.2022,Yes,Yes,Toxicity in online game,Yes,"Explicit toxicity, implicit toxicity",Yes,Github,,https://github.com/usydnlp/CONDA,Yes,CONDA,1,1,"Kaggle, conversation from the game Dota 2",Toxicity,English,N/A,N/A,N/A,Not discussed,"The authors obtained Defense of the Ancients 2 (Dota 2) data dump from Kaggle. The raw data is compiled from game matches including players, duration, match outcomes, and complete chat logs. Then they annotated the datain dual-level, one in token-level and the other Utterance-level.",Not discussed,,"automated, manual",Yes,"For token-level slot annotation, they created six labels for automated slot labelling: T (Toxicity), C (Character), D (Dotaspecific), S (game Slang), P (Pronoun) and O (Other). Then they performed lexicon-based automation by exact matching each lower-cased token against the lexicons.
For utterance-level intent annotation, they first performed test run with six human annotators, two non-game players and four game players. They used four categories: E (Explicit toxicity), I (Implicit toxicity), A (Action) and O (Other)

Annotators were given an instruction for labelling explicit or implicit toxicity. For instance, for explicit toxicity: “Typically contains toxic word(s). The intent is to insult or humiliate others, or to make others want to leave the conversation or quit the game.” ([Weld et al., 2021, p. 4)",Not discussed,,"44,869 utterances and 1,921 matches",,,"Four game players and two non-game players for initial test, then only game players ",Yes,"The annotators were given list of aspects to consider to determine which one of four categories each instance falls into. For Explicit toxicity, list of aspects were provided to consider to determine whether it is a case of explicit toxicity.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
7,No,"(Han & Tsvetkov, 2020)",Fortifying Toxic Speech Detectors Against Veiled Toxicity,arXiv preprint,USA,"In this work, they propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. They augment the toxic speech detector’s training data with
these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",Yes,40,07.09.2020,09.01.2023,Yes,Yes,"Veiled toxicity. ""It can be innocuous on the surface lexicon-level, but as offensive as hate speech and lastingly harmful (Sue et al., 2007; Sue, 2010; Nadal et al., 2014; Jurgens et al., 2019).""",No,,Yes,Github,,https://github.com/ xhan77/veiled-toxicity-detection,Yes,Toxicity training dataset,2,1,Reddit,Toxicity,English,N/A,N/A,N/A,N/A,"They first extract the veiled toxicity set. They randomly sample 10K general reddits from no specific domains and measure their average PerspectiveAPItoxicityscoretoxgeneral ≈0.17onascale [0,1]. They then measure the Perspective API toxicity scores of the posts in SBIC that are offensive to at least one minority group. The extracted veiled offences are equally non-toxic as some random general-domain reddits according to Perspective API. There are about 3K resulting posts.",No,,manual,Yes,"They consider three attributes in SBIC posts: offensive, targeting some marginalized groups, and subtly ex-pressed. Each post’s offensiveness score can be 0 (harmless), 0.5 (maybe offensive), or 1 (offensive). They select the posts with average offensiveness  0.5 (i.e., more than half of the annotators thought it was offensive). SBIC also asks annotators to identify the potential groups of people that might be offended by the post. They keep the posts with at least one identified target group.",Yes,"Random sampling. They randomly sample 10K general reddits from no specific domains and measure their average PerspectiveAPItoxicityscoretoxgeneral ≈0.17onascale [0,1].",3,2,1,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
8,No,"(Srivastava et al., 2020) ",A multi-dimensional view of aggression when voicing opinion.,"In Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","India, USA","The paper analyzes the pattern of aggression and figurative language use when voicing opinion. They present a Hindi-English code-mixed dataset of opinion based on the event of '2016 India banknote demonetisation'. The paper present multi-dimensional annotation alongside the dataset, including aggression, hate speech, emotion arousal and figurative language usage such as sarcasm, irony, metaphors, similes, puns, and word-play.",Yes,1,16.05.2020,16.09.2022,Yes,Yes,"""Stance, nuances of displayed aggression towards supporters / detractors of the opinion as well as the usage of various forms of figurative language such as metaphors, rhetorical questions, sarcasm, irony, puns and word-play.” [Srivastava et al., p. 13]",Yes,,Yes,Github,,https://github.com/arjitsrivastava/MultidimensionalViewOpinionMining,Yes,"nothing specific (corpuss, dataset, Hindi-English code-mixed tweets)",1,1,Twitter,"Aggression, Hate Speech, Sarcasm, Irony, Rhetorical questions, Puns, Word-play, Metaphors, Similes",Hindi-English,N/A,N/A,N/A,2016 India banknote demonetisation,"Sampling from pre-existing dataset from Swami et al. (2018b), which used Twitter Scraper API filtering by the keywords 'notebandi' and demonetisation'",Not discussed,,manual,Yes,"After sampling 1001 tweets from 3500 tweets from Swami et al. (2018), authors annotated sampled tweets for the dimensions. Dimensions are: Aggression (overt vs. covert vs. neutral), Hate speech (true vs. false), Sarcasm/Irony/Rhetorical question (True vs. False), Metaphor/Simile (True vs . False), Pun / Word-play (True vs. False), Emotional Arousal (5 point ordinal scale). ",Yes,random sampling,1001,N/A,N/A,Three domain expert annotators for each dimensions,Yes,They followed the guidelines by Kumar et al. (2018) for annotating Hindi-English code-mixed data.,Not discussed,,Specific,"race, gender, sexuality, political",Non-targeted,N/A,Yes,Yes,,"race, gender, sexuality, political",No,,No,,,No,Human
9,No,"(Al-Khalifa, Aljarah, & Abushariah, 2020)",Hate Speech Classification in Arabic Tweets,Journal of Theoretical and Applied Information Technology,Jordan,"This paper aims to present the work for detecting hate speech over the Twitter platform as one of the main Online Social Networks (OSN) based on the Arabic language. A dataset of 3000 tweets is collected in this work, which was experimented with using Random Forest (RF), Naive Bayes (NB), and Support Vector Machine (SVM) as classification algorithms. In addition, feature extraction is conducted using Bag of Word (BoW) and the Term Frequency–Inverse Document Frequency (TF-IDF). Based on the experimental results, SVM maintained consistently high performance and outperformed other classifiers, and TF-IDF outperformed BoW, which consequently achieved the highest accuracy.",Yes,3,06.15.2020,09.01.2022,Yes,Yes,"Hate speech. ""It refers to the use of hateful content against individuals and groups with an intention of bringing harm and raise violence toward them."" 'Our research deals with the detection of hate speech in Arabic tweets in general, against race, gender, nation, and religion.'",No,,No,,,,,Dataset,1,1,Twitter,Hate Speech,Arabic,14.12.2018 - 26.12.2018,N/A,N/A,Current Arabs political events and Middle East current events جمال_ خاشقجيالعراقالفساد_ السياسينمقاطعةا,"They used the Twitter Application Programming Interface (API) to collect tweets, which enabled them to retrieve a stream of tweets of over 45,000 tweets, within the time duration. Only 3,000 of them have been sampled to be annotated and pre-processed.",No,,manual,Yes,"After the collected data set is sampled to 3,000 tweets, each tweet must be labeled to either “hate” class or “not hate” class, where the hate class is given label 1 and the not hate (clean) class is given label 0.",Yes,Random sampling,3,2.1,900,"They carefully selected three annotators for tweets annotation who possess fair experience in using social media. The selected annotators are Arabic native speakers with different educational backgrounds, gender, and age.",Not discussed,,Not discussed,,General,N/A,Targeted,"race, gender, nationality, religion",Yes,No,,,No,,No,,,No,Human
12,No,"(Guberman, Schmitz, and Hemphil, 2016)",First Steps in Quantifying Toxicity and Verbal Violence on Twitter,"CSCW '16 COMPANION, FEBRUARY 27–MARCH 2, 2016, SAN FRANCISCO, CA, USA",USA,"The authors are developing a scale of online aggression that can be applied to Twitter posts (tweets) and that is based on existing measures of trait aggression and cyberbullying. For the purpose of testing and validating the scale, they are relying on Mechanical Turk, an Amazon Web Service, through which they can enlist and pay workers to code the dataset of tweets. Preliminary results suggest that aggression in tweets is difficult for human coders to identify and that they have not reached consensus about what constitutes harassment online. The authors discuss their preliminary results and propose next steps such as scale modification and automated classifier development.",Yes,32,02.03.2016,20.09.2022,Yes,Yes,Online harassment,Yes,,No,,,,,nothing specific,1,1,Twitter,Online Harassment,English,N/A,N/A,N/A,#GamerGate,They used TwitterGoggles and them randomly sampled from it. ,Not discussed,,manual,Yes,"First, they implemented the scale on a random sample of tweets collected with TwitterGoggles. After a single coder coded the same batch of 200 tweets twice, the authors edited some scale items. 5-point Likert scales varied from 1(Very Uncharacteristic) to 5 (Very Characteristic). 
The authors validated the scale by using human intelligence tasks on Amazon's Mechanical Turk. They also used MTurk's qualification for inter-rater reliability. They performed the second round of human intelligence tasks.",Yes,random sampling,N/A,,,MTurk workers,Yes,"“Participant Instructions for Tweet Coding HITs"" describes how annotators should evaluate each tweets based on 14 item scale based on 3-point Likert style scale. They guide how to deal with link included in tweets.",Yes,Coders earned $1.00 for each completed HIT. ,Specific,other,Non-targeted,N/A,No,No,,,Yes,"Not specified, but affiliation with GamerGate might be the majority harassment in this context.",No,,,No,Human
14,No,"(Curry, Abercrombie, and Rieser, 2021)","ConvAbuse: Data, analysis, and benchmarks for nuanced abuse detection in conversational AI.",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,UK,"The authors present the first English corpus study on abusive language towards three conversational AI systems gathered ‘in the wild’: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, they take a more ‘nuanced’ approach where their ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. They find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, they report results from bench-marking existing models against this data. Unsurprisingly, they find that there is substantial room for improvement with F1 scores below 90%.",Yes,6,20.09.2021,21.09.2022,Yes,Yes,"They used keywords to obtain abusive language using a list of profanities (265 regular expressions from a blacklist obtained from Amazon) and Hatebase. For annotation, they use labels like general, sexist, sexual harassment, homophobic, racist, transphobic, ableist, or intellectual.",Yes,,Yes,Github,,https://github.com/amandacurry/convabuse,Yes,ConvAbuse,1,1,Interaction with three conversational agents (Alana/ CarbonBot/ ELIZA),Abusive Language,English,"Alana v2 (April 2017-November 2018), CarbonBot (01. 10. 2019-07.12.2020), ELIZA (19. 12. 2002 - 26. 11. 2007)",N/A,N/A,N/A,"They collected data from conversations between users and three different conversational AI systems: Alana v2, CarbonBot, ELIZA.",No,,manual,Yes,"Authors created a hierarchical labelling scheme. At top level, they used Poletto et al. (2019)'s unbalanced rating scale. Then they used Waseem et al. (2017)'s two-dimensional typology of abuse. Finally, for fine-grained information about the target of abuse, annotators then label the instances as either general, sexist, sexual harassment, homophobic, racist, transphobic, ableist, or intellectual.",Yes,They used two sets of keywords: one profanities list from Amazon and the other from Hatebase. Then they used stratified sampling to extract utterances at random from six stratas of the dataset. ,6837,4785,1025,"The authors recruited eight gender studies students in their early 20s. Six of them identify as female, and two as non-binary. All are L1 English speakers, predominantly from the United Kingdom, except for one from the United States. One identifies as Asian, the remaining seven as white” (Curry et al., 2021, p. 3)",Yes,"Hierarchical labelling scheme based on Poletto et al. (2019), Waseem et al. (2017) ",Not discussed,,General,N/A,Targeted,"gender, sexuality, race, other",Yes,Yes,,,No,,No,,,No,Human
15,No,"(Chatzakou et al., 2017)","Measuring #GamerGate: A Tale of Hate, Sexism, and Bullying. ",Proceedings of the 26th International Conference on World Wide Web Companion - WWW ’17 Companion,"Greece, UK","In this paper, the authors study the Gamergate controversy. Started in August 2014 in the online gaming world, it quickly spread across various social networking platforms, ultimately leading to many incidents of cyberbullying and cyberaggression. They focus on Twitter, presenting a measurement study of a dataset of 340k unique users and 1.6M tweets to study the properties of these users, the content they post, and how they differ from random Twitter users. They find that users involved in this ""Twitter war"" tend to have more friends and followers, are generally more engaged and post tweets with negative sentiment, less joy, and more hate than random users. They also perform preliminary measurements on how the Twitter suspension mechanism deals with such abusive behaviors. ",Yes,88,07.04.2017,23.09.2022,Yes,Yes,"“Cyberbullying, the digital manifestation of bullying and aggressiveness in online social interactions” (Chatzakou et al., 2017, p. 1285)",Yes,,No,,,,,Nothing specific,2,2,Twitter,"Cyberbullying, Aggressiveness",English,XX.06.2016 - XX.08.2016,N/A,N/A,#GamerGate,"They used Twitter Streaming API. They used #GamerGate as a seed to collect tweets that are likely to involve the type of behaviour. As a baseline, they collected a random sample set of 1M tweets to compare with tweets collected with #GamerGate as a seed for a snowball sampling of other hashtags.",Not discussed,,N/A,Not discussed,,,,Not annotated 1M of random tweets and 659k tweets likely to be abusive (collected using #GamerGate as a seed for snowball collection),N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,,About a person,No,Non-human
18,No,"(Kirk et al., 2021)","Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset
",arxiv preprint,UK,"Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions. In this paper, the authors collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. They find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than `traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.",Yes,5,09.06.2021,23.09.2022,Yes,Yes,"Not explicitly discussed; keywords for 'hate' noisy label includes: sexist, offensive, vulgar, wh*ore, sl*t, prostitute",Yes,,No,,,,,"FB, Pin",2,1,"Pinterest, Facebook",Hate Speech,English,13.03.2021 - 01.04.2021.,N/A,N/A,Not discussed,"They collected memes from Pinterest using keyword search terms as noisy labels for whether the returned images are likely hateful or non-hateful. They used heuristics in choosing keywords. After dropping duplicate memes, there was 2840 images where 37% were hateful.",Not discussed,,N/A,Not discussed,,,,Not annotated 2840 memes (37% belongs to the hateful category),,,,Not discussed,,Not discussed,,Specific,gender,Targeted,"gender, sexuality",Yes,Yes,,gender,Yes,,Yes,,About a person,No,Human
20,No,"(Glavas et al., 2020)",xhate-999: Analyzing and Detecting Abusive Language Across Domains and Languages,International Conference on Computational Linguistics,"Germany, Croatia, UK","The authors present XHate-999, a multi-domain and multi-lingual data set which consits of manual translations of English abusive content of different domains into five languages (i.e. German, Croatian, Russian, Albanian and Turkish) for abusive language detection. By aligning test instances across six typologically diverse languages, XHate-999 for the first time allows for disentanglement of the domain transfer and language transfer effects in abusive language detection. They conduct a series of domain- and language-transfer experiments with state-of-the-art monolingual and multilingual transformer models, setting strong baseline results and profiling XHate-999 as a comprehensive evaluation resource for abusive language detection. Finally, they show that domain- and language-adaption, via intermediate masked language modeling on abusive corpora in the target language, can lead to substantially improved abusive language detection in the target language in the zero-shot transfer setups.",Yes,33,XX.12.2020,31.08.2022,Yes,Yes,Abusiveness,No,,Yes,Github,,https://github.com/codogogo/xhate,Yes,xhate-999,6,5,"English data sets GAO, TRAC, WUL",Abusiveness,"English, Albanian, Croatian, German, Russian, Turkish","Implicitly before 2018, with the newest of the sampled data sets being TRAC which has been published in 2018",Implicitly before 2018,N/A,N/A,"the data from WUL, TRAC and GAO has been checked for words that are non-indicative, text that is not written in English and content that is too specific and/or geographically localized which would hinder proper translation. They end up with 600, 300 and 99 instances for WUL, TRAC and GAO, respectively.",No,,manual,Yes,(translation) The text is translated as accurate as possible into the mother tongue of the translator. The level of abuse present in the original text ought to be preserved in the translation.,Not discussed,,999 (for each language),N/A,N/A,(translators) small number of carefully selected translators per target language,Yes,"(translation) Some ground rules for preserving the abusiveness of the English original are provided. These consist of making semantif modifications, if needed, subtle nuances ought to be preserved and not all expressions are easy to understand and therefore require more information to be translated in a proper way.",Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
23,No,"(Fanton et al., 2021)",Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech,Annual Meeting of the Association for Computational Linguistics / International Joint Conference on Natural Language Processing,Italy,"In this paper,  the authors propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. The experiments comprised several loops including dynamic variations. Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. To the authors' knowledge, the resulting dataset is the only expert-based multi-target hate speech / counter narrative (HS/CN) dataset available to the community.",Yes,10,XX.08.2021,01.09.2022,Yes,Yes,"""Responses, i.e., Counter-Narratives (CN), are non-aggressive textual feedback using credible evidence, factual arguments, alternative viewpoints, and are considered as an effective strategy (Benesch, 2014; Schieb and Preuss, 2016) to confront hate speech while respecting the human rights""",No,,Yes,Github,,https://github.com/marcoguerini/CONAN,Yes,CONAN,6,6,prototypical hate list with written responses and fully written Hate Speech/Counter Narrative pairs,Counter Narratives,English,Implicitly before August 2021,Implicitly before August 2021,N/A,N/A,"NGO workers responded to a prototypical ""hate list"" and in addition created Hate Speech/Counter Narrative pairs themself ",No,,manual,Yes,HS/CN pairs were reviewed in terms of their fitting as a response. In multiple review steps refined datasets were created.,Yes,20 experts from two different NGOs collected HS/CN pairs,5000,N/A,N/A,Three annotators from a pool of internship students,Yes,"They were asked to prove (a) if the pair was valid, (b) if the pair was not perfect, but easily amendable, to modify it, (c) if the CN is completely irrelevant, (d) whenever there were statistics to check them",Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,Yes,Non-human
24,No,"(Hardaker & McGlashan, 2016)",‘‘Real men don’t hate women’’: Twitter rape threats and group identity,Journal of Pragmatics,UK,"This paper investigates this increasingly prominent phenomenon of rape threats made via social networks. Specifically, the authors investigate the sustained period of abuse directed towards the Twitter account of feminist campaigner and journalist, Caroline Criado-Perez. The authors then turn their attention to the formation of online discourse communities which respond to and participate in forms of extreme online misogyny on Twitter. The authors take a corpus of 76,275 tweets collected during a three month period in which the events occurred (July to September 2013), which comprises 912,901 words. They then employ an interdisciplinary approach to the analysis of language in the context of this social network. The approach combines quantitative approaches from the fields of corpus linguistics to detect emerging discourse communities, and then qualitative approaches from discourse analysis to analyse how these communities construct their identities.",Yes,203,XX.XX.2016,01.09.2022,Yes,Yes,"""Users may also come into contact with (or become engaged in) behaviours that pose risks to their personal wellbeing, safety, and security. Issues such as online grooming, cyberharassment, predation, e-fraud and so forth have become a real online threat""",Yes,"high-risk, low-risk, no-risk users",No,,,N/A,,CPCC / CPTMC,2,2,Twitter,Abusiveness,English,N/A,midnight 25/06/13 to midnight 25/09/13,N/A,petition of feminist campaigner Caroline Criado-Perez to have Elizabeth Fry's image replaced with the image of another woman,N/A,No,,manual,Yes,"users entangled with Caroline Criado-Perez are categorized as high-risk, low-risk, or no-risk users",Yes,They sampled tweets one month prior to the abusiveness towards CCP starting and two months after that in order to recognize differences,208 risky users,N/A,N/A,The authors,No,,No,,Specific,religion,Non-targeted,N/A,No,Yes,Jewish,religion,Yes,jews,Yes,Caroline Criado-Perez,To a person,No,Human
27,No,"(Mulki et al., 2019)",L-HSAB: A Levantine Twitter Dataset for Hate Speech and Abusive Language,Workshop on Abusive Language Online,"Turkey, Tunisia","In this paper, the authors introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents. They also provide a detailed review of the data collection steps and how they design the annotation guidelines such that a reliable dataset annotation is guaranteed. This has been later emphasized through the comprehensive evaluation of the annotations as the annotation agreement metrics of Cohen’s Kappa (k) and Krippendorff’s alpha (α) indicated the consistency of the annotations.",Yes,83,XX.08.2019,01.09.2022,Yes,Yes,"""In (Nockleby, 2000), hate speech (HS) is formally deﬁned as “any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic”."" ",Yes,"hate, abusive, normal",Yes,Github,,https://github.com/Hala-Mulki/L-HSAB-First-Arabic-Levantine-HateSpeech-Dataset,Yes,L-HSAB,1,1,Twitter,Hate Speech,Arabic,Implicitly between February and August of 2019,March 2018 - February 2019,N/A,N/A,"They used to Twitter API searching for keywords such as ""refugees"", ""females"", ""Arabs"", and ""Druze"". It was mainly collected form timelines of politicians, activists and TV anchors.",No,,manual,No,,Yes,"They only collected tweets written in the Levantine dialect and also filtered out non-Arabic, non-textual, promoted, and duplicated content. ","6000 (ended up with 5,846 because of disagreement in some cases)","4,676","1,170","Three annotators, one male and two females, that are Levantine native speakers and are at a higher educational level (Postdoc/PhD).",Yes,"Distinguishing between hate, abusive or normal. Whereas normal tweets are those without offensive, aggressive, insulting and profanity content. Abusive tweets contain a combination of offensive, aggressive, insulting and profanity content. Hate tweets are those that contain abusive language, are dedicated towards a specific person and demean or dehumanize that person or that group of people.",Not discussed,,General,N/A,Targeted,"race, gender, sexuality, nationality, religion, other",Yes,No,,,No,,No,,,No,Human
29,No,"(Saeed et al., 2021)",Roman Urdu toxic comment classification,Language Resources & Evaluation,"Pakistan, Belgium","This paper addresses the challenge of Roman Urdu toxic comment detection by developing a first-ever large labeled corpus of toxic and non-toxic comments. The developed corpus, called RUT (Roman Urdu Toxic), contains over 72 thousand comments collected from popular social media platforms and has been labeled manually as either toxic or non-toxic with a strong inter-annotator agreement. With this dataset, the authors train several classification models to detect Roman Urdu toxic comments, including classical machine learning models with the bag-of-words representation and some recent deep models based on word embeddings. Despite the success of the latter in classifying toxic comments in English, the absence of pre-trained word embeddings for Roman Urdu prompted to generate different word embeddings using Glove, Word2Vec and FastText techniques, and compare them with task-specific word embeddings learned inside the classification task. Finally, they propose an ensemble approach, reaching our best F1-score of 86.35%, setting the first-ever benchmark for toxic comment classification in Roman Urdu.",Yes,6,29.01.2021,01.09.2022,Yes,Yes,"""The word ‘‘Toxic’’ means ‘‘Poisonous’’ in its literal meaning. Recently, it gained prominence in the context of text data during a famous competition hosted at Kaggle on classifying comments that are rude, disrespectful, or intended to make someone leave a conversation.1 In this paper, we generalize their notion to the use of hateful or abusive words, obscenity, threat, insult, or execration towards gender, color, race, religion, ethnicity, ideology, culture, etc in a text.""",Yes,toxic and non-toxic,Yes,Github,,https://github.com/hafizhassaan/Roman-Urdu-Toxic-Comments,Yes,RUT,1,1,"YouTube, Twitter, Facebook, hamariweb.com, other Roman Urdu websites",Toxicity,Urdu,Implicitly before January 2021,Implicitly before January 2021,N/A,N/A,"scrapers were used for the different sites. They searched for controversial topics such as politics, celebrities, sportspeople, Indo-Pak wars, or religions.",Yes,They did not collect any information related to the identity of the indiviudals who wrote the comments.,manual,No,,Yes,They discarded all comments written solely in English or solely in Urdu to only keep the Roman Urdu comments.,"72,771",N/A,N/A,"Three annotators; all male and aged 20-30, having a masters degree, having Urdu as first language and English as a second language, familiar with the Roman Urdu writing style.",Yes,"A comment should be labeled if it is at least one of the following; abusive, obscene, contains a threat, insulting, or contains identity hate. It is non-toxic if it does not hold any of these characteristics. ",Not discussed,,General,N/A,Targeted,"gender, race, religion, political, other",Yes,No,,,No,,No,,,No,Human
30,No,"(Aurpa et al., 2022)",Abusive Bangla comments detection on Facebook using  transformer‑based deep learning models,Social Network Analysis and Mining,Bangladesh,"In this work, the authors concentrate on identifying abusive comments of Bangla language in social media (Facebook) that can filter out at the primitive stage of social media’s affixing. To classify abusive comments swiftly and precisely, they apply transformer-based deep neural network models. They employ pre-training language architectures, BERT (Bidirectional Encoder Representations from Transformers) and ELECTRA (Efficiency Learning an Encoder that Classifies Token Replacements Accurately). They have conducted this work with a novel dataset comprises 44,001 comments from multitudinous Facebook posts. In this classification process, we have exhibited an average accuracy, precision, recall, and f1-score to evaluate the proposed models. The outcomes have brought a percipience of the applied BERT and ELECTRA architecture that performs notably with 85.00% and 84.92% test accuracy, respectively.",Yes,5,29.12.2021,01.09.2022,Yes,Yes,"""Identifying many contents, including threats and sexual harassment""",Yes,abusive and non-abusive,No,,,,,N/A,1,1,Facebook,Abusiveness,Bengali,Implicitly before June 2021,Implicitly before June 2021,N/A,N/A,N/A,No,,automated,No,,Yes,All text containing more than 20% of other languages has been removed.,44001,N/A,N/A,N/A,No,,No,,General,N/A,Targeted,sexuality,No,No,,,No,,No,,,No,Human
31,No,"(Papasavva et al., 2021)",“Is it a Qoincidence?”: An Exploratory Study of QAnon on Voat,Web Conference,"UK, USA, Germany","This paper provides an empirical exploratory analysis of the QAnon community on Voat.co, a Reddit-esque news aggregator, which has captured the interest of the press for its toxicity and for providing a platform to QAnon followers. More precisely, the authors analyze a large dataset from /v/GreatAwakening, the most popular QAnon-related subverse (the Voat equivalent of a subreddit), to characterize activity and user engagement. To further understand the discourse around QAnon, they study the most popular named entities mentioned in the posts, along with the most prominent topics of discussion, which focus on US politics, Donald Trump, and world events. They also use word embeddings to identify narratives around QAnon-specific keywords. Their graph visualization shows that some of the QAnon-related ones are closely related to those from the Pizzagate conspiracy theory and so-called drops by “Q.” Finally, they analyze content toxicity, finding that discussions on /v/GreatAwakening are less toxic than in the broad Voat community.",Yes,21,XX.04.2021,16.09.2022,Yes,Yes,"The research is focused on online discussions regarding the QAnon movement, which believes many politicians to be involved in a pedophile ring and holds other such beliefs.",No,,No,,,,,N/A,1,1,Voat,Conspiracy,English,28.05.2020 - 10.10.2020,"May 28 to October 10, 2020",N/A,N/A,Voat API,Yes,They do not attempt to identify users,N/A,Not discussed,,,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,political,Targeted,political,Yes,Yes,Politics,political,Yes,Democrats,Yes,"Hillary Clinton, Joe Biden",About a person,No,Human
33,No,(Lee 2022),Analyzing Zoombombing as a new communication tool of cyberhate in the COVID-19 era,Online Information Review,USA,"The paper studies the occurences of so called ""zoombombing"" at the beginning of the COVID-19 pandemic. In order to this a dataset of all reported cases was created by searching for News Articles and Twitter content regarding the topic.",Yes,6,XX.XX.2022,02.09.2022,Yes,Yes,"""Cyberhate includes “any use of electronic communications technology to spread anti-Semitic, racist, bigoted, extremist or terrorist messages or information. These electronic communications technologies include the Internet as well as other computer- and cell phone-based information technologies""",Yes,cyber-racism,No,,,,,N/A,1,1,"Google News, LexisNexis, Twitter",Cyberhate,English,20.05.2020,10.03.2020 - 10.04.2020,N/A,staying at home due to the COVID-19 pandemic,"They looked for News Articles and Tweets containing the keywords ""Zoombomb"", ""Zoom bomb"", ""Zoombombing"", and ""Zoom bombing"".",No,,N/A,No,,Yes,They checked for duplicates and unnecessary information,449,N/A,N/A,N/A,No,,No,,Specific,race,Targeted,"race, religion",No,Yes,Racist zoombombing,race,No,,No,,,No,Human
35,No,"(Mathew et al., 2019)",Thou Shalt Not Hate: Countering Online Hate Speech,International AAAI Conference on Web and Social Media ,"India, USA","In this paper, the authors create and release the first ever dataset for counterspeech using comments from YouTube. The data contains 13,924 manually annotated comments where the labels indicate whether a comment is a counterspeech or not. This data allows the researchers to perform a rigorous measurement study characterizing the linguistic structure of counterspeech for the first time. This analysis results in various interesting insights such as: the counterspeech comments receive much more likes as compared to the noncounterspeech comments, for certain communities majority of the non-counterspeech comments tend to be hate speech, the different types of counterspeech are not all equally effective and the language choice of users posting counterspeech is largely different from those posting non-counterspeech as revealed by a detailed psycholinguistic analysis. Finally, the authors build a set of machine learning models that are able to automatically detect counterspeech in YouTube videos with an F1-score of 0.71. They also build multilabel models that can detect different types of counterspeech in a comment with an FI-score of 0.60.",Yes,99,06.07.2019,02.09.2022,Yes,Yes,"""In this paper, we deﬁne counterspeech as a direct response/comment (not reply to a comment) that counters the hateful or harmful speech""",Yes,"eigth types of counterspeech: presenting facts, pointing out hypocrisy or contradictions, warning of offline or online consequences, affiliation, denouncing hateful or dangerous speech, humor, positive tone, hostility",Yes,Github,,https://github.com/hate-alert/Countering_Hate_Speech_ICWSM2019,Yes,thou shalt not hate,1,1,YouTube,Counterspeech,English,Implicitly befor July of 2019,Implicitly befor July of 2019,N/A,N/A,"YouTube comment scrapper, focussing on three communities; Jewish people, Black people, and the LGBT community. Looking for keywords regarding these communities.",No,,manual,No,,Yes,They manually selected videos that contain some act of hate against one of the communities.,"13,924",N/A,N/A,Two PhD studens working in the area of social computing and three undergraduate students in computer science with ages ranging between 21 and 30.,Yes,"The annotators have to distinguish between non- and counterspeech and have to classify counterspeech into one of eight categories (presenting facts, pointing out hypocrisy or contradictions, warning of offline or online consequences, affiliation, denouncing hateful or dangerous speech, humor, positive tone, hostility).",Not discussed,,Specific,"religion, race, sexuality",Non-targeted,N/A,No,Yes,"Jewish, Black, LGBT","religion, race, sexuality",No,,No,,,Yes,Human
36,No,"(Fortuna et al., 2019)",A Hierarchically-Labeled Portuguese Hate Speech Dataset,Workshop on Abusive Language Online,"Portugal, Spain","In this paper, the authors present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, they defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels (‘hate’ vs. ‘no-hate’). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, they carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome.",Yes,71,XX.08.2019,02.09.2022,Yes,Yes,"""Fortuna and Nunes (2018) present the following definition of hate speech: “Hate speech is language that attacks or diminishes, that incites violence or hate against groups, based on speciﬁc characteristics such as physical appearance, religion, descent, national or ethnic origin, sexual orientation, gender identity or other, and it can occur with different linguistic styles, even in subtle forms or when humour is used.” We adopted this definition in our work.""",Yes,"Hate, non-hate, and 81 hate speech categories.",Yes,Github,,https://github.com/paulafortuna/Portuguese-Hate-Speech-Dataset,Yes,Hierarchically-Labeled-Portuguese-Hate-Speech-Dataset,1,1,Twitter,Hate Speech,Portuguese,08.03.2017 - 09.03.2017,01.2017 - 03.2017,N/A,N/A,Twitter's search API searching for keywords and profiles,Yes,The data was anonymized by using the tweet id.,manual,Not discussed,,Yes,They kept all tweets written in Portuguese and eliminated repetitions and retweets. They also kept a maximum of 200 tweets per search instance to make it more diverse.,"5,668",N/A,N/A,Three annotators classified every message. 18 Portuguese native speakers (Information Science students) performed the binary annotation.,Yes,The annotators ought to evalute the tweet and classify it as either hate or non-hate accordingly to their opinion.,Not discussed,,General,N/A,Targeted,"body, religion, nationality, race, sexuality, gender, other",Yes,No,,,No,,No,,,No,Human
37,No,"(Botelho et al., 2021)",Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate,Findings of the Association for Computational Linguistics,UK,"This paper evaluates the role of semantic and multimodal context for detecting implicit and explicit hate. The authors show that both text- and visual- enrichment improves model performance, with the multimodal model (0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While the unimodal-text context-aware (transformer) model was the most accurate on the subtask of implicit hate detection, the multimodal model outperformed it overall because of a lower propensity towards false positives. They find that all models perform better on content with full annotator agreement and that multimodal models are best at classifying the content where annotators disagree. To conduct these investigations, they undertook highquality annotation of a sample of 5,000 multimodal entries. Tweets were annotated for primary category, modality, and strategy. ",Yes,0,XX.XX.2021,02.09.2022,Yes,Yes,Hate,Yes,"hate, counterspeech, reclaimed, and neutral",Yes,Github,,https://github.com/botelhoa/Dog_Whistle_Hate,Yes,Dog Whistle,2,1,Twitter,Hate Speech,English,Implicitly between 2019 and 2021,Implicitly before 2019,N/A,N/A,They took the MMHS150K dataset as a basis and look for multimodal tweets in there,Not discussed,,manual,No,,Yes,"They searched for tweets containing words from a list of keywords (i.e. wall, card, confederate, maga, islam, sjw, gender, crim, npc, normie, ))), muslim, illegal, caravan, obama, hillary, america) and sampled 2,500 tweets containing these words. Another 2,500 tweets were sampled randomly out of the dataset.","5,000","4,000",500,"All annotators had prior experience annotating online hate, and each completed a minimum of four weeks of training. All entries with disagreement were sent for review by an expert annotator. The expert was a PhD student researching online and ofﬂine hate, who had previously worked on two annotation projects.",Yes,"First the annotators ought to distinguish between Hate, Counterspeech, Reclaimed, and Neutral to determine the general tone of the post. Then the modality has to be clarified, which means deceding wether the text, the image or both convey the hateful message. Last the strategy has to be identified which can be “Explicit”, “Psuedo-factual”, “Normative statements”, “Coded language”, or “Creative expressions”.",Not discussed,,Specific,race,Non-targeted,N/A,No,Yes,Ethnicity,race,No,,No,,,No,Human
38,No,"(Alsafari & Sadaoui, 2021)",Semi-Supervised Self-Training of Hate and Offensive Speech from Social Media ,Applied Artificial Intelligence,"Saudi Arabia, Canada","This study devises a semi-supervised classification approach with self-training to leverage the abundant social media content and develop a robust Offensive and Hate Speech (OHS) classifier. The classifier is self-trained iteratively using the most confidently predicted labels obtained from an unlabeled Twitter corpus of 5 million tweets. Hence, the authors produce the largest supervised Arabic OHS dataset. To this end, they first select the best classifier to conduct the semi-supervised learning by assessing multiple heterogeneous pairs of text vectorization algorithms (such as N-Grams, World2Vec Skip-Gram, AraBert and DistilBert) and machine learning algorithms (such as SVM, CNN and BiLSTM). Then, based on the best text classifier, they perform six groups of experiments to demonstrate this approach’s feasibility and efficacy based on several self-training iterations.",Yes,2,25.10.2021,05.09.2022,Yes,Yes,"""They annotated using a rogorous labelung process (Alsafari, Sadaoui, and Mouhoub 2020c): Offensive/Hateful: tweets that attack or threaten individuals or groups based on their protected characteristics, including religion, race, gender, ethnicity, and nationality.""",Yes,Clean & Offensive/Hateful,No,,,,,"SEED, OHS1, OHS2",4,1,Twitter,"Offensiveness, Hate Speech",Arabic,Implicitly between 2020 and March of 2021,Implicitly before 2020,N/A,N/A,"The SEED dataset is based on two datasets, the first dataset used four strategies: keyword-based, profile-based, hashtag-based, and defensive-based.",Not discussed,,manual,No,,Yes,"For the SEED dataset they chose two high-quality annotated online hate speech Arabic dataset (OHS1, OHS2)",13140,9338,4002,N/A,Yes,The annotators ought to distinguish between Clean and Offensive/Hate,Not discussed,,General,N/A,Targeted,"religion, race, gender, nationality",Yes,No,,,No,,No,,,No,Human
39,No,"(Maity & Saha, 2021)",A Multi-task Model for Sentiment Aided Cyberbullying Detection in Code-Mixed Indian Languages,International Conference on Neural Information Processing,India,"In this work, the authors have created a benchmark Hindi-English code-mixed corpus called BullySent, annotated with bully and sentiment labels for investigating how sentiment label information helps to identify cyberbully in a better way. For a vast portion of India, both of these languages constitute the primary means of communication, and language mixing is common in everyday speech. A multi-task framework called MT-BERT+VecMap based on two different embedding schemes for the efficient representations of code-mixed data, has been developed. The proposed multi-task framework outperforms all the single-task baselines with the highest accuracy values of 81.12(+/−1.65)% and 77.46(+/−0.99)% for the cyberbully detection task and sentiment analysis task, respectively.",Yes,1,XX.12.2021,16.09.2022,Yes,Yes,Bullying; Sentiment,No,,Yes,Github,,https://github.com/MaityKrishanu/Bully_Sentiment (Deleted),No,BullySent,1,1,Twitter,"Bullying, Sentiment",Hindi-English,Implicitly from November 2020 to March 2021,Implicitly before March 2021,N/A,N/A,Twitter Search API,Not discussed,,manual,No,,Yes,"They filtered out tweets through different steps: (i) If a tweet is a duplicate; (ii) If only the URL is used in a tweet; (iii) Every tweet but the Hindi, English, or Hinglish tweets; (iv) If the length of the tweet is below ten characters; (v) If only a few user references are used in a tweet.",6084,N/A,N/A,Three annotators with a background in English and Hindi,Yes,"They ought to label each tweet twice, for the cyberbully class (Non-bully/bully) and for the sentiment class (Positive/Neutral/Negative).",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
40,No,"(Smetanin & Komarov, 2021)",Share of Toxic Comments among Different Topics: The Case of Russian Social Networks,Conference on Business Informatics,Russia,"Within this study, the authors identified the share of toxic comments among different topics in Russian-language comments from social network Pikabu. Firstly, for toxic comments classification, they manually labelled the training dataset and fine-tuned several language models. To provide further toxic comments studies with strong classification baselines, they made their pre-trained publicly available. Secondly, they proposed an approach for topics labelling based on six major objective and observable dimensions for objective wellbeing measurement used by intergovernmental and government organisations. Lastly, they conducted an analysis of Pikabu data. They found that the largest share of toxic comments was under posts about politics, while security and socioeconomic topics ranked second and third, and the rest of the topics showed roughly the same values.",Yes,2,XX.09.2021,05.09.2022,Yes,Yes,"""This paper aims at identifying the share of toxic comments among different topics in Russian-language comments from social network Pikabu. In contrast with existing studies, we primarily focused on the relation between toxic comments and online topics in posts from social networks.""",Yes,"health, socioeconomic development, job opportunities, safety, environment, and politics",No,,,,,N/A,3,1,Pikabu,Toxicity,Russian,2019,Implicitly before 2019,Russia,N/A,"For each of the topics, they compiled a list of Russian keywords, which was made up by a total of 111 words. ",Yes,All posts were anonymized,manual,No,,Yes,"12,000 comments were selected randomly.",12000,N/A,N/A,Russian-language speakers form the crowdsourcing platform Yandex.Toloka,Yes,They used the annotation instructions for toxicity from Jigsaw Toxic Comment Classiﬁcation Challenge with minor modiﬁcations for binary classiﬁcation.,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
41,No,"(Albadi et al., 2019)",Investigating the effect of combining GRU neural networks with handcrafted features for religious hatred detection on Arabic Twitter space,Social Network Analysis and Mining,USA,"This research investigates the problem of recognizing Arabic tweets using inflammatory and dehumanizing language to promote hatred and violence against people on the basis of religious beliefs. In this work, the authors create the first public Arabic dataset of tweets annotated for religious hate speech detection. They also create three public Arabic lexicons of terms related to religion along with hate scores. They then present a thorough analysis of the labeled dataset, reporting most targeted religious groups and hateful and non-hateful tweets' country of origin. The labeled dataset is then used to train seven classification models using lexicon-based, n-gram-based, and deep-learning-based approaches. These models are evaluated on new unseen dataset to assess the generalization ability of the developed classifiers. While using Gated Recurrent Units with pre-trained word embeddings provides best precision (0.76) and F1 score (o.77), training that same neural network on additional temporal, users, and content features provides the state-of-the-art performance in terms of recall (0.84).",Yes,17,05.08.2019,05.09.2022,Yes,Yes,"""Religious hate speech is insulting, offensive, or hurtful and is intended to incite hate, discrimination, or violence against an individual or a group of people on the basis of religious beliefs or lack of such.""",Yes,"different beliefs: Judaism, Christianity, Atheism, Islam, Shia, and Sunni",Yes,Github,,https://github.com/nuhaalbadi/Arabic_hatespeech,Yes,AraHate,1,1,Twitter,Religious Hate Speech,Arabic,November 2017 & January 2018,Implicitly before November 2017 and between November and December 2017,N/A,N/A,"Twitter's search API, using keywords for the different religions",Not discussed,,manual,No,,Yes,They selected an equal amount of tweets for each religion. They excluded retweets and did not collect any reply tweets.,"6,600","6,000",600,"Arabic-speaking annotators with IP addresses from one of the Arabic-speakng Middle Eastern countries from the crowdsourcing platform ""Figure Eight"".",Yes,"They asked annotators to read a tweet carefully before deciding if the tweet: (a) included an instance of religious hate speech (we refer to this as hate class); (b) didn’t contain any instances of religious hate speech (we refer to this as ¬hate class); (c) was unclear or unrelated to religious hate speech. If annotators decided that the tweet contained an instance of hate speech, they were asked to select one or more religious groups that the tweet was being hateful to. ",Not discussed,,Specific,religion,Targeted,religion,No,Yes,Religion,religion,No,,No,,,No,Human
42,No,"(Febriana & Budiarto, 2019)",Twitter Dataset for Hate Speech and Cyberbullying Detection in Indonesian Language ,International Conference on Information Management and Technology,Indonesia,"This paper presents a process of creating a dataset from Twitter which potentially can be used to detect tweets that contains hate speech. More than 1 million tweets were collected during the 2019 Election Debate Program in Indonesia. Finally, only 83,752 tweets were included in the analysis based on language filtering and some other preprocessing steps. Latent Dirichlet Allocation (LDA) algorithm was used to extract the topic for each tweet to see whether these topics can be associated with debate themes. Pretrained sentiment analysis was also applied to the dataset to generate a polarity score for each tweet. From 83,752 tweets included in the analysis step, the number of positive and negative tweets are almost the same.",Yes,25,XX.08.2019,16.09.2022,Yes,Yes,Cyberbullying regarding political debates,No,,No,,,,,N/A,1,1,Twitter,Cyberbullying,Indonesian,"17.01.2019, 17.02.2019, 17.03.2019, 30.03.2019, 13.04.2019","17th Jan, 17th Feb, 17th Mar, 30th Mar, and 13th Apr of 2019",Indonesia,Presidential election debates,Twitter API,Not discussed,,N/A,Not discussed,,Yes,Only tweets in Indonesian were included,"83,752 tweets",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,political,Targeted,political,No,No,,,Yes,Politicians,No,,,No,Human
43,No,"(Salminen et al., 2018)",Neural Network Hate Deletion: Developing a Machine Learning Model to Eliminate Hate from Online Comments,Internet Science Conference,"Finland, Qatar","THe authors propose a method for modifying hateful online comments to non-hateful comments without losing the understandability and original meaning of the comments. To accomplish this, they retrieve and classify 301,153 hateful and 1,041,490 non-hateful comments from Facebook and YouTube channels of a large international media organization that is a target of considerable online hate. They supplement this dataset by 10,000 Reddit comments manually labeled for hatefulness. Using these two datasets, they train a neural network to distinguish linguistic patterns. The model they develop, Neural Network Hate Deletion (NNHD), computes how hateful the sentences of a social media comment are and if they are above a given threshold, it deletes them using a language dependency tree. They evaluate the results by comparing crowd workers’ perceptions of hatefulness and understandability before and after transformation and find that their method reduces hatefulness without resulting in a significant loss of understandability. In some cases, removing hateful elements improves understandability by reducing the linguistic complexity of the comment. In addition, they find that NNHD can satisfactorily retain the original meaning on average but is not perfect in this regard. In terms of practical implications, NNHD could be used in social media platforms to suggest more neutral use of language to agitated online users.",Yes,7,XX.10.2018,05.09.2022,Yes,Yes,"""In this research, we tackle the problem of online hate, deﬁned as offensive use of language""",Yes,"hateful, non-hateful",No,,,,,NNHD (model),2,1,"Facebook, YouTube, Reddit",Offensiveness,English,Implicitly before October of 2018,December 2013 to January 2018,N/A,N/A,"Using the Facebook and YouTube API, also collecting comments from Reddit.",Not discussed,,"automated, manual",No,,Yes, They make use of a hateful keyword dictionary.,1342597,"20,000 (10,000 Reddit, 10,000 news comments)",N/A,"Crowdworkers were asked to do a ""sanity check"" to verify the automatically annotated comments.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
46,No,"(Pitenis et al., 2020)",Offensive Language Identiﬁcation in Greek,Conference on Language Resources and Evaluation,"UK, USA","This paper presents the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD is a manually annotated dataset containing 4,779 posts from Twitter annotated as offensive and not offensive. Along with a detailed description of the dataset, the authors evaluate several computational models trained and tested on this data.",Yes,105,XX.05.2020,07.09.2022,Yes,Yes,"""There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups.""",No,,No,,,,,OGTD,1,1,Twitter,Offensiveness,Greek,XX.05.2019 - XX.06.2019,Implicitly before June of 2019,N/A,N/A,"Twitter API, using different curse words to find offensive content",Not discussed,,manual,No,,Yes,"Out of 50,000 collected tweets, duplicates and retweets were deleted. Out of the remaining tweets, they selected 5,000 tweets randomly.",4779,3345,1434,Three volunteers,Yes,"The annotators ought to classify each tweet as Offensive, Not Offensive, or Spam",Not discussed,,General,N/A,Targeted,"race, gender",Yes,No,,,No,,No,,,No,Human
47,No,"(Sahmoud & Safi, 2020)",Detecting Suspicious Activities of Digital Trolls During the Political Crisis,International Conference on Informatics,"Turkey, Iraq","This paper introduces a set of first stage analyses automated methods that can be used to detect the effect of “digital trolls”. The authors considered the recently occurred Iraq unrest and Iraqi people protest as a case study to analyze the activities of Twitter users and detect if there are any external groups try to influence the people's opinions and orientations during the crisis. They gathered a new related dataset from Twitter that includes tweets and users' information collected during the crisis. Using this dataset, they analyzed the behavior and activities of twitter users by employing different features and tools. The results show that there are suspicious activities from external groups try to affect the general and normal political orientations in Iraq and there is a high possibility to be a kind of digital trolls. Additionally, the empirical study shows that “user favourites”, “tweet favorited”, and “is a retweet” features can be used effectively to detect the digital trolls in Twitter.",Yes,2,XX.02.2020,08.09.2022,Yes,Yes,"""There is an increasing number of formal and informal groups that gather or create a big number of accounts and use them to support or attack speciﬁc political agenda on social media depending on the organizations that support or belongs to these groups. In the worst scenario, these groups of people continuously try to affect the political opinions of people by spreading fake and bad news.""",No,,No,,,,,N/A,1,1,Twitter,Political Trolling,Arabic,18.11.2019 - 26.11.2019,18th to 26th November of 2019,N/A,Iraqi protests,"Twitter API, collecting data via different hashtags such as #iraq.",Not discussed,,manual,Not discussed,,Yes,removed duplicates,135922,N/A,N/A,N/A,No,,Not discussed,,Specific,political,Targeted,political,No,No,,,Yes,people involved in the political situation in iraq ,No,,,No,Human
48,No,"(Pamungkas et al., 2022)",Investigating the role of swear words in abusive language detection tasks,Language Resources and Evaluation,"Indonesia, Italy","In this study, the authors explore the phenomenon of swearing in Twitter conversations, by automatically predicting the abusiveness of a swear word in a tweet as the main investigation perspective. They developed the Twitter English corpus SWAD (Swear Words Abusiveness Dataset), where abusive swearing is manually annotated at the word level. This collection consists of 2577 instances in total from two phases of manual annotation. They developed models to automatically predict abusive swearing, to provide an intrinsic evaluation of SWAD and confirm the robustness of the resource. They model this prediction task as three different tasks, namely sequence labeling, text classification, and target-based swear word abusiveness prediction. They experimentally found that their intention to model the task similarly to aspect-based sentiment analysis leads to promising results. Subsequently, they employ the classifier to improve the prediction of abusive language in several standard benchmarks. The experiment results show that additional abusiveness feature of the swear words is able to improve the performance of abusive language detection models in several benchmark datasets.",Yes,1,11.01.2022,08.09.2022,Yes,Yes,"""We deﬁne abusive swearing as the use of swear word or profanity in several cases such as name-calling, harassment, hate speech, and bullying involving several sensitive topics including physical appearance, sexuality, race & culture, and intelligence, with intention from the author to insult or abuse a target (person or group). The other uses, such as reclaimed uses, catharsis, humor, or conversational uses, are considered as not-abusive swearing."" ",No,,Yes,Github,,https://github.com/dadangewp/SWAD-Repository,Yes,SWAD,3,1,Twitter,"Abusive Swearing, Casual Swearing",English,Implicitly before 2022,Implicitly before 2022,N/A,N/A,They collected tweets from the OLID dataset and the Holgate's dataset.,Yes,The datasets conform to Twitter Develepor Agreement,manual,Not discussed,,Yes,"They sampled tweets from the OLID and Holgate's dataset, that contained swear words.","2,577","2,319",258,The authors (Three annotators with different gender and ages),Yes,Distinguishing between abusive and non-abusive swearing,Not discussed,,General,N/A,Targeted,"body, sexuality, race, other",Yes,No,,,No,,No,,,No,Human
50,No,"(Hande et al., 2020)",KanCMD: Kannada CodeMixed Dataset for Sentiment Analysis and Offensive Language Detection,"Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media","India, Ireland","The authors introduce Kannada CodeMixed Dataset (KanCMD), a multi-task learning dataset of 7,671 comments for sentiment analysis and offensive language identification. The KanCMD dataset highlights two real-world issues from the social media text. First, it contains actual comments in code mixed text posted by users on YouTube social media, rather than in monolingual text from the textbook. Second, it has been annotated for two tasks, namely sentiment analysis and offensive language detection for under-resourced Kannada language. ",Yes,72,XX.12.2020,09.09.2022,Yes,Yes,"""Sentiment analysis and offensive language identiﬁcation are related and has common aspects between them. Having the model to learn both tasks would be advantages to utilise some cues from one task to improve the other.""",Yes,"different sentiments, and if offensive, targets are differentiated",Yes,Github,,https://github.com/adeepH/KanCMD,Yes,KanCMD,1,1,YouTube,"Offensiveness, Sentiment",Kannada-English,XX.02.2020 - XX.08.2020,Implicitly before August of 2020,N/A,N/A,"YouTube Comment Scrapper, searching for videos regarding different topics: movie trailers, current trends about the ban on mobile apps in India, India-China border issue, Mahabharata, and Transgenders. ",Not discussed,,manual,No,,Not discussed,,"7,671","6,136",768,"Five annotators, male and female with different education levels and schooling in Kannada and English.",Yes,"Each comment is annotated for sentiment and offensiveness. Regarding sentiment the annotators ought to distinguish between positive state, negative state, mixed feelings, neutral state, or if it's not in Kannada. 
The offensiveness is measured in the categories of not offensive, offensive untargeted, offensive targeted individual, offensive targeted group, offensive targeted other, or not in Kannada.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
52,No,"(Qureshi & Sabih, 2021)",Un-Compromised Credibility: Social Media Based Multi-Class Hate Speech Classification for Text,IEEE Access,Pakistan,"Ten separate binary classified datasets consisting of different hate speech categories are constructed. Each dataset was annotated by experts with the strong agreement of annotators under comprehensive, clear definition and well-defined rules. Datasets were well balanced and broad. They were also supplemented with language subtleties. Addressing different categories of hate separately, this paper aims to accurately predict their different forms, by exploring a group of text mining features. Two distinct groups of features are explored for problem suitability. These are baseline features and self-discovered/new features. Baseline features include the most commonly used effective features of related studies. Exploration found a few of them, like character and word n-grams, dependency tuples, sentiment scores, and count of 1st, 2nd person pronouns are more efficient than others. Due to the application of latent semantic analysis (LSA) for dimensionality reduction, this problem is benefited from the utilization of many complex and non-linear models and CAT Boost performed best. The proposed model is compared with related studies in addition to system baseline models. The results produced by the proposed model were much appreciating.",Yes,9,02.08.2021,09.09.2022,Yes,Yes,"Hate speech. ""In contrast to related concepts, it has also been identified in studies that there are different types of hate speech as well concerning its categories or targets on social media, e.g.: race, religion, ethnicity, gender, class, sexual orientation, behavior, physical, disability, and other (i.e. drunk, shallow people).…They aim to classify Hate Speech into different categories to ensure a more thorough approach of Hate Speech classification.""",Yes,"African, Arab, Asian, Christian, Islam, Jews, Race, Xenophobia, Gender, Sexual ... Hate",No,,,,,N/A,6,1,Twitter,Targets of Hate Speech,English,Implicitly before August 2021,Implicitly before August 2021,N/A,N/A,N/A,Not discussed,,manual,No,,Yes,"Out of over a 100,000 from different datasets they sampled ~40,000 tweets to balance out the used categories.",45688,N/A,N/A,12 experts,No,,Not discussed,,Specific,"religion, nationality, gender, race",Targeted,"race, religion, gender, class, sexuality, body, disability, other ",No,Yes,"Religion, Nationality, Gender","religion, nationality, gender",Yes,"Jews, Christians, Muslims, African, Asian",No,,,No,Human
54,No,"(Fagni et al., 2019)",Six Things I Hate About You (in Italian) and Six Classiﬁcation Strategies to More and More Eﬀectively Find Them,Italian Conference on CyberSecurity,Italy,"In this work, the authors propose six distinct machine learning classification strategies for hate speech detection: three based on conventional machine learning approaches, three based on neural networks. The main goal of the paper is to investigate whether it is possible to rely on neural networks and to achieve performance results at least comparable with those of NLP-based classifiers. The performances of the six configurations are evaluated over an annotated dataset consisting of 4,000 Italian tweets and 4,000 Italian Facebook comments. The results are encouraging. In particular, a deep learning model, based on an ensemble approach, obtains a F1 score of 0.786 on the Twitter data and 0.775 on the Facebook ones, the best results, compared to the ones obtained with the other tested configurations.",Yes,6,XX.02.2019,12.09.2022,Yes,Yes,"""Violence or hatred against individuals or groups based on certain attributes, such as: race or ethnic origin, religion, disability, gender, age, veteran status, sexual orientation and gender identity’2. Such violent content is usually referred as hate speech.""",No,,No,,,,,N/A,3,1,"Twitter, Facebook",Hate Speech,Italian,Implicitly before February 2019,Implicitly before February 2019,N/A,N/A,They combined parts of a Twitter and a Facebook dataset.,Not discussed,,manual,Not discussed,,Not discussed,,8000,6000,2000,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"race, religion, disability, gender, age, sexuality, gender, organization/institution",Yes,No,,,No,,No,,,No,Human
56,No,"(González & Cantu-Ortiz, 2021)",A Sentiment Analysis and Unsupervised Learning Approach to Digital Violence Against Women: Monterrey Case,2021 4th International Conference on Information and Computer Technologies (ICICT),Mexico,"This article seeks to address Violence Against Women in Digital Space in Monterrey, Mexico through Sentiment Analysis and Unsupervised Learning Techniques. The hypothesis is that Sentiment Analysis can associate sentiments to specific subjects that will lead to identify potential Digital Violence Against Women. The work uses Spanish-language text data from Twitter datasets.",Yes,1,11.03.2021,31.08.2022,Yes,Yes,"Digital Violence Against Women…specifically with the acts related to threats, insults and hate messages through social networks that violate integrity, dignity, or infringes some Women Human Right.",No,,No,,,N/A,No,"Twitter Monterrey’s dataset (New)

Hateval 2019

Kaggle dataset",3,1,Twitter,Digital Violence Against Women,Spanish,XX.09.2020 - XX.11.2020,Implicitly before 11.2020 (data collection time),"Monterrey, Nuevo León, México",N/A,"Use Twitter API to extract the tweets with daily frequency. Tweets are georeferenced in Monterrey, Nuevo León, México and are extracted in Spanish-language. The retweets are not included in order to avoid getting biased or repeated information, but are considered to be reflected in a specific attribute called retweet count. 11 variables or features to disaggregate data in text are established as follows: Date, Media (yes=1/no=0), Text of tweet, Favourite count, Length of tweet (characters), Retweet count, Hashtags, Possibly sensitive (yes=1/no=0), URLs (quantity), Is reply (yes=1/no=0), User mentions (quantity).",Not discussed,,N/A,Not discussed,,,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,gender,Targeted,gender,No,Yes,Sexism,gender,Yes,Women,No,,,No,Human
57,No,"(Subramani et al., 2019)",Deep Learning for Multi-Class Identification From Domestic Violence Online Posts,IEEE Access,"Australia, China, Pakistan","Focusing on identifying domestic violence online posts, this study aims to: 1) construct the novel ""gold standard” dataset from Facebook with manual multi-class annotation; 2) perform the extensive experiments with multiple deep learning architectures; 3) train the domain-specific embeddings for performance improvement and knowledge discovery; and 4) produce the visualizations to facilitate models analysis and results in interpretation. The empirical evidence on a ground truth dataset has achieved an accuracy of up to 92% in classes prediction.",Yes,39,11.04.2019,31.08.2022,Yes,Yes,"Domestic Violence (DV) refers to the various acts of abuse such as physical, sexual, emotional or any controlling behaviour within an intimate relationship.",Yes,"Domestic violence posts: Awareness, Empathy, Personal Story, Fund Raising or General",Yes,Github,,https://github.com/sudhasmani/DV_Dataset,Yes,Gold standard dataset,1,1,Facebook,Domestic Violence,English,Implicitly before 11.04.2019 (publication date),Implicitly before 11.04.2019 (publication date),N/A,N/A,"The Domestic Violence (DV) posts were collected from Facebook pages that discuss the range of DV-related matters. The Facebook Graph API was used in the extraction process and the search terms were ‘Domestic violence’ and ‘Domestic Abuse’. A number of posts and comments of approximately 100, 000 was returned following the data collection from the 10 most active DV pages.",Yes,"The posts were collected solely from publicly available pages, and the identities of individuals included in the extracted dataset remained confidential.",manual,Yes,"The annotation was performed by 2 research students. The Kappa coefficient was calculated to validate the inter-rater reliability. In case of uncertainty, the final label was assigned following an advice of the expert.",Yes,"The random 3,000 posts were sampled.",1654,3-fold stratified cross-validation,3-fold stratified cross-validation,"Two research students under the supervision of a consultant psychiatrist with specialisation in DV field, plus the advice from expert for uncertain cases.",Yes,"The exemplary messages, corresponding labels, and classification rationale have been presented in the following points:

P1 post as Personal Story: Emotional support from the community seeking through personal experience sharing (critical);

P2 post as Fund Raising: Financial assistance in the crisis moment solicitation (critical);

P3 post as Awareness: Awareness about the violence promotion (non-critical);

P4 post as Empathy: Empathy expression from community (non-critical);

P5 post as General: No additional insight into the DV problem (non-critical).",Not discussed,,Specific,other,Targeted,"body, sexuality",No,No,,,Yes,Domestic violence,No,,,No,Human
58,No,"(Parikh et al., 2019)",Multi-label Categorization of Accounts of Sexism using a Neural Framework,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),India,"This work explored classifying an account recounting any type(s) of sexism without the assumption of the mutual exclusivity of classes. It proposed a neural framework for the multi-label classification of accounts of sexism that can combine sentence representations built from word embeddings of different kinds through learnable model parameters with those created using pre-trained models. It yields results superior to many deep learning and traditional machine learning baselines. This work also provided the largest (13023) dataset for sexism classification, linked with 23 categories.",Yes,31,03.11.2019,31.08.2022,Yes,Yes,"Sexism, discrimination on the basis of one’s sex",Yes,"Sexism Categorization (23 in total): Role stereotyping, Attribute stereotyping, Body shaming, Hyper-sexualization (excluding body shaming), Internalized sexism, Pay gap, Hostile work environment (excluding pay gap), Denial or trivialization of sexist misconduct, Threats, Rape, Sexual assault (excluding rape), Sexual harassment (excluding assault), Tone policing, Moral policing (excluding tone policing), Victim blaming, Slut shaming, Motherhood-related discrimination, Menstruation-related discrimination, Religion-based sexism, Physical violence (excluding sexual violence), Mansplaining, Gaslighting, Other.",Yes,Other,The dataset can be requested for academic purposes only by providing some prerequisites as recommended by an ethics committee and agreeing to certain terms through the website (https://irel.iiit.ac.in/sexism-classification).,https://irel.iiit.ac.in/sexism-classification,No,Multi-label sexism account categorization dataset,1,1,Everyday Sexism Project website,Sexism,English,Implicitly before 03.11.2019 (publication date),Implicitly before 03.11.2019 (publication date),N/A,N/A,The data was crawled from the Everyday Sexism Project (ESP) website.,Yes,"Only use accounts of sexism and tags from entries on the Everyday Sexism Project website (ESP). The entry titles, which could contain sensitive information related to the names or locations of the victims (or contributors), are not saved or used at all. The dataset can be requested for academic purposes only by providing some prerequisites as recommended by an ethics committee and agreeing to certain terms. The requesters who fulfill these conditions will be emailed the data comprising only numerical placeholders and labels, with a script that fetches only accounts of sexism from ESP, to ensure that if an entry gets removed from ESP by a victim (or contributor), any and all parts of it in the dataset will also be removed.",manual,Yes,"A three-phase annotation process to ensure that the categorization of each account of sexism in the final dataset involved the labeling of it by at least two of our 10 annotators, most of whom have studied topics related to gender and/or sexuality formally. The annotators were given detailed guidelines, which evolved during the course of their work. Each annotator was given training, which included a pilot round involving evaluation and feedback. Phase 1 involved identifying one or more textual portions capturing distinct accounts of sexism from an entry obtained from Everyday Sexism Project and subsequently tagging each portion with at least one of the 23 categories of sexism, producing over 23000 labeled accounts. In phases 2 and 3, we sought redundancy of annotations for improved quality, as permitted by the availability of annotators adequately knowledgeable about sexism. Over 21000 accounts were categorized again in phase 2 such that the annotators for phases 1 and 2 were different. The inter-annotator agreement across phases 1 and 2, measured by the average of the Cohen’s Kappa scores for the per-category pairs of binary label vectors, is 0.584. Each account for which the label sets annotated across phases 1 and 2 were identical was included in the dataset along with the associated label set. In phase 3, some of the accounts for which there was a mismatch between the phase 1 and phase 2 annotations were selected. For each account, the annotators were presented with only the mismatched categories and asked to select or reject each. Duplicates and records for which the Everyday Sexism Project entry numbers match but the accounts do not fully match were removed at multiple stages. In order to improve the annotation reliability further, some records for which the annotations differed across phases 1 and 2 were discarded based on the annotators involved and sensitivities of the categories, resulting in a multi-label sexism categorization dataset of 13023 accounts. For our automated sexism classification experiments, we merge the categories found in less than 400 records with others as follows, resulting in 14 categories.",Yes,"After removing entries with less than 7 words, around 20000 entries were shortlisted for annotation; the authors prioritized shorter ones and tried to approximate the tag distribution on the website. Though shorter entries were preferred keeping in mind the potential future work of transfer learning to Twitter content, the neural framework is devised in a size-agnostic way.",13023,over 90000 unlabelled entries (pre-trained BERT); 9116 labelled entries,1953(15%),"Ten annotators, most of whom have formally studied topics related to gender and/or sexuality",Yes,"The annotators were given detailed guidelines, which evolved during the course of their work. The annotation guidelines are available along with the dataset upon request if prerequisites are fulfilled.",Not discussed,,Specific,gender,Targeted,gender,No,Yes,Sexism,gender,Yes,Women,No,,,No,Human
59,No,"(Berk & Filatova, 2019)",Incendiary News Detection,The Thirty-Second International Flairs Conference,USA,"This work intends to detect the incendiary news, news that incite hate. The authors leverage a resource from Turkish activists who manually tagged the news articles inciting hate. The non-incendiary news articles are retrieved from BBC and CNN. collect three different sets of non-incendiary news to ensure that both incendiary and non-incendiary news cover the same topics, issues, people, events, etc. The authors use several feature sets and classification approaches (Linear SVC, Naive Baysian, and MLP) to differentiate between incendiary and non-incendiary news. The classification system achieves 97.0% accuracy.",Yes,3,19.05.2019,31.08.2022,Yes,Yes,"Incendiary news, the news articles that ignite hatred.",No,,Yes,Github,,https://github.com/EnisBerk/Incendiary_news,Yes,"Nefret Söylemi Incendiary News corpus

Non-Incendiary News: BBC-1 corpusBBC-2 corpusCNN corpus",4,4,"Nefret Söylemi, BBC Turkish, CNN Turkish",Hate Speech,Turkish,Implicitly before 19.05.2019 (publication date),Implicitly before 19.05.2019 (publication date),N/A,N/A,"Collecting Incendiary News: The data were collected and labeled in the Nefret Söylemi (hate speech) project [http://www.nefretsoylemi.org/](http://www.nefretsoylemi.org/). To locate incendiary news, the Nefret Söylemi project activists use a set of keywords corresponding to the topics that are sensitive or controversial in the Turkish language community. The Nefret Söylemi website contains links to more than 1036 articles manually annotated as incendiary. The annotation includes: the picture of the article (in PDF format); the newspaper title; page number where the article was published; date the article was published; a brief (optional) abstract of the article in either English or Turkish; tags identifying why this article is an example of an incendiary article. The authors then use Tesseract Open Source OCR Engine to collect 80% of the incendiary news articles and retrieve the rest 20% utilizing Google queries with the title of the news article and the name of the newspaper where the article is published. The Nefret Söylemi Incendiary News corpus consists of 1036 manually labeled incendiary news articles.
Collecting Non-Incendiary News: On Step 1, the authors use the BBC and CNN web sites’ search bars and submit as queries the keywords (refugee, unbeliever, coconspirator, turcophobe, zealot, muslim, Jew, Armenian), and then collect two sets of non-incendiary news: CNN (948 docs) and BBC-1 (1031 docs). On Step 2, they run a classification experiment that allows them to identify those keywords that are typical for incendiary news articles. In this classification experiment they use the set of the collected incendiary news and the BBC-1 set of non-incendiary news. They then use information gain to identify which words are the best predictors of incendiary versus non-incendiary news (classification features). They use the 20 terms that are typical for incendiary news, to collect 50+ documents for each of these terms using the BBC’s web site search tool bar. Thus the BBC-2 collection of 1038 non-incendiary news articles is obtained.",No,,manual,Not discussed,,Not discussed,,Nefret Söylemi Incendiary News corpus: 1036 Non-Incendiary News:  BBC-1 corpus: 1031 BBC-2 corpus: 1038 CNN corpus: 948,80% (830) of the BBC-2 corpus in both experiments,1st experiment: 20% (208) of the BBC-2 corpus;,The activists who contributed in Nefret Söylemi project,Not discussed,,Not discussed,,Specific,"religion, nationality ",Non-targeted,N/A,Yes,Yes,"Muslim, Jew, Armenian","religion, nationality ",Yes,"refugee, turcophobe, coconspirator, zealot, unbeliever",No,,,No,Human
62,No,"(Aquino et al., 2021)",Toxic Comment Detection: Analyzing the Combination of Text and Emojis,2021 IEEE 18th International Conference on Mobile Ad Hoc and Smart Systems (MASS),USA,The authors propose a machine learning approach for detecting the toxicity of a comment by analyzing both the text and the emojis within the comment. The approach utilizes word embeddings derived from GloVe and emoji2vec to train a bidirectional Long Short Term Memory (biLSTM) model. They also create a new labeled dataset with comments with text and emojis. The accuracy score of the model on preliminary data is 0.911.,Yes,0,04.10.2021,31.08.2022,Yes,Yes,"Online toxicity such as harassment, bullying, and violence",No,,No,,,N/A,No,Augmented Twitter Dataset (with emojis),1,1,Twitter,Toxicity,English,Implicitly before 2021 (publication date of the original dataset),Implicitly before 2021 (publication date of the original dataset),N/A,N/A,"The authors modified an existing Twitter dataset of toxic comments to contain emojis. They parsed the dataset through the DeepMoji model which outputs emojis based on the sentiment of textual input, and then concatenated the text of the tweet with the outputted emojis.",Not discussed,,N/A,Not discussed,,,,56742,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
63,No,"(Lu et al., 2020)",Cyberbullying detection in social media text based on character-level convolutional neural network with shortcuts,Concurrency and Computation: Practice and Experience,"China, USA","This research focuses on textual cyberbullying detection. The authors propose a Char-CNNS (Character-level Convolutional Neural Network with Shortcuts) model to identify whether the text in social media contains cyberbullying. They use characters as the smallest unit of learning, enabling the model to overcome spelling errors and intentional obfuscation in real-world corpora. Shortcuts are utilized to stitch different levels of features to learn more granular bullying signals, and a focal loss function is adopted to overcome the class imbalance problem. We also provide a new Chinese Weibo comment dataset specifically for cyberbullying detection, and experiments are performed on both the Chinese Weibo dataset and the English Tweet dataset. The experimental results show that our approach is competitive with state-of-the-art techniques on cyberbullying detection task.",Yes,27,03.01.2020,31.08.2022,Yes,Yes,"Cyberbullying is bullying that takes place over digital devices such as cell phones, computers, and tablets, … can be achieved in various ways, such as sending a message containing abusive or offensive content to a victim",No,,Yes,Github,,https://github.com/NijiaLu/BullyDataset,Yes,Weibo comment dataset (new); English Tweet dataset (old),2,1,Weibo,Cyberbullying,"Chinese, English",Implicitly before 03.01.2020 (publication date),Implicitly before 03.01.2020 (publication date),N/A,N/A,"From Sina Weibo, the authors collected the large numbers of comments under the Weibo posts that belong to more than 20 celebrities who have reputations or who have experienced some vicious incidents. As a supplement, they also collected the comments on some brands, games, and social news that involve a wide range of fields to enrich the objects and scenes in the dataset.",Yes,All names in the comments are replaced by USERNAME for privacy purposes.,manual,Not discussed,,Yes,The authors manually adjusted the distribution of the data by discarding some comments that were not related to bullying.,19395,10-fold cross-validation for TF-IDF+SVM and Char n-grams+LR; Not specified for Word-CNN and Char-CNN,10-fold cross-validation for TF-IDF+SVM and Char n-grams+LR; Not specified for Word-CNN and Char-CNN,Three members who are familiar with Weibo and have a certain understanding of the target bloggers that we crawled.,Yes,"A Weibo comment is labeled as bullying if it

1. uses a sexist, racial or geographical slur.
    
2. uses swear words or humiliation to blame someone without a well founded argument.
    
3. blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims.
    
4. expresses a tendency to violence or curse on a minority.
    
5. contains attacks on appearance, body or family members.
    
6. is one of repeated negative comments, or call others to join the attack.
    
7. imposes a nickname that others are unwilling to accept or insulting.
    

Not all negative comments that marked as bullying followed the criteria. Other offensive content that is not in the criteria will be subjectively determined by the annotators based on the degree of maliciousness.",Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,20 celebrities who have bad reputations or who have experienced some vicious incidents,To a person,Yes,Human
64,No,"(Chakravarthi et al., 2022)",DravidianCodeMix: sentiment analysis and offensive language identification dataset for Dravidian languages in code-mixed text,Language Resources and Evaluation,"Ireland, India, UK","This paper describes the development of a dataset with more than 60,000 YouTube comments for three under-resourced Dravidian languages (Tamil-English, Kannada-English, and Malayalam-English). The dataset was manually annotated by volunteer annotators for sentiment analysis and offensive language identification and has a high inter-annotator agreement in Krippendorff’s alpha. The authors also present baseline experiments to establish benchmarks on the dataset using machine learning and deep learning methods.",Yes,18,04.02.2022,31.08.2022,Yes,Yes,Offensive…comment contains offence or profanity,Yes,"Sentiment: Positive, Negative, Mixed, Neutral Offensive language: Offensive untargeted, Offensive Targeted Individual, Offensive Targeted Group, Offensive Targeted Other.",Yes,Github,,https://github.com/bharathichezhiyan/DravidianCodeMix-Dataset,Yes,"DravidianCodeMix: Sentiment Analysis dataset, Offensive Language Identification Dataset (for Dravidian Languages in Code-Mixed Text)",2,2,YouTube,"Sentiment, Offensiveness","Tamil-English, Kannada-English, Malayalam-English",2019,2019,N/A,N/A,"The authors compiled the comments from different film trailers of Tamil, Kannada, and Malayalam languages from YouTube in the year 2019. The comments were gathered using YouTube Comment Scraper tool. They then utilized langdetect library to filter out the unintended languages.

They constructed an offensive language identification dataset for Dravidian languages by adapting the work of Zampieri et al. (2019). We reduced the three-level hierarchical annotation scheme of this work into a flat scheme with five labels to account for the types of offensiveness in the comments and the sixth label Not in intended language accounts for comments written in a language other than the intended language.",Yes,All the user-related information is removed from the corpora,manual,Yes,"The annotation setup involved three stages. To begin with, each sentence was annotated by two individuals. In the second step, the data was included in the collection if both the annotations agreed. In the event of contention, a third individual was asked to annotate the sentence. In the third step, in the uncommon case that all the three of them disagreed, at that point, two additional annotators were brought in to label the sentences. Each form was annotated by at least three annotators.

Information on gender, education background and medium of schooling were collected to know the diversity of the annotators. The annotators were cautioned that the user remarks may have hostile language. They were given a provision to discontinue with the annotation process in case the content is too upsetting to deal with. They were asked not to be partial to a specific individual, circumstance or occasion during the annotation process. Each Google form had been set to contain up to 100 comments and each page was limited to contain ten comments. If an annotator offered to volunteer more, the next Google Form was sent to them with another set of 100 sentences and in this way each volunteer chose to annotate as many sentences from the corpus as they wanted. The annotators were instructed to agree that they understood the scheme before they were allowed to proceed further. The researchers sent out the same comment forms to annotators but some of the forms were incomplete so these were discarded.",Not discussed,,"Sentiment analysis dataset: Tamil (41,933), Malayalam (18,171),  Kannada (6535) Offensive language identification dataset: Tamil (42,133), Malayalam (18,403),  Kannada (5874).",90% for all languages in both datasets,5% for all languages in both datasets,"Students in Indian Institute of Information Technology and Management-Kerala for Malayalam, Indian Institute of Information Technology-Tiruchirapalli and Madurai Kamaraj University for Tamil, and students in Visvesvaraya College of Engineering, Bangalore University for Kannada. The authors’ family members also volunteered to annotate the data. The majority of the annotators have received postgraduate level of education. All the annotators who volunteered to annotate the Tamil-English, Kannada-English and Malayalam-English datasets had bilingual proficiency in the respective code-mixed pairs and they were prepared to take up the task seriously. The majority of the annotators’ medium of schooling is English even though their mother tongue is Tamil, Kannada or Malayalam. All of them are fully proficient in using their native language.

A sample form (first assignment) was annotated by experts and a gold standard was created. The experts were a team of NLP researchers who have experience working with creating annotation standards and guidelines.",Yes,"Descriptions of each category label.

For Sentiment analysis dataset:

– Positive state: Comment contains an explicit or implicit clue in the content recommending that the speaker is in a positive state.

– Negative state: Comment contains an explicit or implicit clue in the content recommending that the speaker is in a negative state.

– Mixed feelings: Comment contains an explicit or implicit clue in both positive and negative feeling.

– Neutral state: Comment does not contain an explicit or implicit indicator of the speaker’s emotional state.

– Not in intended language: If the comment is not in the intended language. For example, for Tamil, if the sentence does not contain Tamil written in Tamil script or Latin script, then it is not Tamil. These comments were discarded after the data annotation process.

For offensive language identification dataset:

– Not Offensive: Comment does not contain offence or profanity.

– Offensive Untargeted: Comment contains offence or profanity not directed towards any target. These are the comments which contain unacceptable language without targeting anyone.

– Offensive Targeted Individual: Comment contains offence or profanity which targets an individual.

– Offensive Targeted Group: Comment contains offence or profanity which targets a group or a community.

– Offensive Targeted Other: Comment contains offence or profanity which does not belong to any of the previous two categories (e.g. a situation, an issue, an organization or an event).

– Not in indented language: If the comment is not in the intended language. For example, in Tamil task, if the sentence does not contain Tamil written in Tamil script or Latin script, then it is not Tamil. These comments were discarded after the data annotation process.",Not discussed,,Specific,other,Non-targeted,N/A,Yes,Yes,Not specified,other,Yes,Not specified,Yes,Not specified,About a person,No,Human
65,No,"(Xu et al., 2021)",Bot-Adversarial Dialogue for Safe Conversational Agents,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,USA,"The authors introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. They then go on to propose two novel methods for safe conversational agents, by either training on data from their new human-and-model-in-the-loop framework in a two-stage system, or ”baking-in” safety to the generative model itself. They find their new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, they expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.",Yes,15,XX.06.2021,31.08.2022,Yes,Yes,"In this paper, the authors use “offensive”, “toxic”, and “unsafe” interchangeably.",Yes,"Unsafe/Offensive Language Types: Hate Speech, Personal Attack, Profanity, and Other offensiveness",Yes,Other,ParlAI,https://parl.ai/projects/safety_recipes/,Yes,BAD(Bot-Adversarial Dialogues) dataset,5,1,"Dialogues between bots and crowdworkers, Reddit","Offensiveness, Toxicity",English,Implicitly before 06.2021 (publication date),Implicitly before 06.2021 (publication date),N/A,N/A,"Using the BAD method, the authors collect 5,784 dialogues (78,874 utterances) between bots and crowdworkers. Crowdworkers are instructed to converse naturally with a bot while simultaneously trying to probe or adversarially goad it into replying in an unsafe manner. To encourage the bot to use offensive language more often, humans tended to use either unsafe language themselves in the dialogues, or raise probing questions that are considered inappropriate to ask.",Not discussed,,manual,Yes,"Each bot’s response is tagged as either safe or unsafe by the crowdworker.The unsafe responses into four bins: those that nearly 0% of the population would consider unsafe (according to the annotator’s best guess), and those that &lt10%, &lt;50% or ;= 50% of the population would consider unsafe. After collection, a separate task verifies the labels with 3 other annotators, who annotate which type of offensive language (Hate Speech, Personal Attack, Profanity, and Other offensiveness) each utterance from the adversarial dialogues contains. An utterance is considered safe by the annotator in this task if none of the above was tagged.

To ensure data quality, each annotator was required to pass a test of at least 10 questions covering all types of offensiveness included in the taxonomy defined above and annotators whose accuracy fell below 70% would be removed from the annotation task.

Inter-annotator agreement (IAA) was calculated using Krippendorff’s alpha.",Not discussed,,"78,874 utterances",69274 utterances,2598 utterances,English-speaking annotators located in the United States.,Yes,"The taxonomy is shown in the bot adversarial task instructing crowd workers to probe the bots in the those directions.

Hate Speech: the text that attacks or demeans a group based on race, gender, ethnic origin, religion, disability, age or sexual orientation.

Personal Attack: the text contains rude remarks, insults, threats that are targeting an individual.

Profanity: the text contains profanities such as sexual remarks, swearing and curse wordsalso weakly pejoratives and obscenities such as ’stupid’.

Other Offensiveness: the text is offensive, but it does not contain hate speech, personal attacks or profanity.",Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,Subcategory: personal attack,To a person,No,Non-human
66,No,"(Risch & Krestel, 2018)",Aggression Identification Using Deep Learning and Data Augmentation,"Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)",Germany,"For automatic identification of aggressive posts, the authors propose to augment the training dataset - use machine translation to translate each comment into a foreign language and translate back (e.g. English-to-French-to-English), to introduce language variety. With the larger amount of training data, they are able to train a special deep neural net, which generalizes especially well to unseen data. To further boost the performance, they combine this neural net with three logistic regression classifiers trained on character and word n-grams, and hand-picked syntactic features. This ensemble is more robust than the individual single models. They achieves an F1-score of 60% on both English datasets, 63% on the Hindi Facebook dataset, and 38% on the Hindi Twitter dataset.",Yes,70,25.08.2018,31.08.2022,Yes,Yes,Toxicity and aggressiveness are not clearly distinguished in this paper.,Yes,"Aggressive: overtly aggressive (OAG), covertly aggressive (CAG), and non-aggressive (NAG)",Yes,Github,,https://github.com/julian-risch/TRAC-COLING2018,Yes,"Augmented dataset: English dataset; Hindi dataset; Facebook dataset, Twitter dataset",4,2,"Facebook, Twitter",Aggressiveness,"English, Hindi",Implicitly before 2018 (publication of the original dataset),Implicitly before 2018 (publication of the original dataset),N/A,N/A,"The data collection methods that were used to compile the original dataset are described in Kumar et al. (2018b). The authors of this paper present the data augmentation as following: Machine translating a user comment into a foreign language and then translating it back to the initial language preserves its meaning but results in different wording. For English dataset, they used English to French/German/Spanish to English, and increased the dataset size from 15000 posts to 60000. For Hindi dataset, this methods does not work since the meaning of the initial Hindi post are not preserved after machine translation.",Not discussed,,N/A,Not discussed,,Not discussed,,60000 in the augmented training dataset,60000,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
67,No,"(Pereira-Kohatsu et al., 2019)",Detecting and Monitoring Hate Speech in Twitter,Sensors,"Spain, UK","In this paper, the authors present HaterNet (used by the Spanish National Office Against Hate Crimes of the Spanish State Secretariat for Security) which identifies and monitors the evolution of hate speech in Twitter. The contributions of this research are many-fold: (1) It introduces the first intelligent system that monitors and visualizes, using social network analysis techniques, hate speech in Social Media. (2) It introduces a novel public dataset on hate speech in Spanish consisting of 6000 expert-labeled tweets. (3) It compares several classification approaches based on different document representation strategies and text classification models. (4) The best approach consists of a combination of a LTSM+MLP neural network that takes as input the tweet’s word, emoji, and expression tokens’ embeddings enriched by the tf-idf, and obtains an area under the curve (AUC) of 0.828 on the dataset.",Yes,98,26.10.2019,31.08.2022,Yes,Yes,"Hate speech refers to a kind of speech that denigrates a person or multiple persons based on their membership to a group, usually defined by race, ethnicity, sexual orientation, gender identity, disability, religion, political affiliation, or views.",No,,Yes,Other,Zenodo,https://zenodo.org/record/2592149#.Yx6YnS8Rr0o,Yes,hate speech dataset (HaterNet),3,1,Twitter,Hate Speech,Spanish,XX.02.2017 - XX.12.2017,XX.02.2017 - XX.12.2017,Spain (identified by UTC offset and Spanish),N/A,"A corpus composed of tweets collected at different random dates between February 2017 and December 2017—having a final size of 2M tweets—has been created. This strategy of collecting tweets at different dates is an approach for reducing the problem of the triggering events mentioned by Downs, as more topics and events are captured than in shorter observation periods. The tweets are collected using the Twitter Rest API. In each request, a json file is received with the tweets’ content and metadata, including the UTC offset, which is used to identify the tweets originating from Spain. Once HaterNet was trained and running only the latest tweets written in Spanish are retrieved for their evaluation and monitoring.",Not discussed,,manual,Yes,"To label the corpus, the SNOAHC-SES selected four experts with different backgrounds. Each expert read the whole filtered corpus and indicated for each tweet if they considered it contained hate speech or not according to the definition given. The final label of each tweet has been decided by majority vote. As there is an even number of labelers, it is possible to incur in a tie. In this event, a fifth person (Labeler E, a 49-year-old professor of Computer Science) casts the deciding vote. At the end of this stage, due to time restrictions, 6000 tweets out of the 8710 filtered were labeled. Labelers’ Inter-Rater Agreement were measured by Cohens’ kappa and Fliess’ kappa.",Yes,"The filtering process for selecting the tweets for annotation is shown in Figure 4:  (1) Does the tweet contain word with absolute hate? (If yes, then it is selected for annotationif not, it goes to the second step). (2) Does the tweet contain word with relative hate? (If yes, then it goes to next stepif not, it is not selected) (3) Does it contain generic insults? (If yes, then it is selectedif not, it is not selected)

The filter makes use of seven files. Six of them are dictionaries of words that represent different types of hate speech (i.e., ethnicity, race, gender, disability, politics, and religion), whereas the last one contains generic insults.

Words inside each of the first six files are tagged with two possible degrees of hate: absolute or relative. A word is said to contain absolute hate if the word unequivocally expresses hate, regardless of its context. Otherwise, if hate depends on the context, the word is said to contain relative hate. At this point, each tweet is represented as a vector of elements. Therefore, for this process, each element in the vector is scanned using the first six files of the filter. If the tweet contains at least one element tagged as absolute hate, then it is selected as possible container of hate speech and passes the filter. If, on the other hand, it contains an element tagged as relative hate, then the context of that term is assessed using the seventh file of the filter: if at least one of the elements of the tweet appears in this file, then the tweet is selected as possible container of hate speech and, therefore, passes the filter. Only 8710 tweets out of the original 2m in the raw corpus were selected for labelling.",6000,10-fold cross validation,10-fold cross validation," Labeler A: 44-year-old, public servant.

 Labeler B: 23-year-old, Psychology graduate.

 Labeler C: 24-year-old, Law graduate.

 Labeler D: 23-year-old, Criminology graduate.

 Labeler E: 49-year-old, Professor of Computer Science",Yes,"Each expert read the whole filtered corpus and indicated for each tweet if they considered it contained hate speech or not according to the definition given.

Hate speech refers to a kind of speech that denigrates a person or multiple persons based on their membership to a group, usually defined by race, ethnicity, sexual orientation, gender identity, disability, religion, political affiliation, or views.The Rabat Plan of Action of the United Nations which defines the guidelines to distinguish between free speech and hate speech, recommends differentiating between three types of expressions: “expression that constitutes a criminal offenceexpression that is not criminally punishable, but may justify a civil suit or administrative sanctionsexpression that does not give rise to criminal, civil or administrative sanctions, but still raises concern in terms of tolerance, civility and respect for the rights of others.”",Not discussed,,General,N/A,Targeted,"race, sexuality, gender, disability, religion, political",Yes,No,,,No,,No,,,No,Human
69,No,"(Jacobs et al., 2022)","Automatic classification of participant roles in cyberbullying: Can we detect victims, bullies, and bystanders in social media text?",Natural Language Engineering,Belgium,"This paper aims to automatically detect different participant roles involved in textual cyberbullying traces, including bullies, victims, and bystanders. The authors describe the construction of two cyberbullying corpora (a Dutch and English corpus) that were both manually annotated with bullying types and participant roles and we perform a series of multi-class classification experiments to determine the feasibility of text-based cyberbullying participant role detection. The representative datasets present a data imbalance problem for which we investigate feature filtering and data resampling as skew mitigation techniques. They investigate the performance of feature-engineered single and ensemble classifier setups as well as transformer-based pretrained language models (PLMs).",Yes,5,18.11.2020,31.08.2022,Yes,Yes,…define cyberbullying as content that is published online by an individual and that is aggressive or hurtful against a victim.,Yes,"Types of cyberbullying: threats, insults, defensive statements from a victim, encouragements to the harasser, sexual talk, defamation, etc.  Participant roles in cyberbullying: Harasser, Victim, Bystander-assistant, Bystander-defender.",Yes,Other,OSF,https://osf.io/nb2r3,Yes,English dataset; Dutch dataset,2,2,ASKfm,Cyberbullying,"English, Dutch",April and October 2013,Implicitly before October 2013 (data collection),N/A,N/A,"A Dutch and English corpus were constructed by collecting data from the SNS ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. ASKfm data typically consist of question–answer pairs published on a user’s profile. The data were retrieved by crawling a number of seed profiles using the GNU Wget software in April and October 2013. Non-English and non-Dutch posts were removed, which resulted in 113,698 and 78,387 posts for English and Dutch, respectively.",Not discussed,,manual,Yes,"In formulating an annotation scheme for supervised classification of participant roles, annotators were asked to infer the bullying role as an illocutionary act by means of the information present in the ASKfm corpus. The “outsider” role has been left out, given that passive bystanders are impossible to recognize in online text because they do not leave traces.

The annotation scheme describes two levels of annotation. Firstly, the annotators were asked to indicate, at the post level, whether the post under investigation contained traces of cyberbullying. If so, the annotators identified the author’s role as one out of the four mentioned above. Secondly, at the subsentence level, the annotators were tasked with the identification of a number of fine-grained categories related to cyberbullying. More concretely, they identified all text spans corresponding to one of the categories described in the annotation scheme. To provide the annotators with some context, all posts were presented within their original conversation when possible. All annotations were done using the Brat rapid annotation tool (Stenetorp et al. 2012).

To demonstrate the validity of our annotations, IAA scores were calculated using Kappa (κ) on a subset of the English and Dutch ASKfm corpus.",Not discussed,,"113,694 (English) and 78,387 (Dutch) posts",5-fold cross validation,"5-fold cross validation; 10,000 random corpus samples for holdout test",Trained linguists after supervised instruction and practice with the guidelines. All were Dutch native speakers or English second-language speakers.,Yes,"For types of cyberbullying, the guidelines used to annotate our corpora describe specific textual categories related to cyberbullying, including threats, insults, defensive statements from a victim, encouragements to the harasser, sexual talk, defamation, etc. (The authors refer to Van Hee et al. 2015c for a complete overview)

For participant roles in cyberbullying, four cyberbullying roles were annotated in both corpora:

Harasser: person who initiates the harassment, that is the bully.

Victim: person who is harassed.

Bystander-assistant: person who does not initiate but takes part in the actions of the harasser.

Bystander-defender: person who helps the victim and discourages the harasser from continuing.",Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,"The “Victim"" in participant roles",To a person,No,Human
71,No,"(Zhang et al., 2018)",Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network,The Semantic Web,UK,"For the task of hate speech detection, this paper introduces a new method based on a deep neural network combining convolutional and gated recurrent networks. The authors conduct an extensive evaluation of the method against several baselines and state of the art on the largest collection of publicly available Twitter datasets to date and show that compared to previously reported results on these datasets. The proposed method is able to capture both word sequence and order information in short texts, and it sets new benchmark by outperforming on 6 out of 7 datasets by between 1 and 13% in F1. We also extend the existing dataset collection on this task by creating a new dataset covering different topics (subjects of religion and refugees).",Yes,475,03.06.2018,31.08.2022,Yes,Yes,"…identify that hate speech (1) targets individual or groups on the basis of their characteristics (targeting characteristics); (2) demonstrates a clear intention to incite harm, or to promote hatred; (3) may or may not use offensive or profane words.",No,,Yes,Github,,https://github.com/ziqizhang/chase/tree/master/data,Yes,RM (Refugees and Muslims) dataset,7,1,Twitter,Hate Speech,English,Implicitly before 2018 (publication),Implicitly before 2018 (publication),N/A,Finsbury park attack; Manchester and London Bridge attacks,"The authors started with using the Twitter Streaming API to collect tweets containing any of the following words for a period of 7 days: muslim, islam, islamic, immigration, migrant, immigrant, refugee, asylum. This created a corpus of over 300,000 tweets (duplicates and retweets removed), from which they randomly sampled 1,000 for annotation (batch 1). However, tweets annotated as hate speech were extremely rare (&lt;1%). They then manually inspected the annotations and further filtered the remaining tweets (disjoint with batch 1) by the following words found to be frequent for hate speech: ban, kill, die, back, evil, hate, attack, terrorist, terrorism, threat, deport. They then sampled another 1,000 tweets (batch 2) from this collection for annotation. The amount of true positives was still very low (1.1%). They then created another batch (batch 3) by using the Twitter Search API to retrieve another 1,500 tweets with the following hashtags considered to be strong indicators of hate speech: #refugeesnotwelcome, #DeportallMuslims, #banislam, #banmuslims, #destroyislam, #norefugees, #nomuslims. The dataset however, contains over 400 tweets after removing duplicates, and about 75% were annotated as hate speech. Finally they merge all three batches to create a single dataset.",Not discussed,,manual,Yes,"All tweets are annotated for two classes: hate and non-hate, firstly by a computational linguistic researcher and then cross-checked by a student researcher. Disputed annotations were then discussed and corrected to ensure both agree with the correction.",Yes,"Batch 1: They randomly sampled 1,000 from the whole corpus for annotation.

Batch 2: They manually inspected the annotations and further filtered the remaining tweets (disjoint with batch 1) by the following words found to be frequent for hate speech: ban, kill, die, back, evil, hate, attack, terrorist, terrorism, threat, deport. They then sampled another 1,000 tweets from this collection.

Batch 3: They used the Twitter Search API to retrieve another 1,500 tweets with the following hashtags considered to be strong indicators of hate speech: #refugeesnotwelcome, #DeportallMuslims, #banislam, #banmuslims, #destroyislam, #norefugees, #nomuslims. The duplicates were then removed.

They then merged the three batches to create a single dataset.",2435,Within 75% of the data: 5-fold cross-validation,Within 75% of the data: 5-fold cross-validation;,"Computational linguistic researcher, cross-checked by a student researcher.",Yes,"The authors refer to (Waseem & Hovy, 2016: Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter) for the annotation guidelines.",Not discussed,,Specific,"religion, other",Non-targeted,N/A,Yes,Yes,Religion,religion,Yes,"Refugees, Muslims",No,,,No,Human
72,No,"(Sigurbergsson & Derczynski, 2020)",Offensive Language and Hate Speech Detection for Danish,Proceedings of the 12th Language Resources and Evaluation Conference,Denmark,"The authors construct a Danish dataset DKhate containing user-generated comments from various social media platforms, annotated for various types and target of offensive language. They develop four automatic classification systems, each designed to work for both English and Danish. The work captures the type and targets of offensive language, and present automatic methods for detecting different kinds of offensive language such as hate speech and cyberbullying. In the detection of offensive language in English, the best performing system achieves a macro averaged F1-score of 0.74, and the best performing system for Danish achieves a macro averaged F1-score of 0.70. In the detection of whether or not an offensive post is targeted, the best performing system for English achieves a macro averaged F1-score of 0.62, while the best performing system for Danish achieves a macro averaged F1-score of 0.73. Finally, in the detection of the target type in a targeted offensive post, the best performing system for English achieves a macro averaged F1-score of 0.56, and the best performing system for Danish achieves a macro averaged F1-score of 0.63.",Yes,106,11.05.2020,31.08.2022,Yes,Yes,Offensive posts include insults and threats as well as any form of untargeted profanity,Yes,"Type of offensive language: targeted, untargeted Target of offensive language: Individual, Group, Other",Yes,Other,paperwithcode,https://paperswithcode.com/dataset/dkhate,Yes,DKhate (DA),2,1,"Facebook, Reddit","Offensiveness, Hate Speech","Danish, English",Implicitly before 11.05.2020 (publication date),Implicitly before 11.05.2020 (publication date),N/A,N/A,"The authors collected randomly selected user-generated comments from Ekstra Bladet’s public page on Facebook. This was done manually due to restrictions on scraping public pages with Facebook. They then scraped Reddit, collecting the top 500 posts from the Danish sub-reddits r/DANMAG and r/Denmark, as well as the user comments contained within each post.",Yes,"Personally identifying content (such as the names of individuals, not including celebrity names) was removed. This was handled by replacing each name of an individual (i.e. author or subject) with @USER. All comments containing any sensitive information were removed. We classify sensitive information as any information that can be used to uniquely identify someone by the following characteristics; racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, and bio-metric data.",manual,Yes,"Offensive content is broken into three sub-tasks.

Sub-task A - Offensive language identification. Each sample is annotated with one of the following labels: Not Offensive, Offensive.

Sub-task B - Automatic categorization of offensive language types. Only posts labeled as offensive (OFF) in sub-task A are considered in this task. Each sample is annotated with one of the following labels: Targeted Insult, Untargeted.

Sub-task C - Target identification. Only posts labeled as targeted insults (TIN) in sub-task B are considered in this task (Zampieri et al., 2019b). Samples are annotated with one of the following: Individual, Group, Other.

As a warm-up procedure, the first 100 posts were annotated by two annotators and the results compared. This exercise was used to refine the understanding of the task at hand and to discuss the mismatches in these annotations for each sub-task.",Yes,"The authors published a survey on Reddit asking Danish speaking users to suggest offensive, sexist, and racist terms for a lexicon. This gave 113 offensive and hateful terms which were used to find offensive comments. This Danish lexicon was used to filter the social media comments to find potentially-offensive comments. The remainder of comments in the corpus were shuffled and a subset of this corpus was then used to fill the remainder of the final dataset, ensuring that the data would have significant coverage beyond just terms found in the lexicon. The resulting dataset contains 3600 user-generated comments, 800 from Ekstra Bladet on Facebook, 1400 from r/DANMAG and 1400 from r/Denmark.",3600,2879,721,"Age: 25-40.

Gender: male.

Race/ethnicity: white northern European.

Native language: Icelandic, English.

Socioeconomic status: higher education student / university faculty",Yes,The authors base the annotation procedure on the guidelines and schemas presented in Zampieri et al. (2019a). The guidelines were refined and made more strict after the analysis of the disagreements in the warm-up around of the first 100 posts.,Not discussed,,Specific,other,Non-targeted,N/A,Yes,Yes,"Group-targeted, not specified",other,Yes,"Group-targeted, not specified",Yes,"Individual-targeted, not specified",About a person,No,Human
74,No,"(Setiawan et al., 2018)",Supervised Learning Model for Combating Cyberbullying: Indonesian Capital City 2017 Governor Election Case,Intelligent Systems Design and Applications,Indonesia,This research develops a cyberbullying corpus in Bahasa Indonesia in order to develop a supervised learning algorithm. The data have been crawled from the social media during the Indonesian capital city 2017 governor election event where the cyberbullying between the candidates supporters occurred. A set of the cyberbullying feature space is designed and checked by a linguist. The feature space then has been used for collecting data for training and testing dataset as well as developing the rules for the supervised learning algorithm.,Yes,1,22.03.2018,31.08.2022,Yes,Yes,"Cyberbullying is the use of cell phones, instant messaging, e-mail, chat rooms or social networking sites to harass, threaten or intimidate someone.",No,,No,,,N/A,No,Cyberbullying corpus; the new dataset,2,2,"Twitter, Facebook",Cyberbullying,Indonesian,XX.02.2017 - XX.04.2017,XX.02.2017 - XX.04.2017,N/A,Indonesian 2017 capital city governor election,"The data collection happened during the campaign period of Indonesian 2017 capital city governor election. The dataset was made up of crawled Tweeter and Facebook users’ status using the R Studio application. The dataset not only contains the texts but also includes the emoji, hashtag, punctuation marks. Filters have been set in the R studio as “Pilkada” (capital city election), “Jakarta”, “debat pilkada” (candidates debate), “pemilihan” (election), “kampanye” (campaign), and the governor as well as the vice governor candidates’ name. The crawling process has collected 10000 tweets in the period of February to April 2017.

The ones that classified as bullying (200 tweets out of 250 selected tweets) then assessed further in order to design the feature space. Further, the designed feature space has been used to crawl more for a new dataset.",Not discussed,,manual,Not discussed,,Yes,"The number of the tweets was down to 5000 after both automatic redundancy filtering and manual redundancy check by the linguist. From the 5000 provided tweets, the linguist selected 250 tweets and manually classified them into bullying or not bullying",250,60% in the new dataset,40% in the new dataset,Expert/linguist,Yes,"From the manual classification by the linguist, five rules of how a cyberbullying is formed have been established. A sentence can be classified as a cyberbullying (1) if there are two or three different types of feature space appear in a sentence, (2) if there is only one type of feature space but appears, (3) when two or more combination of emoji, punctuation and hashtag feature space are combined together more than one times, or (4) if one of these three feature spaces appears more than one times.",Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
76,No,"(Aggarwal et al., 2020)",Comparative Study for Predicting the Severity of Cyberbullying Across Multiple Social Media Platforms,2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS),India,"This paper aims to identify instances of cyberbullying across various social media platforms and classify them based on the level of severity of cyberbullying. The authors wish to provide a comparative study of various traditional machine learning and deep learning models that can be used for the above objective. They also wish to release 3 datasets that are annotated in 4 labels based on the severity of cyberbullying (none, low, medium, and high). The paper aims to work on social media platforms trending amongst the youth, like Reddit, Twitter, and Formspring.",Yes,5,13.05.2020,31.08.2022,Yes,Yes,"Cyberbullying is a kind of harassment that occurs over digital mediums like mobiles, desktops, or tablets.",Yes,"Cyberbullying: Low cyberbullying (L), Medium cyberbullying (M), High cyberbullying (H)",Yes,Github,,https://github.com/Kavita309/Severity-of-cyberbullying-across-SMPs,Yes,Formspring dataset; Twitter dataset; Reddit dataset,3,3,"Formspring, Twitter, Reddit",Cyberbullying,English,Implicitly before 13.05.2020 (publication date),Implicitly before 13.05.2020 (publication date),N/A,N/A,N/A,Not discussed,,manual,Not discussed,,Not discussed,,Formspring dataset (1006); Twitter dataset (3120); Reddit dataset (3002),N/A,N/A,Students (after careful research and understanding of what falls into the category of cyberbullying and comprehending the severity of cyberbullying present in the posts),Yes,"The dataset was labelled into 4 categories namely:

None ( No cyberbullying): None was assigned to the data which had no trace of cyberbullying in it.

Low cyberbullying (L): Low was given to the data which consisted of mildly offensive or mildly inappropriate content, which can/cannot be taken as cyberbullying by the victim.

Medium cyberbullying (M): The data consisting of threats, offensive content, and racist/sexist/dirty/abusive remarks targeting an individual or a community were adjudged as ‘medium’.

High cyberbullying (H): The instances where there were death threats and extremely intolerable comments about someone’s physical looks or mental faculties or sexual harassment which made the comment completely unacceptable as per the society standards were all labeled as ‘High.’",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
78,No,"(Founta et al., 2018)",Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior,Twelfth International AAAI Conference on Web and Social Media,"Greece, Cyprus, USA, UK","The authors present an eight month study of the various forms of abusive behavior on Twitter, in a holistic fashion. Departing from past work, they examine a wide variety of labeling schemes, which cover different forms of abusive behavior. They propose an incremental and iterative methodology that leverages the power of crowdsourcing to annotate a large collection of tweets with a set of abuse-related labels. By applying the methodology and performing statistical analysis for label merging or elimination, they identify a reduced but robust set of labels to characterize abuse-related tweets. Finally, they offer a characterization of our annotated dataset of 80 thousand tweets, which is publicly available for further scientific exploration.",Yes,379,15.06.2018,31.08.2022,Yes,Yes,"Abusive Language: Any strongly impolite, rude or hurtful language using profanity, that can show a debasement of someone or something, or show intense emotion. In the dataset, Abusive, Offensive and Aggressive were merged into a single category Abusive.
Hate Speech: Language used to express hatred towards a targeted individual or group, or is intended to be derogatory, to humiliate, or to insult the members of the group, on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.",Yes,"Inappropriate speech: Abusive, Hateful, Spam",Yes,Github,,https://github.com/ENCASEH2020/hatespeech-twitter,Yes,the large-scale annotated dataset,1,1,Twitter,"Abusiveness, Hate Speech",English,30.03.2017 - 09.04.2017,30.03.2017 - 09.04.2017,N/A,N/A,"The authors utilize the Twitter Stream API and collect all the tweets provided by the API (1% of the entire traffic) during the period of 30th March 2017 - 9th April 2017, consisting of 32 million tweets in total. From the tweet’s content they extract the number of URLs, hashtags, mentions, emojis/smilies, and numerals. They also tag retweets and mentions. Finally, we extract metadata from Twitter, such as the detected language, the account age, etc. Additionally, we apply sentiment analysis, such as polarity and subjectivity of the tweet, using the TextBlob Python library. They count the number of offensive terms found in the collected tweets using two dictionaries (HateBase and an offensive words dictionary).

These tweets are removed: (1) spam, applying filtering criteria for the elimination of such spam-related tweets(2) non-original tweets (e.g., retweets without new content)(3) tweets with only small text content (e.g., only URLs, images, etc.)(4) tweets not written in English, using Twitter’s language detection.",Yes,"Since a number of the tweets are not available anymore for download on Twitter, the authors also provide one more file with the full 100k tweet text and their associated majority labels. The tweets are shuffled so that there is no connection between tweet IDs and texts (in order to be aligned with the T&C of Twitter). The file is available at [this Zenodo page](https://zenodo.org/record/3706866#.YjzZfDUReUk). (**[README.md](https://github.com/ENCASEH2020/hatespeech-twitter#readme)**)",manual,Yes,"The authors introduce an iterative process of annotation.

(1) Exploratory Rounds were conducted in order to tune the crowdsourcing parameters on a smaller dataset, in order to quickly get some insights but minimize the cost.

The first round includes 300 tweets and 5 judgments per tweet. They begin with the most extensively used labels found in literature: Offensive, Abusive, Hateful, Aggressive, Cyberbullying, Spam, Normal. The payment start with a default scheme (5¢ for a batch of 10 tweets). Annotators were asked in a primary selection, to first classify tweets into three general categories: normal, spam, and inappropriate. In the case that inappropriate was selected, then a secondary panel offered them the five aforementioned inappropriate speech categories. Furthermore, they had the option to suggest a new subcategory utilizing the “other” option and a text box. The participants were encouraged to select multiple subcategories whenever appropriate.

In the second round, they use only the tweets that were previously annotated as Inappropriate, with a high agreement score. In total, these tweets are 88 out of the initial 300. Each tweet was consequently annotated by at least 10 workers, but usually around 20. We kept the same setup regarding labels and instructions.

They then proceed with extra validation rounds, before the large-scale annotation. They removed Cyberbullying and merged Abusive, Offensive and Aggressive into a single category Abusive. The 4 labels used in this round are Abusive, Hateful, Normal, and Spam.

(2) Large Scale Annotation of 80k tweets, with 5 judgments per tweet, were launched based on previous insights. The setup for this round is kept the same with the last validation round. They built our own custom platform to host the annotation task. To accommodate such a large-scale task, they also created a database schema to store the data and the results, and to calculate the statistics.",Yes,"They count the number of offensive terms found in the collected tweets using two dictionaries (HateBase and an offensive words dictionary). For the boosted sample, they use the metadata extracted earlier. They choose tweets that, based on the sentiment analysis, show strong negative polarity (&lt; −0.7) and contain at least one offensive word. Two datasets: D1 is a sample of just 300 tweets that is used for the exploratory analysis, and D2 that contains 80K tweets that will be used for the final annotation.",~80k (~100k in the updated version),N/A,N/A,CrowdFlower workers,Yes,"Before starting, annotators are provided with definitions for each label which they have to acknowledge reading. The definitions are constructed based on all the descriptions the authors found in the related literature, as cited on each category, as well as Cambridge and Black’s Law dictionaries. In total, the following definitions were displayed to the annotators:

Offensive Language: Profanity, strongly impolite, rude or vulgar language expressed with fighting or hurtful words in order to insult a targeted individual or group.

Abusive Language: Any strongly impolite, rude or hurtful language using profanity, that can show a debasement of someone or something, or show intense emotion.

Hate Speech: Language used to express hatred towards a targeted individual or group, or is intended to be derogatory, to humiliate, or to insult the members of the group, on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.

Aggressive Behavior: Overt, angry and often violent social interaction delivered via electronic means, with the intention of inflicting damage or other unpleasantness upon another individual or group of people, who perceive such acts as derogatory, harmful, or unwanted.

Cyberbullying Behavior: It’s the use of force, threat, or coercion to abuse, embarrass, intimidate, or aggressively dominate others, using electronic forms of contact. It typically denotes repeated and hostile behavior performed by a group or an individual.

Spam: Posts consisted of related or unrelated advertising / marketing, selling products of adult nature, linking to malicious websites, phishing attempts and other kinds of unwanted information, usually executed repeatedly.

Normal: all tweets that do not fall in any of the prior categories.

(In the large-scale annotation, Abusive, Offensive and Aggressive were merged into a single category Abusive. )",Yes,The payment starts with a default scheme (5¢ for a batch of 10 tweets),General,N/A,Targeted,"race, religion, sexuality, disability, gender",Yes,No,,,No,,No,,,No,Human
80,No,"(Ravikiran & Annamalai, 2021)",DOSA: Dravidian Code-Mixed Offensive Span Identification Dataset,Association for Computational Linguistics,"India, USA","This paper presents the Dravidian Offensive Span Identification Dataset (DOSA) for under-resourced Tamil-English and Kannada-English code-mixed text. The dataset addresses the lack of code-mixed datasets with annotated offensive spans by extending annotations of existing code-mixed offensive language identification datasets. It provides span annotations for Tamil-English and Kannada-English code-mixed comments posted by users on YouTube social media. Overall the dataset consists of 4786 Tamil-English comments with 6202 annotated spans and 1097 Kannada-English comments with 1641 annotated spans, each annotated by two different annotators. The authors further present some of our baseline experimental results on the developed dataset, thereby eliciting research in under-resourced languages, leading to an essential step towards semi-automated content moderation in Dravidian languages.",Yes,5,20.04.2021,31.08.2022,Yes,Yes,Offensiveness,No,,Yes,Github,,https://github.com/teamdl-mlsg/DOSA,Yes,"Kannada-English dataset, Tamil-English dataset",2,2,YouTube,Offensiveness,"Tamil-English, Kannada-English",implicitly before 2020 (publication of the original dataset),implicitly before 2020 (publication of the original dataset),N/A,N/A,"The authors reuse TamilMixSentiment and KanCMD datasets consisting of 15k and 7k YouTube code-mixed comments respectively in Tamil-English and Kannada-English languages. For the final annotation process, they considered all of the code-mixed Kannada-English comments (1311) and 5000 out of 9049 Tamil-English comments that were already annotated as offensive in the original datasets.",Yes,"They anonymized all the personal information and user-related tags to protect actual users’ privacy during our annotation process. Besides, no personal information of annotators was collected except their educational background and expertise in the language they volunteered to annotate.",manual,Yes,"To start with, a total of 15 annotators were selected. All of them had minimal education of Bachelors Degree with either medium of schooling to be one of the English, Tamil, and Kannada languages or proficient in both speaking and writing of one or both the Dravidian languages. Further, the annotation was done iteratively in a cycle of 500 sentences where each of the annotators was asked to report back to verify the quality of annotations and receive their next batch of 500 sentences. Each batch was manually verified by an annotation verifier. Annotators who did not annotate well or had a significant delay in annotations were removed. At the end of this process, there are six annotators, out of which all of the annotators were native speakers or writers of either Kannada or Tamil or both. Also, two of the annotators acted as annotation verifiers. Each YouTube comment was initially sent to two annotators for span annotation without revealing that the comment was offensive. If there was a disagreement in annotation, then the comment was sent to the third annotator. If all the three disagreed, the annotation of that particular comment was skipped. Overall this leads to the annotation of each comment by two annotators.",Yes,"They considered only a subset of the comments that were already annotated as offensive in the original datasets for the span annotation process. Out of this subset, they rechecked and removed non-code mixed comments resulting in 9049 and 1311 comments in Tamil-English and Kannada-English, respectively. For the final annotation process, they considered all of the code-mixed Kannada-English comments and 5000 Tamil-English comments.",4786 Tamil-English comments with 6202 annotated spans; 1097 Kannada-English comments with 1641 annotated spans,3-fold cross-validation,3-fold cross-validation,"Educational Background: Bachelor, Master;

Medium of Schooling: English, Tamil, or Kannada",Yes,"The annotators have explained the meaning of offensiveness with illustrative examples. Annotators who agreed that they understood this were given the following instructions:

1. Extract the offensive word sequences (spans) of the comment by highlighting each such
    

span and labeling them as CAUSE.

2. If the comment is not offensive or if the offensiveness is context-dependent, do not high-

light any span.

3. If the whole comment should be annotated, then annotate the whole comment and convey

the annotation verifier about the same after completion.",Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
81,No,"(Chiril et al., 2021)",“Be nice to your wife! The restaurants are closed”: Can Gender Stereotype Detection Improve Sexism Classification?,Association for Computational Linguistics,France,"In this paper, the authors focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. They propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation based on sentence similarity with multilingual external datasets, and (3) a set of deep learning experiments first to detect gender stereotypes and then, to use this auxiliary task for sexism detection. Although the presence of stereotypes does not necessarily entail hateful content, the results show that sexism classification can definitively benefit from gender stereotype detection.",Yes,1,07.11.2021,31.08.2022,Yes,Yes,"Gender stereotypes (GS hereafter) defined by the Office of the High Commissioner for Human Rights as ""a generalised view or preconception about attributes, or characteristics that are or ought to be possessed by women and men or the roles that are or should be performed by men and women""",Yes,"Categories of stereotypes: Physical characteristics, Behavioural characteristics, and Activities",Yes,Github,,https://github.com/patriChiril/An-Annotated-Corpus-for-Gender-Stereotype-Detection-in-French-Tweets,Yes,"StereoO, Stereoaug",2,2,Twitter,Gender Stereotype,French,Implicitly before 07.11.2021 (publication date),Implicitly before 07.11.2021 (publication date),N/A,N/A,"(1)StereoO: The Original Dataset
The authors used a non-annotated subset of 9,282 French tweets from the available corpus collected by (Chiril et al., 2020a) which contains 115,000 tweets collected using: (i) a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc., (ii) the names of women/men potentially victims or guilty of sexism (mainly politicians), (iii) specific hashtags to collect stories of sexism experiences (#balancetonporc, #sexisme, etc.). Given a tweet, its annotation consists in assigning it at least one of the following categories: physical characteristic, behavioural characteristic, activity and non-stereotype (the first 3 categories are not mutually exclusive). A tweet is annotated as ""non-stereotype"" when it does not contain a stereotype. 1,000 tweets have been annotated by both annotators so that the inter-annotator agreement could be computed.

(2) Stereoaug: The Augmented Dataset

They propose a new approach for data augmentation based on sentence similarity. SentenceBERT (a modification of BERT) is used to derive semantically sentence embeddings that can be compared using cosine-similarity (Reimers and Gurevych, 2019). The similar sentences are collected from two sources: (S1) New tweets in French collected with a small set of keywords usually used in stereotypes about women: moche (ugly), fesses (butt), jupe (skirt), bavarde (gossipy), dépensière (spendthrift), dévouée (devoted), infirmière (nurse), poupée (doll). These keywords are different from those used for the initial data collectionand (S2) New tweets from existing multilingual datasets annotated for stereotypes.

The initial training corpus is extended in two ways:

(a) Augmenting with multilingual instances annotated as stereotypes from AMI (English, Italian, Spanish) and the English sexism corpus (Parikh et al., 2019).

(b) Augmenting with the most similar instances to the ones labelled as stereotype in the corpus as given by SentenceBERT. To this end, they consider the aforementioned corpora. The dataset augmented via similarity from the English IberEval lead to best results. This is the one  used hereafter (Stereoaug).

For both sources of augmentation, a threshold T was set experimentally and the most similar instances from IberEval dataset and new collected tweets were automatically labelled as stereotype and added to the training dataset.

Finally, Stereoaug is now composed of 4,891 tweets which represents an augmentation of about 45% of the initial corpus.",Yes,"The authors do not make any claims about the authors of the tweets, neither share a large numbers of tweets from the same users. Additionally, if any of the users want to opt out from having their data being used for research, they can request that they be removed from the dataset by sending an email to the authors of this paper.","automated, manual",Yes,"(1) StereoO: The Original Dataset
Given a tweet, its annotation consists in assigning it at least one of the following categories: physical characteristic, behavioural characteristic, activity and non-stereotype (the first 3 categories are not mutually exclusive). A tweet is annotated as ""non-stereotype"" when it does not contain a stereotype. 1,000 tweets have been annotated by both annotators so that the inter-annotator agreement could be computed. After a training stage, two native French speaking annotators annotated the corpus. 1,000 tweets have been annotated by both annotators so that the inter-annotator agreement could be computed (Kappa=0.79).

(2) Stereoaug: The Augmented Dataset
In the process of augmentation, a threshold T was set experimentally and the most similar instances from IberEval dataset and new collected tweets were automatically labelled as stereotype.",Yes,"To create the initial dataset (StereoO), the authors used a non-annotated subset of 9,282 French tweets from the available corpus collected by (Chiril et al., 2020a) which contains 115,000 tweets collected using: (i) a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc., (ii) the names of women/men potentially victims or guilty of sexism (mainly politicians), (iii) specific hashtags to collect stories of sexism experiences (#balancetonporc, #sexisme, etc.).","(1) StereoO: 9,282; (2) Stereoaug: 4,891","80% of the initial dataset (*˜*7426) + all new augmented instances (4,891)",20% of the initial dataset (˜1856),"Two French native speakers, one male and one female, both master’s degree students in Linguistics, Communication and Gender",Yes,"(1) Physical characteristics are related to physical strength or aspect. For example, the message Short hair for a girl it’s a bad idea conveys the stereotype ""Girls must have long hair"".

(2) Behavioural characteristics are related to intelligence, emotions, sensibility or behaviour as in the denouncing tweet Am I supposed to recognize myself in the ""Just Fab"" ad with a screaming hysterical bitch?.

(3) Activities are activities, jobs, hobbies that are stereotypically assigned to women as in Never marry a woman who cannot cook which implies that a woman’s place is in the kitchen, or no woman understands football.",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Gender,gender,Yes,Women,No,,,No,Human
82,No,"(Solorio et al., 2014)",Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive Writing for Linking Identities,Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14),USA,"This paper describes a corpus of sockpuppet cases from Wikipedia. A sockpuppet is an online user account created with a fake identity for the purpose of covering abusive behavior and/or subverting the editing regulation process. The authors used a semi-automated method for crawling and curating a dataset of real sockpuppet investigation cases, the first corpus available on real-world deceptive writing. They describe the process for crawling the data and some preliminary results that can be used as baseline for benchmarking research.",Yes,33,XX.05.2014,31.08.2022,Yes,Yes,Sockpuppet: When a user creates a secondary account for malicious purposes it is called a sockpuppet.,Yes,"Sockpuppet Investigations (SPI): Confirmed SPI cases, Denied SPI cases  Created non-sock puppet cases",Yes,Website,,https://www.uab.edu/cas/cis/tools-and-datasets/,No,The Sockpuppet Corpus,1,1,Wikipedia,Sockpuppet,English,Implicitly before 05.2014 (publication date),Implicitly before 05.2014 (publication date),N/A,N/A,"All the data we collected from Wikipedia is readily available from the Wikipedia website. The process for data collection is semi-automated. The sockpuppet cases are collected / crawled from the following urls:

• https://en.wikipedia.org/wiki/Wikipedia:Sockpuppet_investigations/SPI/Closed/2009

• https://en.wikipedia.org/wiki/Wikipedia:Sockpuppet_investigations/SPI/Closed/2010

• http://en.wikipedia.org/w/index.php?title=Wikipedia:Sockpuppet_investigations/Cases/Overview&offset=&limit=500&action=history

For each case selected for inclusion in the corpus,  the authors collect all data from the talk pages of each editor involved in the SPI case. This step is done automatically by crawling the corresponding Wikipedia archives. Only data from discussion pages are included, since they show more stylistic writing markers.

The manual process for this task involves retrieving the final decision reached by the investigative administrator or check user. The authors visited each SPI case and read the discussion of any administrators investigating the case and check users involved.

To provide a larger number of non-sockpuppet cases, they crawled pairs of editors that have not been involved in SPI before but that have participated in the same talk pages as editors involved in SPI cases.

They originally collected around 700 cases, but after manual inspection we removed about 80 cases where editors did not have content on the talk pages. These were editors that just made contributions directly to Wikipedia pages but did not engage in any side discussions. The resulting corpus currently has 623 cases where 305 of them were confirmed SPI cases by Wikipedia administrators or check users. The remaining 318 are non-sockpuppet cases that combine 105 SPI cases where the administrators verdict was negative, and 213 cases we created from other editors.",No,,manual,Not discussed,,Not discussed,,623 cases,10 fold cross-validation,10 fold cross-validation,"Once a case (to request investigation) is filed, an administrator will investigate the case. An administrator is an editor with privileges to make account management decisions, such as banning an editor. The administrator performs a behavioral evidence investigation and will try to determine whether the two accounts are related and will then issue a decision confirming or rejecting the sockpuppetry case, or request involvement of a check user. Check users are higher privileged editors, who have access to private information regarding editors and edits, such as the IP address from which an editor has logged in. Check users perform a technical evidence investigation.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
83,No,"(Bozyiğit et al., 2021)",Cyberbullying detection: Utilizing social media features,Expert Systems with Applications,Turkey,"In this study, it is aimed to present the importance of social media attributes in cyberbullying detection. Firstly, a balanced dataset consisting of 5000 labeled contents with many social media features were prepared. Then, the relationship between social media features and cyberbullying were analyzed using the chi-square test. It is seen that some features (e.g., sender followers) are strongly related to online bullying events according to the test results. For instance, users that have more followers on social networks are disinclined to post online bullying content. Then, machine learning algorithms experimented on two different variants of the prepared datasets. The first variant includes only textual features whereas the second variant consists of the determined social media features and textual features. It is observed that each experimented machine learning algorithm give more successful prediction performance on the variant containing social media features. The obtained results motivate doing further research about social media characteristics in cyberbullying.",Yes,16,01.10.2021,31.08.2022,Yes,Yes,"Cyberbullying, which is defined as bullying a person or a group of people using digital tech­nologies ",No,,Yes,Other,Mendeley Data,https://data.mendeley.com/datasets/xf9ck7ntbs/1,Yes,"The dataset has two variants: D(T) that only includes textual features, and D(T+S) that consists of the determined social media features and textual features.",2,2,Twitter,Cyberbullying,Turkish,N/A,N/A,N/A,N/A,"The software application was developed in the n-tier design architecture to collect data regularly from the official Twitter Application Programming Interface. The developed software consists of three tiers called Scheduled Task, Api Controller, Cyberbullying Context.

Scheduled Task is a console application that was developed for running it regularly on a Windows operating system as a task. The task was set to run every 15 min for the defined days. The only mission of this task is calling Api Controller periodically.

Api Controller is a class library developed for collecting data from official Twitter Api and processing the collected data. This class library mainly consists of two methods. The first method is SetCrediantals which establishes a connection between the client and the official Twitter Application Programming Interface. The second method is CollectRecentTweets that collects Turkish Tweets posted in the last fifteen minutes from Twitter API. Additionally, CollectRecentTweets processes the collected data and sends them as models (User, Tweet, Mention) to Cyberbullying Context.

Cyberbullying Context, a class library using entity framework , was developed to retrieve and return models from the relational database. ",Not discussed,,manual,Yes,"A web application where multi-user can label records simulta­neously was developed by using the Model-View-Controller pattern. When a user (annotator) logs in the developed web application, random tweets from the Cyberbullying Context are presented to the user. Accordingly, the user determines whatever the presented tweet is cyberbullying or not. Additionally, users can mark a given content as undesirable content to prevent adding uncertain or political issues to the dataset. 

The activity diagram of the user annotation process is presented in the paper. (see Fig. 6)

The annotation process was completed when 5000 (the desired dataset size) tweets that include at least two of the same annotations from different users were obtained. These 5000 tweets, which include the same annotations, were selected for the dataset to be used in the machine learning methods. ",Yes,"Firstly, approximately one third of the collected tweets which are non-original (retweet or spam) were eliminated. Then, the original 83,025 tweets were flagged in terms of cyberbullying by applying the previously trained classifier proposed in our previous work (Bozyigit et al., 2019). According to the used classifier, 3612 tweets were flagged as potential cyberbullying contents. Then, nearly 3400 random non-flagged tweets with 3612 flagged tweets were kept, and the rest of the samples were eliminated. Thus, the dataset size was reduced to 7000 records.",5000 tweets,10-fold cross-validation,10-fold cross-validation,Three data scientists with a master's degree from computer science-related departments.,Yes,The definition of cyberbullying was explained to these users with the examples that occurred in social networks. ,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
84,No,"(Ahammad et al., 2021)",Identification of Abusive Behavior Towards Religious Beliefs and Practices on Social Media Platforms,International Journal of Advanced Computer Science and Applications,"Bangladesh, Australia","This paper aims to propose an abusive behavior detection approach to identify hatred, violence, harassment, and extremist expressions against people of any religious belief on social media. For this, first religious posts from social media users’ activities are captured and then the abusive behaviors are identified through a number of sequential processing steps. In the experiment, Twitter has been chosen as an example of social media for collecting dataset of six major religions in English Twittersphere. In order to show the performance of the proposed approach, five classic classifiers on n-gram TF-IDF model have been used. Besides, Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) classifiers on trained embedding and pre-trained GloVe word embedding models have been used. The experimental result showed 85% accuracy in terms of precision.",Yes,1,28.04.2021,31.08.2022,Yes,Yes,"Abusive attitudes towards religion on social media…refer to those activities religiously abusive that represents hatred, violence, harassment and extremist expressions against people of any religion or community",Yes,"abusive, non-abusive, neutral",No,,,N/A,No,Tweet Dataset,1,1,Twitter,"Religious Abusive Behavior, Religious Hatred",English,Implicitly before 28.04.2021 (publication),Implicitly before 28.04.2021 (publication),N/A,N/A,"The authors retrieved total 9,787 publicly available users’ Tweet related to English language and letters using Twitter’s search API method with Python Tweepy library based on a predefined list of keywords of religious beliefs:

Christianity: Christian, Roman Catholic, Christianity, Apocalypticism, Catholic Church, Baptism, Bible, Bishop

Islam: Quran, Islam, Muslims, Islamic State, Kurdish, Shia, Sunni, Jihad, Wahhabi, Islamphobia

Atheist: Atheist, Atheists, Atheism

Hinduism: Hindu, Bhagavad-Gita, Brahman, Mahabharata, Ma Kali, Ramayana, Durga, Saraswati, Jai Hanuman

Buddhism: Buddhism, Gautama Buddha, Bodhisattva, Buddha, Dalai Lama, Mahayana, Nirvana

Judaism: Judaism, Jews, Jew, Berit

Duplicates are then removed.",Not discussed,,automated,Yes,"(1) Text cleaning: Text data is cleaned in order to remove unwanted texts or symbols, such as URLs, hashtags, special symbols, and punctuation.

(2) Generating lexicon: generate religious lexicon containing words and phrases with their own polarity scores to be abusive, non-abusive, or neutral.

(3) Labeling class: the cleaned texts are attributed as abusive or non-abusive class label using rule-based and generated lexicon-based approaches. Various techniques are used, including position of words, surrounding of words, contexts, parts of speech, phrases, religious slangs (e.g., bible thumper), punctuations (e.g., good!!!), and degree of modifier (e.g., very, kind of). VADER (Valence Aware Dictionary and sEntiment Reasoner) Lexicon Tool is used to effectively find motifs from texts on social media platforms, especially for microblogging contexts. Using this tool, different polarity scores of words of a tweet text can be calculated. The aggregated score of each tweet is then compared to a predefined threshold value (0.05) and the tweets are classified into 3 categories (non-abusive, abusive, neutral) according to the comparison results.",Yes,"The filtering process follows three phase filtering schemes. In phase one, the information of

the best possible impacted posts (or status) are considered, including keeping all unique posts, and posts that have length greater than a threshold. The second phase filters information based on the reputation of the user account, calculated from the number of followers and number of friends of each user. Phase third of the filtering represents users’ activity including the duration of account, date of posts, number of sharing (or retweeting) posts, number of posts issued, and number of likes or dislikes.","4,903 (only 4,141 are considered in the experiment setup since tweets with neutral class labels are skipped. )","70% (2,898 samples)","30% (1,243 samples)",N/A,Not discussed,,Not discussed,,Specific,"religion, gender, race",Targeted,religion,Yes,Yes,Religion,"religion, gender, race",Yes,"Christianity, Islam, Secular/Nonreligious/Agnostic/Atheist, Hinduism, Buddhism, and Judaism",No,,,No,Human
86,No,"(Al-Hassan & Al-Dossari, 2021)",Detection of hate speech in Arabic tweets using deep learning,Multimedia Systems,Saudi Arabia,"This research aims to develop a model for detecting Arabic hate speech in Twitter platform, then classifying the Arabic tweets based on the type of hate used in each tweet. A dataset of 11 K tweets was collected and labelled (5 distinct classes: none, religious, racial, sexism or general hate). The authors then built the SVM baseline using TF-IDF words representation and proposed four deep learning architectures that are capable of identifying and classifying Arabic hate speech in twitter into 5 classes. They compared the proposed models with the SVM baseline. Comparison results show that the deep learning approaches outperformed the baseline in the multiclassification of hate classes, and the CNN +LTSM model produced the best results.",Yes,14,21.01.2021,31.08.2022,Yes,Yes,"Hate speech can be recognized when stereotyping group of people together or individuals by using racial and sexist slurs with intent to harm. In addition, indecently speaking about religion or specific country.",Yes,"Hate classes: Religious, Racism, Sexism, General hate speech",No,,,N/A,No,Arabic hate‑speech corpus,1,1,Twitter,Hate Speech,Arabic,Implicitly before 21.01.2021 (publication),Implicitly before 21.01.2021 (publication),N/A,N/A,"For collecting the tweets, Twitter API and Tweepy Python library was used for the authentication using our Twitter credentials and to search for tweets using a cursor. A list of hashtags that attract and trigger the hateful content has been created. The hashtags have been used in the search query parameter:

Examples (Hashtag in English):

Religious: #Houthi, #Sheie (Islamic groups).

Racism: #Khadiri (Person with unrecognized tribe).

Sexism: #Feminists.

General hate speech: A number of known hated Arab names including politicians, Social media influencers, TV Actors",Not discussed,,manual,Not discussed,,Not discussed,,11k,70% (~7700),30% (~3300),"The authors performed a manual annotation for the dataset, with help from 2 volunteers to review the labelled data in order to get rid of any annotator bias. The volunteers were provided with a guide to follow to distinguish the hate classes.",Yes,"Interpretation of the 5 classes of hate

Religious: Any Religious discrimination, such as: Islamic sects “Sunni, Sheie, Alrafidhah, …etc.” Also, anti-Judaism or anti-Hinduisand, anti-Christian and their respective denominations, calling for atheism or other religions. Also attaching relations of following or not following a particular religious group, these groups include but not limited to: ISIS and Al-qaedah, Muslim Brotherhood. Al-Houthi and many others

Racism: Any Racial offense or tribalism, regionalism, prejudice against particular tribe or region, xenophobia (especially for migrant workers) and nativism (hostility against immigrants and refugees). Also, offending the appearance and color of individual or offending particular country leader or country politics

Sexism: Any post that offense particular gender using any form of hostility or devaluation based on person’s gender. In addition, any form of misogyny tendency

General hate speech: Any general type of hate which is not mentioned in the previous classes. Whether it contains: general hatred, obscene, offensive and abusive words that are not related to religion, race or sex

No hate: If the tweet does not contain any form of hatred",Not discussed,,Specific,"religion, gender, race, body, political",Targeted,"race, gender, religion, nationality",Yes,Yes,"Religion, Gender, Race","religion, gender, race",Yes,"Any Religious discrimination, such as Islamic sects, anti-Judaism or anti-Hinduisand, anti-Christian and their respective denominations, calling for atheism or other religions. Also attaching relations of a particular religious group  Any Racial offense or tribalism, regionalism, prejudice against particular tribe or region, xenophobia (especially for migrant workers) and nativism (hostility against immigrants and refugees). Also, offending the appearance and color of individual or offending particular country leader or country politics  Hostility or devaluation based on person’s gender. In addition, any form of misogyny tendency",Yes,offending the appearance and color of individual or offending particular country leader,About a person,No,Human
90,No,"(Mulki & Ghanem, 2021)",Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language,Proceedings of the Sixth Arabic Natural Language Processing Workshop,"Turkey, Canada","In this paper, the authors introduce an Arabic Levantine Twitter dataset for Misogynistic language (LeT-Mi) to be the first benchmark dataset for Arabic misogyny. They further provide a detailed review of the dataset creation and annotation phases. The consistency of the annotations for the proposed dataset was emphasized through inter-rater agreement evaluation measures. Moreover, Let-Mi was used as an evaluation dataset through binary/multi-/target classification tasks conducted by several state-of-the-art machine learning systems along with Multi-Task Learning (MTL) configuration. The obtained results indicated that the performances achieved by the used systems are consistent with state-of-the-art results for languages other than Arabic, while employing MTL improved the performance of the misogyny/target classification tasks.",Yes,20,19.04.2021,31.08.2022,Yes,Yes,Misogyny is one type of hate speech that disparages a person or a group having the female gender identity; it is typically defined as hatred of or contempt for women,Yes,"misogynistic categories: discredit, derailing, dominance, stereotyping & objectification, sexual harassment, threat of violence, and damning  target annotation: Active (individual), Passive (generic)",Yes,Other,request to have access to the full corpus by filling the form,https://github.com/bilalghanem/let-mi,No,Let-Mi (Arabic Levantine Twitter dataset for Misogynistic language),1,1,Twitter,Misogyny,Arabic,20.10.2019 - 03.11.2019,20.10.2019 - 03.11.2019,N/A,October 17th protests in Lebanon,"The proposed dataset was constructed out of Levantine tweets harvested using Twitter API. The collection process relied on scraping tweet replies written at the timelines of several Lebanese female journalists who covered the protests in Lebanon during the period (October 20- November 3, 2019). The authors identified seven journalist accounts as resources of the tweet replies, who represent different national news agencies in Lebanon.

Initially, 77,856 tweets were retrieved. The non-Levantine tweets were then manually removed. The authors also removed the tweets that (1) are non-textual, (2) are Arabic-Arabizi mixed, (3) are retweets and duplicated instances, (4) only contain a single hashtag or a sequence of hashtags, or (5) mention accounts other than the journalist’s (to assure that the collected replies are written to target the journalist herself). Thus, it ended up with 6,603 direct tweet replies.",Not discussed,,manual,Yes,"The annotation task was assigned to three annotators. Besides the annotation guidelines, and based on the domain and context of the proposed dataset, the authors made the annotators aware of specific phrases and terms which look normal/neutral while they indicate toxicity. These phrases/terms are either related to the Lebanese culture or developed during the protests in accordance with the incidents.

The 3 annotators are asked to label 6603 tweets as either non-misogynistic or one of the seven misogynistic language categories. For tweets falling under the unanimous annotation case, the final labels were directly deduced, while for those falling under the majority annotation case, the authors selected the label that has been agreed upon by two annotators out of three. The tweets having three conflicted judgments are excluded. The final version of Let-Mi composed of 6,550 tweets. The authors evaluated the annotations using two inter-annotator agreement measures: Cohen’s kappa and Krippendorff’s α.",Not discussed,,"6,550",60% (~4K) of the tweets from each class (stratified sampling),20% (~1.3K) of the tweets from each class (stratified sampling),"Three annotators, one male and two females Levantine native speakers",Yes,"Based on the definition of misogynistic behaviors in (Poland, 2016), the authors designed the annotation guidelines for Let-Mi dataset such that the eight label categories are identified as follows:

• Non-Misogynistic (none): tweets are those instances that do not express any hatred, insulting or verbal abuse towards women.

• Discredit refers to tweets that combine slurring over women with no other larger intention.

• Derailing: used to describe tweets that indicate a justification of women abuse while rejecting male responsibility with an attempt to disrupt the conversation in order to refocus it.

• Dominance: tweets are those that express male superiority or preserve male control over women.

• Stereotyping & objectification: used to annotate tweets that promote a widely held but fixed and oversimplified image/idea of women. This label also refers to tweet instances that describe women’s physical appeal and/or provide comparisons to narrow standards.

• Threat of violence: used to annotate tweets that intimidate women to silence them with an intent to assert power over women through threats of violence physically.

• Sexual harassment: used for tweets that describe actions such as sexual advances, requests for sexual favors, and sexual nature harassment.

• Damning: used to annotate tweets that contain prayers to hurt womenmost of the prayers are death/illness wishes besides praying God to curse women.

The authors also asked the annotators to tag each misogynistic tweet as belonging to one of the following two target categories:

• Active (individual): the text includes offensive tweets purposely sent to a specific target (explicit indication of addressee or mention of the journalist name);

• Passive (generic): it refers to tweets posted to many potential receivers (e.g. groups of women).",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Gender,gender,Yes,Women,Yes,"7 Lebanese female journalists who covered the protests in Lebanon during the period (October 20- November 3, 2019)",To a person,No,Human
91,No,"(Sajadi Ansari et al., 2021)",An Approach to Detect Cyberbullying on Social Media,Model and Data Engineering,France,"In this paper, the authors propose an ontology and classifiers-based approach to detect cyberbullying cases in the context of social media. They propose a cyberbullying ontology in terms of cyberbullying categories and representative terms vocabulary. This ontology is used to build and annotate the toxicity of our training dataset extracted from different data sources. Various unit classifiers are used including messages toxicity detection, gender classifier, age estimation, and personality estimation. Outputs of these classifiers can be combined to intercept contents that could be cyberbullying cases.",Yes,0,14.06.2021,31.08.2022,Yes,Yes,"Cyberbullying…willful and repeated harm inflicted through the use of computers, cell phones, and other electronic devices",Yes,"Cyberbullying (positive): insult (sexual, physical/appearance, personal, racist, sexist, homophobic), mockery (sexual, physical/appearance, personal, racist, sexist, homophobic), threat (sexual, physical, extortion, psychological), sexual content, defamation  Negative: advertising, no bullying, nan(other languages)",No,,,N/A,No,Our dataset(new); Toxic Comment Classification Challenge (old),2,1,"Twitter, Facebook",Cyberbullying,French,Implicitly before 14.06.2021 (publication),Implicitly before 14.06.2021 (publication),N/A,N/A,"Different messages were collected from Twitter and Facebook at different times. This extraction is ontology-driven, and representative terms of the cyberbullying categories are used. In total, 15472 messages were extracted from data providers. All these texts are in French. The authors firstly extracted 5158 message from Twitter at time t1 then 9433 messages at time t2, and finally 881 messages from Facebook at time t3.",Yes,"All collected data (Tweets and Facebook posts) are cleaned and anonymized to remove personal identifying information in compliance with existing data protection regulations (e.g., GDPR). Specifically, the account names (i.e. message senders or receivers) were replaced with random numbers with the help of a mapping table that was deleted by the end of the anonymization process (e.g., @John.doe was replaced by @914812). Person names in collected messages have also been replaced with fictive names (e.g., John Doe was replaced by @Christopher Yu), ages were replaced, when available, by their categories: Child, Teenager, Adult.","automated, manual",Yes,"Annotation is carried out semi-automatically. First, a set of syntaxical and linguistic rules were applied to detect the messages with negative content, e.g., insulting, obscene, profane, aggressive, threatening, intimidating content. Then these messages are marked with the corresponding category. Then, the semantic annotations are validated manually by domain experts.",Not discussed,,15472 messages,N/A,N/A,Annotations are validated manually by domain experts.,Not discussed,,Not discussed,,Specific,"gender, sexuality, race",Non-targeted,N/A,Yes,Yes,"Gender, Sexuality, Race","gender, sexuality, race",No,,No,,,No,Human
92,No,"(Ghosh Chowdhury et al., 2019)","Speak up, Fight Back! Detection of Social Media Disclosures of Sexual Harassment",Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,India,"In this work, the authors proposed a Disclosure Language Model, a three-part ULMFiT architecture, for the task of analyzing disclosures of sexual harassment on social media. On a manually annotated real-world dataset, created in two steps to capture a broad demographic, the proposed systems could often achieve significant performance improvements over (i) systems that rely on handcrafted textual features and (ii) Generic deep learning based systems. An extensive comparison shows the merit of using Medium-Specific Language Models based on an AWD-LSTM architecture, along with an augmented vocabulary which is capable of representing deep linguistic subtleties in the text that pose challenges to the complex task of sexual harassment disclosure.",Yes,30,03.06.2019,31.08.2022,Yes,Yes,Sexual harassment,No,,Yes,Github,,https://github.com/ramitsawhney27/NAACLSRW19meToo,No,"labeled real-world dataset (new), Twitter Sentiment140 dataset (old)",2,1,Twitter,Sexual Harassment,English,Implicitly before 2019 (publication),XX.11.2016 - XX.12.2018,N/A,N/A,"(1) linguistic markers: a corpus of words and phrases were developed using anonymized data from known Sexual Harassment forums (http://www.aftersilence.org/, https://pandys.org/, http://isurvive.org/)User posts containing tags of metoo, sexual violence and sexual harassment were also collected from microblogging sites like Tumblr and Reddit. For e.g., subreddits like r/traumatoolbox, r/rapecounseling, and r/survivorsofabuse. Then the TF-IDF method was applied to these texts to determine words and phrases (1-grams, 2-grams and 3-grams) which frequently appeared in posts related to sexual harassment and violence. Finally, human annotators were asked to remove terms from this which were not based on sexual harassment, as well as duplicate terms. This process generated 70 words/phrases which were used as a basis for extraction of tweets.

(2) data collection: The public Streaming API was used for the collection and extraction of recent and historical tweets. These texts were collected without knowing the sentiment or context.

Words/Phrases linked with Sexual Harassment: was assaulted, molested me, raped me, touched me, groped, I was stalked, forced me, #WhyIStayed, #WhenIwas, #NotOkay, abusive, relationship, drugged, underage, inappropriate, followed, boyfriend, workplace.",Yes,"To maintain the privacy of the individuals in the dataset, the authors do not present direct quotes from any data, nor any identifying information. Anonymized data was collected from microblogging website Twitter",manual,Yes,"All the annotators had to review the entire dataset. The tweets were segregated based on the following criteria: Is the user recollecting their personal experience of sexual harassment? Every post was scrutinized and carefully analyzed by three independent annotators H1, H2 and H3 due to the subjectivity of text annotation. Ambiguous posts were set to the default level of Non-Disclosure. Cohen Kappas inter-annotator agreement was calculated in the end.",Not discussed,,5117,N/A,10% (~512) of the dataset,The annotators included Clinical Psychologists and Academia of Gender Studies.,Yes,"Is the user recollecting their personal experience of sexual harassment?

• The default category for all posts is Non-Disclosure.

• The text is marked as Disclosure if it explicitly mentions a personal abuse experiencee.g., ”I was molested by my ex-boyfriend” e.g., ”I was told by my boss that my skirt was too distracting.”

• Posts which mentioned other people’s recollections were not marked as Disclosuree.g.”My friend’s boss harassed her”

• If the tone of the text is flippant. e.g.”I can’t play CS I got raped out there hahaha”, then it is marked as Non-Disclosure

• Posts related to sexual harassment related news reports or incidents, e.g., ”Woman gang-raped by 12 men in Uttar Pradesh”, are marked as Non-Disclosure.

• Posts about sexual harassment awareness e.g.”Sexual assault and harassment are unfortunately issues that continue to plague our college community.”, are marked as Non-Disclosure.",Not discussed,,Specific,"gender, sexuality",Targeted,sexuality,Yes,Yes,"Gender, Sexuality","gender, sexuality",Yes,"Women, LGBTQIA+",No,,,No,Human
93,No,"(Kaiser et al., 2021)",Social Media Opinion Mining Based on Bangla Public Post of Facebook,2021 24th International Conference on Computer and Information Technology (ICCIT),Bangladesh,"This paper developed a properly annotated corpus and robust machine learning classifiers to classify online hate speech. The authors created a corpus that contains 11006 Bangla comments from Facebook, analyzed them demographically, annotated them to create robust classifiers to classify these comments as positive, negative, and neutral polarity. They then decomposed these polarities to further sentiments based on contents of the text varying from wishful thinking to gender-based hate speech. The multiclass classification algorithm, consisting of TF-IDF vectorizer alongside uni-gram, bi-gram, and tri-gram followed by MNB, MNB, and KNN, gives 82.60%, 82.33%, and 79.63% accuracy, respectively.",Yes,0,18.12.2021,31.08.2022,Yes,Yes,Hate speech,Yes,"Positive: Wishful thinking, Appreciation  Negative: Gender-based hate, Religious hate, Political hate, Personal hate, Sarcasm  Neutral",No,,,N/A,No,annotated dataset,1,1,Facebook,Hate Speech,Bengali,Implicitly before 18.12.2021 (publication),Implicitly before 18.12.2021 (publication),N/A,N/A,"To make a dataset with different types of Bangla texts, the authors identified Facebook pages of Bangla celebrities, politicians, religious figures, actors, actresses, cricketers, social media influencers, singers and various other professions of people. We collected data from public posts with the help of ""Instant Data Scrapper"". We extracted the comments and compiled them in an excel file. The dataset consists of the comments, the number of reactions and the number of replies each comment has.",Yes,"For privacy, the authors didn’t include any names or profile links.",manual,Yes,"Four annotators went through the comments of the dataset. The comments are classified into three main categories, positive, negative and neutral. Positive and negative have their own subcategories with an assigned index number for each of those subcategories. The hate comments are identified based on the community standards of Facebook. If any comment didn’t follow the rules they put that comment in the negative category and after further analysis, they decided on a subcategory for the comment.

After completing the annotations, all four of the annotators are assigned to cross-validate the annotation done by their counterparts. The validation process corrected the original annotator’s annotation when needed.

Cohen’s Kappa statistic was used to evaluate the quality of annotation.",,,11006 comments,90% (~9905),10% (~1101),Four annotators are all native Bangla speakers,Yes,"The hate comments are identified based on the community standards of Facebook.

Facebook community standards and guidelines. https://www.facebook.com/communitystandards/introduction. Accessed: 2021-09-04.",Not discussed,,Specific,"gender, religion, political",Non-targeted,N/A,Yes,Yes,"Gender, Religion,","gender, religion",Yes,political related,Yes,personal hate,About a person,Yes,Human
95,No,"(Aljarah et al., 2021)",Intelligent detection of hate speech in Arabic social network: A machine learning approach,Journal of Information Science,Jordan,"This article aims to detect cyber hate speech based on Arabic context over Twitter platform, by applying Natural Language Processing (NLP) techniques, and machine learning methods. The article considers a set of tweets related to racism, journalism, sports orientation, terrorism and Islam. Several types of features and emotions are extracted and arranged in 15 different combinations of data. The processed dataset is experimented using Support Vector Machine (SVM), Naive Bayes (NB), Decision Tree (DT) and Random Forest (RF), in which RF with the feature set of Term Frequency-Inverse Document Frequency (TF-IDF) and profile-related features achieves the best results. Furthermore, a feature importance analysis is conducted based on RF classifier in order to quantify the predictive ability of features in regard to the hate class.",Yes,32,01.08.2021,31.08.2022,Yes,Yes,"Hate speech relates to using expressions or phrases that are violent, offensive or insulting for a person or a minority of people.",No,,No,,,N/A,No,the collected dataset,1,1,Twitter,Hate Speech,Arabic,Implicitly before 01.08.2021 (publication),Implicitly before 01.08.2021 (publication),N/A,N/A,"The collection of data is performed based on Twitter streaming Application Programming Interface (API) with ‘rtweet’ library, and R programming language using RStudio framework. The collection of data has targeted different areas like sport, religion, racism and journalism. The translated (from Arabic) keywords are: Alwahadat sport club, The Jordanian league, Faisaly Jordan, Islam and terrorism, damage Islam, Racism, Refugees, Freedom, media, homeland, Nahed Hattar, extreme.

The retrieved data from Twitter includes a set of features that are related to user-profile features, which are the user identification number (ID), reply to certain user ID, does the tweet is retweeted, the tweet favourite count, the retweet count, the double retweet count, the retweet-friends count, the retweet-statuses count, the count of followers, the friends count, the listed-count, the count of statuses, the count of favourites and whether the account is verified.",Not discussed,,manual,Not discussed,,,,3694 (only 1633 from positive / negative class are used in the experiments),10-fold cross-validation,10-fold cross-validation,The two annotators labelled the data independently from each other,Not discussed,,Not discussed,,Specific,"race, religion, organization/institution, other",Non-targeted,N/A,Yes,Yes,"Racism, Religion, Journalism, Sports orientation","race, religion, organization/institution, other",Yes,"The keywords used to retrieve data: Alwahadat sport club, The Jordanian league, Faisaly Jordan, Islam and terrorism, damage Islam, Racism, Refugees, Freedom, media, homeland, Nahed Hattar, extreme",No,,,No,Human
96,No,"(Litvinova & Litvinova, 2020)",Analysis and Detection of a Radical Extremist Discourse Using Stylometric Tools,Digital Science 2019,Russia,"In the last few years extremist groups including radical Islamic ones have started to produce content in languages other than English and, more specifically, in Russian which has been increasingly employed. This work shows the possibility of using stylometric tools allowing different types of texts analysis to be performed in order to analyze and detect radical extremist content. Using a meticulously designed dataset consisting of real-world extremist forum texts on two topics and texts by common Internet users on the same topics, this work showed that the authors from extremist forum remain consistent in their style irrespective of the topic and their texts could thus be detected using a variety of methods. The work conclude by the underscoring the importance of combining qualitative and quantitative analysis of a radical extremist discourse for better understanding of radical minds and development of counter-extremist tools.",Yes,3,20.12.2019,31.08.2022,Yes,Yes,Radical Islamist extremist content,No,,No,,,N/A,No,"Extremist_Moscow, Extremist_DMDUser_Moscow, User_DMD

Datasets are divided into 10000-word chunks (Corpus10000) or 1000-word chunks (Corpus1000)",4,4,"Kavkazchat, The Village, Svoboda",Radical Extremist Discourse,Russian,Implicitly before 20.12.2019 (publication),"KavkazChat: 3/21/2003–5/21/2012, implicitly after the terrorist attacks in 2010 and 2011  News comments: Implicitly before 20.12.2019 (publication) and after the terrorist attacks in 2010 / 2011",N/A,"The 2010 Moscow Metro bombings, the 2011 Domodedovo attack","(1)Extremist_Moscow and Extremist_DMD:

KavkazChat dataset contains 699,981 posts written by 7,125 members in the period 3/21/2003–5/21/2012. These posts are organized into 16,854 threads (topics). For the ongoing analysis this research selected two threads dedicated to the discussion of two severe terrorist attacks for which Caucasus Emirate leader Doku Umarov claimed responsibility: the 2010 Moscow Metro bombings and the 2011 Domodedovo attack.

Details for the first thread is referred to a previous work.

The second thread dedicated to the discussion the deadly terrorist attack at the international arrivals terminal of the Domodedovo Airport on January 24, 2011. All the texts by extremist forum authors were compiled into one txt file per thread. Citations and usernames were removed. As the second thread is smaller (21453 tokens), only the first 21453 tokens from the first thread were extracted to avoid data imbalance.

(2) User_Moscow and User_DMD

They then compiled two datasets which consist of the texts by common Internet users who commented on the news about these same attacks. These corpora were as large as the datasets Extremist_Moscow and Extremist_DMD derived from the Kavkazchat. The first dataset contains comments on the material on the Moscow bombing posted by an online newspaper “The Village” and by the Radio Svoboda. The second dataset compiles comments from users for the news on Domodedovo attack posted by the Radio Svoboda and Komsomolskaya Pravda.",Yes,Citations and usernames were removed.,N/A,Not discussed,,,,"21453 tokens in each dataset (Extremist_Moscow, Extremist_DMD, User_Moscow, User_DMD)",6-fold cross-validation,6-fold cross-validation,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,religion,Yes,No,,,No,,No,,,No,Human
97,No,"(Dementieva et al., 2021)",Methods for Detoxification of Texts for the Russian Language,"Multimodal Technologies and Interaction
",Russia,"This research introduces the first study of the automatic detoxification of Russian texts to combat offensive language, which can be used to process toxic content on social media or eliminate toxicity in automatically generated texts. The study introduces two models: an approach based on BERT architecture that performs local corrections and a supervised approach based on a pre-trained GPT-2 language model. The study provides the training datasets and describes the evaluation setup and metrics for automatic and manual evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.",Yes,7,04.09.2021,26.09.2022,Yes,Yes,"""We consider a message toxic if it is considered toxic by annotators.""",Yes,"non-toxic converted from normal and toxic label by merging insult, threat, obscenity",Yes,Github,,https://github.com/skoltech-nlp/rudetoxifier,Yes,RuToxic,2,1,"Odnoklassniki, Pikabu",Toxicity,Russian,Implicitly before July 2021,Implicitly before 04.09.2022,Russia,N/A,"Two corpora of toxic comments were released on Kaggle [36,37]. We use the concatenation of these two resources. ",Not discussed,,automated,Yes,"They merge the labels as follows:
• __label__NORMAL of the Toxic Russian Comments dataset [37] is converted to non-toxic label;
• __label__INSULT, __label__THREAT, and __label__OBSCENITY labels of the Toxic Russian Comments dataset are converted to toxic label.
They denote the joint corpus RuToxic dataset.
The detoxGPT method requires a parallel dataset for training. They use a part of the RuToxic dataset to create it. They randomly select 200 toxic sentences and manually rewrite them into non-toxic ones.",Yes,random sampling,"163,187 texts (31,407 (19%) toxic and 131,780 (81%) non-toxic)",200,"10,000","The annotation was conducted by 4 people, who are the authors of the paper. All annotators were native speakers of Russian, held a BSc degree or above in Computer Science, and had experience in NLP. ",Yes,"All the instructions and markup forms were provided in the Russian language.
The annotation guidelines are the following:
• If the content is preserved, the sentences should be labeled as fully matching. In particular, this is true for the cases when the output sentence is toxic or grammatically incorrect.
• If a rude or obscene word describing a person or a group of people (e.g., idiot) was replaced with an overly general non-toxic synonym (e.g., human) without a significant loss of meaning, this is considered a fully matching pair of sentences.
• If the non-toxic part of the original sentence was fully saved but the toxic part was replaced inadequately, this is considered a partially matching pair.
• If the output sentence is senseless or if the content difference is obvious, the pair of sentences is considered different.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,Yes,Human
98,No,"(Singh et al., 2020)",Aspect Based Abusive Sentiment Detection in Nepali Social Media Texts,2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),"USA, Nepal","This study creates a dataset for the targeted aspect-based sentiment analysis in the social media domain, sets up a dataset benchmark and evaluates using various machine learning models. The dataset consists of code-mixed and code-switched comments extracted from Nepali YouTube videos. The research presents convincing baselines using a multilingual BERT model for the Aspect Term Extraction task and BiLSTM model for the Sentiment Classification Task achieving 57.978% and 81.60% F1 scores respectively.",Yes,2,24.03.2021,26.09.2022,Yes,Yes,"We used binary sentiment polarity schema and divided the comments into 6 aspect categories General, Profanity, Violence, Feedback, Sarcasm and Out-of-scope to annotate the data.",Yes,"General, Profanity, Violence, Feedback, Sarcasm and Out-of-scope ",Yes,Github,,https://github.com/oya163/nepali-sentiment-analysis,Yes,NepSA,1,1,YouTube,Abusiveness,Nepali,Implicitly before December 2020,Implicitly before December 2020,Nepal,N/A,The dataset consists of 3068 comments extracted from 37 different YouTube videos of 9 different YouTube channels.,Not discussed,,manual,Yes,"They used the BRAT [20] annotation tool to annotate the dataset. Annotators first began by reading the annotation guidelines and examples. Each annotator was then required to annotate a small subset of the data. After completion, an inter-annotator agreement was calculated and disagreements were discussed. This procedure was repeated with gradual changes in the guidelines until a reasonable agreement was reached.
After a reasonable agreement was reached, each annotator started to annotate the remaining dataset equally. The comments were annotated at a sentence level. To find the underlying entity-aspect relationship, the annotators were asked to determine the aspect terms and entity terms if present and identify the relationship between them. Phrase-level annotation was performed only for the General, Profanity, Violence and Feedback categories. Each aspect term was also given a polarity value to determine if it is positive/neutral or negative. Therefore, the dataset can be divided into two schemas, fine-grain and coarse-grain, its example is presented in figures 1 and 2. As seen in figure 2, only the end of the sentence is tagged with the aspect category which is equivalent to tagging the whole sentence.",Yes,We collected the comments from the most popular Nepali YouTube channels having the highest subscribers under the News & Politics category.,3068,306,,Two annotators whose native language is Nepali,Yes,https://github.com/oya163/nepali-sentiment-analysis /tree/master/guidelines,Not discussed,,Specific,"organization/institution, other",Non-targeted,N/A,Yes,Yes,"Person, Organization, Location, Miscel laneous.","organization/institution, other",No,,No,,,No,Human
99,No,"(Hine et al., 2017)","Kek, Cucks, and God Emperor Trump: A Measurement Study of 4chan’s Politically Incorrect Forum and Its Effects on the Web",Proceedings of the Eleventh International AAAI Conference on Web and Social Media (ICWSM 2017),"Italy, UK, USA, Cyprus","This research discusses the discussion-board site 4chan about its user base, the content it generates, and how it affects other parts of the Web. They address the gap by analyzing /pol/ along several axes, using a dataset of over 8M posts they collected over two and a half months. First, they perform a general characterization, showing that /pol/ users are well distributed around the world and that 4chan’s unique features encourage fresh discussions. We also analyze content, finding, for instance, that YouTube links and hate speech are predominant on /pol/. Overall, the analysis not only provides the first measurement study of /pol/, but also insight into online harassment and hate speech trends in social media.",Yes,261,03.05.2017,09.10.2022,Yes,Yes,"hate speech: ""…generally distasteful content – even by 4chan standards – to be discussed with- out disturbing the operations of other boards, with many of its posters subscribing to the altright and exhibiting characteristics of xenophobia, social conservatism, racism, and, generally speaking, hate.""",No,,Yes,Github,,https://github.com/4chan/4chan-API,Yes,"/pol/, /sp/, /int/",6,4,4chan,Hate Speech,"English, French, German, Spanish, Portuguese","30.06.2016 - 12.09.2016, 18.09.2016 - 05.10.2016",30.06.2016 - 25.09.2016,N/A,N/A,"They started crawling 4chan using JSON API from June 30, 2016. They retrieve /pol/’s thread catalog every 5 minutes and compare the threads that are currently live to those in the previously obtained catalog. For each thread that has been purged, they retrieve a full copy from 4chan’s archive, which allows to obtain the full/final contents of a thread. 
On August 6, 2016 they also started crawling /sp/, 4chan’s sports board, and on August 10, 2016 /int/, the international board. 
The analysis presented in this paper considers data crawled until September 12, 2016, except for the raids analysis pre- sented later on, where they considered threads and YouTube comments up to Sept. 25. They also use a set of 60,040,275 tweets from Sept. 18 to Oct. 5, 2016 for a brief comparison in hate speech usage. ",Yes,"we have followed standard ethical guidelines (Rivers and Lewis 2014), and encrypted data at rest, while making no attempt to de-anonymize users. ","automated, manual",Not discussed,,Yes,random sampling,"10,893,125",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"nationality, race, political",No,No,,,No,,No,,,No,Human
100,No,"(Vidgen et al., 2020)",Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection,arXiv preprint,UK,"This study presents a human-and-model-in-the-loop process for dynamically generating datasets and training better-performing and more robust hate detection models. They provide a new dataset of ∼40, 000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes ∼15, 000 challenging perturbations and each hateful en- try has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. The study shows that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also perform better on HATECHECK, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.
",Yes,53,03.06.2021,12.12.2022,Yes,Yes,"Hate speech: ""‘Hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” (Warner and Hirschberg, 2012).""",Yes,Derogation; Animosity; Threatening language; Support for hateful entities; Dehumanization,Yes,Github,,https://github.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset,Yes,Dynamically-Generated-Hate-Speech-Dataset,1,1,open source web,Hate Speech,English,N/A,N/A,N/A,N/A,"The dataset was generated over four rounds, each of which involved ∼10, 000 entries. The final dataset comprises 41, 255 entries. Entries could target multiple groups. After each round, the data was split into training, dev and test splits of 80%, 10% and 10%, respectively. Approximately half of the entries in the test sets are produced by annotators who do not appear in the training and dev sets (between 1 and 4 in each round). The other half of each test set consists of content from annotators who do appear in the training and dev sets.",Not discussed,,manual,Yes,"Data were annotated using an open-source web platform for dynamic dataset creation and model benchmarking. The platform supports human-and-model-in-the-loop dataset creation for a variety of NLP tasks. Annotation was overseen by two experts in online hate. Annotation guidelines were created at the start of the project and then updated after each round in response to the increased need for detail from annotators. They followed the guidance for protecting and monitoring annotator well-being provided by Vidgen et al. (2019a). 20 annotators were recruited. They received extensive training and feedback during the project. All entries are assigned to either ‘Hate’ or ‘Not Hate’. For
‘Hate’, we also annotate secondary labels for the type and target of hate.",Yes,They identify the best sampling ratio of previous rounds’ data using the dev sets.,"41,255","33,005","4,125","20 annotators were recruited. Ten were recruited to work for 12 weeks and ten were recruited for the final four weeks. Of the 20 annotators, 35% were male and 65% were female. 65% were 18-29 and 35% were 30- 39. 10% were educated to high school level, 20% to undergraduate, 45% to taught masters and 25% to research degree (i.e. PhD). 70% were native English speakers and 30% were non-native but fluent. Respondents had a range of nationalities, including British (60%), as well as Polish, Spanish and Iraqi. Most annotators identified as ethnically white (70%), followed by Middle Eastern (20%) and two or more ethnicities (10%). Participants all used social media regularly, including 75% who used it more than once per day. All participants had seen other people targeted by online abuse before, and 55% had been targeted personally.",Yes,"Annotation guidelines were created at the start of the project and then updated after each round in response to the increased need for detail from annotators. All annotators attended a project onboarding session, a half-day training session, at least one one-to-one session and a daily ’standup’ meeting when working. They were given a test assignment and guidelines to review before starting work and received feedback after each round. The guideline can be found through this link: https://github.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset ",Not discussed,,Specific,"disability, gender, nationality, race, sexuality, religion, class, other",Targeted,"race, religion, gender, sexuality",Yes,Yes,"Disability, Gender, Immigration status, Race / Ethnicity, Religion or belief, Sexual orientation, National origin, Class, Intersectional","disability, gender",Yes,People with disabilities; Gender minorities (e.g. non binary); Women; Trans; Immigrants; Foreigner; Refugee; Asylum seeker; Black people; Indigenous; East Asians (e.g. China); East Asians (e.g. Korea); South East Asians (e.g. India); Pakistanis; Aboriginal people (e.g. Native Americans); Mixed race; Minority groups; Arabs; Travellers (e.g. Roma); People from Africa; Muslims; Jews; Gay; Lesbian; Bisexual; Polish; Hindus; Working class; Hispanic (e.g. Latinx) ; Black women; Black men; Indigenous women; Asian women; Muslim women,No,,,No,Human
101,No,"(De La Vega & Ng, 2018)",Determining Trolling in Textual Comments,11th International Conference on Language Resources and Evaluation,USA,"With the goal of facilitating the computational modelling of trolling, this study proposes a trolling categorization that is novel in the sense that it allows comment-based analysis from both the trolls’ and the responders’ perspectives, characterizing these two perspectives using four aspects, namely, the troll’s intention and his intention disclosure, as well as the responder’s interpretation of the troll’s intention and her response strategy. Using this categorization, researchers annotate and release a dataset containing excerpts of Reddit conversations involving suspected trolls and their interactions with other users. ",Yes,5,12.05.2018,27.12.2022,Yes,Yes,"Trolling. ""Trolling is “the activity of posting messages via a communications network that are intended to be provocative, offensive or menacing” (Bishop, 2013).""",Yes,Mock trolling or playing,No,,,,,N/A,1,1,Reddit,Trolling,English,N/A,XX.08.2015,N/A,N/A,"They collected all comments in the stories’ conversations on Reddit that were posted in August 2015. They used Lucene to create an inverted index from the comments. Given that trolling comments have malicious intentions, they queried the inverted index for comments containing one or more of the following six categories of words: (1) the word “troll” as well as words having an edit distance of 1 from it; (2) the list of highly offensive words identified by Nitta et al. (2013); (3) the list of impoliteness cues identified by Danescu-Niculescu-Mizil et al. (2013); (4) the list of words having a negative prior polarity according to Wil- son et al.’s (2005) prior polarity lexicon; (5) 1061 swear words and short phrases collected from the internet, blogs, and forums; and (6) words that appear in the same Word- Net synset as “anger”.",Not discussed,,manual,Not discussed,,Not discussed,,1000,N/A,N/A,Two human annotators,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
103,No,"(Ishmam, Arman, & Sharmin, 2019) ",Towards the Development of the Bengali Language Corpus from Public Facebook Pages for Hate Speech Research,Proceedings of Asian CHI Symposium 2019: Emerging HCI Research Collection,Bangladesh,"In this paper, they have discussed the development and annotation of the corpus of hateful speech in the Bengali language on public Facebook pages. They have classified hateful comments into six major classes based on the social aspects of Bangladesh. The corpus (4753 comments) is the maiden contribution as a publicly available data set that can be enhanced and utilized for future hate speech research in SNS.",Yes,2,09.05.2019,31.12.2022,Yes,Yes,"Hate speech. ""Hate Speech: In general, hate Speech (Tribune, 2018) is a form of comment that is intended to attack a person regarding sex, ethnicity, race, disability, sexual orientation or any kind of personal attack.""",Yes,"Hate Speech, Inciteful, Religious Hatred, Communal Attack, Religious Comments, Political Comments",No,,,,,Data set,1,1,Facebook,Hate Speech,Bengali,"22.12.2017 - XX.04.2018, XX.05.2018 - XX.11.2018",N/A,Bangladesh,N/A,"They have selected at least one page from each of the page categories. They have selected noyon chaerjee5, বাঁেশরেকল্লা (Basherkella), Awami League, Sakib Al Hasan for developing our corpus. These pages are of popular Facebook celebrities, extremist political parties, the official page of the ruling party, and the famous cricketer of Bangladesh. These pages have likes of about 0.086, 0.175, 2.5, and 10 million respectively. Each page gives three posts per day on average having 2k reactions and 80 comments per post. We have used Facebook graph API (version 2.9) from 22 December 2017 to April 2018 and generated about 3,000 comments from these pages along with various metadata such as replies of comments, reactions, user id. However, Facebook's recent policies have reduced the access of Graph API for gathering the content of pages. Therefore, from May 2018 to November 2018, they have manually collected about 2000 comments from these pages.",Not discussed,,manual,Not discussed,,Not discussed,,5000,N/A,N/A,"Three university students, who are from different disciplines such as history, and language. Most importantly, all of them are active users of Facebook. Among the three, one is from an ethnic minority since a religious minority can identify the communal attack beer than others.",Not discussed,,Not discussed,,Specific,"religion, race",Targeted,"gender, race, disability, sexuality, other",Yes,Yes,Religion,religion,No,,No,,,No,Human
105,No,"(Chiril, Moriceau, Benamara, Mari, Origgi, & Coulomb-Gully, 2020) ",An Annotated Corpus for Sexism Detection in French Tweets,In Proceedings of the 12th language resources and evaluation conference,France,"This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In the context of offensive content mediation on social media now regulated by European laws, it is important to be able to detect automatically not only sexist content but also to identify if a message with sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of this study's annotation scheme.It also proposes some preliminary results for sexism detection obtained with a deep learning approach. ",Yes,33,16.05.2020,31.12.2022,Yes,Yes,"Sexism. ""Sexism is prejudice or discrimination based on a person’s gender. It is based on the belief that one sex or gender is superior to another and it mainly affects women and girls.""",Yes,"direct, descriptive, reporting",Yes,Github,,https://github.com/patriChiril/An- Annotated-Corpus-for-Sexism-Detection- in-French-Tweets,Yes,Annotated Corpus for Sexism Detection in French Tweets,1,1,Twitter,Sexism,French,XX.10.2017-XX.05.2018,N/A,N/A,N/A,"To collect sexist and non-sexist tweets, they followed Anzovino et al. (2018) approach using the:
• a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc.,
• the names of women/men potentially victims or guilty of sexism (mainly politicians): Ségolène Royal, Nadine Morano, Theresa May, Hillary Clinton, Dominique Strauss-Kahn, Nicolas Hulot, etc.,
• specific hashtags to collect stories of sexism experiences: #balancetonporc, #sexisme, #sex- iste, #SexismeOrdinaire, #EnsembleContreLeSexisme, #payetashnek, #payetontaf, etc..
Thus, they collected around 115,000 tweets among which about 30,000 contain specific hashtags. Before detailing the annotation scheme and the result of the annotation procedure, the next subsection presents the theoretical backgrounds on which they based our study.",Not discussed,,manual,Yes,"300 tweets have been used for the training of 5 annotators and then removed from the corpus. Then, 1,000 tweets have been annotated by all annotators and the average Cohen’s Kappa is 0.72 for sexist content/non-sexist/no decision categories and 0.71 for direct/descriptive/reporting/non-sexist/no decision categories which means a strong agreement. For these 1,000 tweets, the final labels have been assigned according to a majority vote.
Finally, a total of 11,834 tweets have been annotated according to the guidelines after removing the tweets annotated as ""no decision"".",Not discussed,,"11,834","3,564",923,Five master degree’s students (3 female and 2 male) in Communication and Gender,Yes,"Given a tweet, annotation consists in assigning it one of the following three categories:
(i) Sexist content: it can be either direct (cf. (1) and (2)), descriptive (cf. (3) and (3)) or reporting (cf. (5) and (6)). The first two are real sexist messages but not the last one as reporting tweets must not be considered as sexist in a context of moderation.
Direct sexist content, directly addressed to a woman or a group of women, generally uses second person pro-noun/verb and imperatives, as shown in the examples below (linguistic clues are underlined).
In descriptive sexist content where the tweet describes a woman or women in general, clues can be the presence of a named entity or the use of generalizing terms.
When the sexist content is in fact a report of a sexist experience or a denunciation of sexist behaviour, we ob- serve the presence of reporting verbs, quotations and specific hashtags.
(ii) Non-sexist: when the tweet has no sexist content (it may contain a specific hashtag but the content is not sexist), as in (7) and (8).
(iii) No decision: when the tweet is not understandable (not well-written, lack of context) or when the sexist content is not in the text but only in a photo, video, or URL (because we cannot process them), as in (9).",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Sexism,gender,No,,No,,,No,Human
106,No,"(Cruz, de Sousa, & Cavalcanti, 2022)",Selecting and combining complementary feature representations and classifiers for hate speech detection,Online Social Networks and Media,"Canada, Brazil","This work argues that a combination of multiple feature extraction techniques and different classification models is needed. They propose a framework to analyze the relationship between multiple feature extraction and classification techniques to understand how they complement each other. The framework is used to select a subset of complementary techniques to compose a robust multiple classifiers system (MCS) for hate speech detection. The experimental study considering four hate speech classification datasets demonstrates that the proposed framework is a promising methodology for analyzing and designing high-performing MCS for this task. MCS system obtained using the proposed framework significantly outperforms the combination of all models and the homogeneous and heterogeneous selection heuristics, demonstrating the importance of having a proper selection scheme.",Yes,1,02.02.2022,01.01.2023,Yes,Yes,"Hate speech. ""Hate speech is usually defined as any sort of speech targeting a particular group, especially based on race, religion, or sexual orientation. Davidson et al. (2017) defined hate speech as: ‘Language that is used to express hatred towards a targeted group or intended to be derogatory, to humiliate, or to insult the members of the group’.”",No,,Yes,Github,,https://github.com/Menelau/Hate- Speech- MCS,Yes,"TD, ZW, TD+ZW, HatEval",4,1,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"They modified the labels racism and sexism of the ZW dataset to hate to unify the records, resulting in a single dataset containing 30,131 instances. They removed the records labelled as ‘‘Neither’’ since they do not have a clear class description. Since the combined datasets were proposed with different types of hate speech, their combination is expected to present multi-modal properties.",Not discussed,,N/A,Not discussed,,Not discussed,,"30,131",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"race, religion, sexuality",No,No,,,No,,No,,,No,Human
107,No,"(Hua, Ristenpart, & Naaman, 2020)",Towards Measuring Adversarial Twitter Interactions against Candidates in the US Midterm Elections,Proceedings of the international AAAI conference on web and social media,USA,"In this study, they measure the adversarial interactions against candidates for the US House of Representatives during the run-up to the 2018 US general election. They gather a new dataset consisting of 1.7 million tweets involving candidates, one of the largest corpora focusing on political discourse. They then develop a new technique for detecting tweets with toxic content that are directed at any specific candidate. Such technique allows them to quantify adversarial interactions towards political candidates. Further, they introduce an algorithm to induce candidate-specific adversarial terms to capture more nuanced adversarial interactions that previous techniques may not consider toxic. Finally, they use these techniques to outline the breadth of adversarial interactions seen in the election, including offensive name-calling, threats of violence, posting discrediting information, attacks on identity, and adversarial message repetition.",Yes,23,26.05.2020,01.01.2023,Yes,Yes,"Adversarial interactions. ""We broadly define adversarial interactions as messages intending to hurt, embarrass, or humiliate a targeted individual.""",Yes,"Offensive name-calling, Threats of violence, Posting discrediting information, Attacks on identityAdversarial message repetition",Yes,Other,Figshare,https://figshare.com/articles/U S Midterm Election Twitter Dataset 2018/11374062,No,US Midterm Election Twitter Dataset ,1,1,Twitter,Adversarial Interactions,English,17.09.2018 - 06.11.2018,N/A,N/A,N/A,"They retrieved the full list of candidates running in 2018 for the United States’ House of Representatives from Ballotpedia (Ballotpedia 2018). They filtered out candidates who didn’t pass the primary election (except for those in Louisiana, where the primary election is held with the general election), resulting in 1-2 candidates for each of the 435 congressional races.
They obtained the candidates’ Twitter accounts by manually verifying the campaign accounts listed on their Ballotpedia pages and campaign websites. They included the candidates’ personal or office accounts (for incumbents) when found. Our final dataset includes a list of 786 candidates (87% of all House candidates competing in November 2018): 431 Democrats (D) and 355 Republicans (R) candidates from all 50 states with 1, 110 Twitter accounts in total. They obtained the gender of each candidate based on a manual inspection of candidate profiles. In total, our dataset includes 231 female candidates and 555 male candidates1.
They collected data from Twitter (using the Twitter streaming API) from September 17th, 2018 until November 6th, 2018, including all tweets posted by, mentioning, replying to, or retweeting any of the candidate accounts. They estimate good coverage on all data except mentions due to the limited access to the Twitter API. In total, our data consists of 1.7 million tweets and 6.5 million retweets of candidates from 992 thousand users (including the candidate accounts). They publish all the tweet ids collected at Figshare.
On Twitter, the following relationships among users have been shown to be informative for inferring user interest or political preference (Romero, Tan,andUgander2013; Barbera ́ 2015). They, therefore, retrieved the 5,000 most recently followed accounts by each user account via the Twitter Standard API (Twitter 2019). Due to Twitter API rate limits, this data collection was only completed by March 2019.",Not discussed,,manual,Not discussed,,Yes,Random sampling,1.7 million,N/A,N/A,A team of three graduate students and two researchers. The labelling is done by two raters and conflicts are resolved by the third.,Not discussed,,Not discussed,,Specific,"race, religion, gender, political",Non-targeted,N/A,No,Yes,"Race, Religion, Gender","race, religion, gender",Yes,Politicians ,Yes,Trump,About a person,No,Human
110,No,"(Özel, Saraç, Akdemir, & Aksu, 2017)",Detection of cyberbullying on social media messages in Turkish,2017 International Conference on Computer Science and Engineering (UBMK) ,Turkey,"The aim of this study is to detect cyberbullying on social media messages written in Turkish. They prepare a dataset from Instagram and Twitter messages written in Turkish and then we applied machine learning techniques that are Support Vector Machines (SVM), decision tree (C4.5), Naïve Bayes Multinomial, and k Nearest Neighbors (kNN) classifiers to detect cyberbullying. They also apply information gain and chi-square feature selection methods to improve the accuracy of classifiers. They observe that when both words and emoticons in text messages are taken into account as features, cyberbullying detection improves. Among the classifiers, Naïve Bayes Multinomial is the most successful one in terms of both classification accuracy and running time. When feature selection is applied classification accuracy improves up to 84% for the dataset used.",Yes,62,02.11.2017,01.01.2023,Yes,Yes,"Cyberbullying. ""Cyberbullying is defined as an aggressive, intentional action against a defenceless person by using the Internet or other electronic methods such as emails, text messages, social media messages, or contents on websites/blogs (Smith et al., 2008; Snakenborg, Van Acker, & Gable, 2011).""",No,,No,,,,,dataset,2,1,"Twitter, Instagram",Cyberbullying,Turkish,N/A,N/A,N/A,N/A,"They collected our dataset manually from Twitter and Instagram messages. The dataset contains 900 messages written in Turkish, 450 of them have cyberbullying content, and 450 messages do not have any cyberbullying content. As shown the gender of the message author affects cyberbullying; one-half of the messages in each class belongs to male authors and the other half of the messages are collected from female authors. For example, 225 of 450 cyberbullying messages are written by males, and the other half are written by females. Same-gender distribution is also applied to messages that do not have cyberbullying content.",Not discussed,,manual,Not discussed,,,,900,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
112,No,"(Uyheng & Carley, 2020)",Bots and online hate during the COVID-19 pandemic: case studies in the United States and the Philippines,Journal of computational social science,USA,"This work presents and employs a methodological pipeline for assessing the links between hate speech and bot-driven activity through the lens of social cybersecurity. Using a combination of machine learning and network science tools, they empirically characterize Twitter conversations about the pandemic in the United States and the Philippines. Their integrated analysis reveals idiosyncratic relationships between bots and hate speech across datasets, highlighting different network dynamics of racially charged toxicity in the US and political conflicts in the Philippines. Most crucially, they discover that bot activity is linked to higher hate in both countries, especially in communities which are denser and more isolated from others. They discuss several insights for probing issues of online hate speech and coordinated disinformation, especially through a global approach to computational social science.",Yes,74,20.10.2020,01.01.2023,Yes,Yes,"Hate speech. ""Hate speech has been broadly defined as abusive language that targets a specific group (Davidson et al., 2017).""",No,,No,,,,,"PH, US",3,2,Twitter,Hate Speech,English,05.03.2020 - 19.05.2020,05.03.2020-19.05.2020,"USA, Philippines",Covid19,"Online conversations around the COVID-19 pandemic were collected using Twitter’s REST application programming interface (API) updated on a daily basis. They used search terms about the pandemic in the Philippines and the US. Localized hashtags were used to delineate tweets of interest, specifically ‘#COVID19PH’ for the Philippines and ‘#COVID19US’ for the US. To enforce reasonable comparability between the datasets, hashtags which were not geographically specific to either the US or the Philippines were not included, such as #Wuhanvirus, #Chinavirus, and #coronavirus, which related studies have otherwise used. In this manner, they sought to delineate the study to the discourse surrounding the mainstream, country-specific hashtag related to the topic. For the comparative purposes of the study, all data on the US and all data on the Philippines were processed in parallel; that is, datasets were treated separately from each other with comparisons drawn after analytical strategies were implemented.
They continued data collection over a 75-day period from March 5 to May 19 of 2020. For parsimony, data were strategically time-bound before the emergence of the ‘#BlackLivesMatter’ protests, which also impacted public discourse worldwide. They stored user metadata, tweet metadata, data on user interactions, and data about the hashtags and URLs each tweet used.",Not discussed,,N/A,Not discussed,,Not discussed,,17 millions,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,"political, gender, race, nationality, religion",Non-targeted,N/A,No,Yes,"Political, Gender, Racial/nationality, Religious","political, gender, race, nationality, religion",No,,No,,,No,Non-human
113,No,"(Wullach, Adler, & Minkov, 2020)",Towards Hate Speech Detection at Large via Deep Generative Modeling,IEEE Internet Computing,"Israel, USA","They first present a dataset of 1 million hate and non-hate sequences, produced by a deep generative model. They further utilize the generated data to train a well-studied DL detector, demonstrating significant performance improvements across five hate speech datasets.",Yes,22,23.10.2020,02.01.2023,Yes,Yes,"Hate speech. ""…a fraction of the users use a language that expresses hatred toward a specific group of people, known as Hate Speech (Lunden, 2017; Twitter, 2020).""",No,,No,,,,,MegaSpeech,6,1,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"They created MegaSpeech, a large-scale resource comprised of 1M text sequences, automatically generated and assessed as hate and non-hate speech.",Not discussed,,manual,Not discussed,,Not discussed,,1 million,71611,17903,Not discussed,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
114,No,"(Zhao, Zhang, & Hopfgartner, 2022)",Utilizing subjectivity level to mitigate identity term bias in toxic comments classification,Online Social Networks and Media,UK,"In this work, they propose a novel approach to debias the model in toxic comment classification, leveraging the notion of the subjectivity level of a comment and the presence of identity terms. They hypothesize that toxic comments containing identity terms are more likely to be expressions of subjective feelings or opinions. Therefore, the subjectivity level of a comment containing identity terms can be helpful for classifying toxic comments and mitigating identity term bias. To implement this idea, they propose a model based on BERT and study two different methods of measuring the subjectivity level. The first method uses a lexicon-based tool. The second method is based on the idea of calculating the embedding similarity between a comment and a relevant Wikipedia text of the identity term in the comment. They thoroughly evaluate our method on an extensive collection of four datasets collected from different social media platforms. The results show that: (1) the models that incorporate both features of subjectivity and identity terms consistently outperform strong SOTA baselines, with the best performing model achieving an improvement in F1 of 4.75% over a Twitter dataset; (2) their idea of measuring subjectivity based on the similarity to the relevant Wikipedia text is very effective on toxic comment classification as the model using this has achieved the best performance on 3 out of 4 datasets while obtaining comparative performance on the remaining dataset. They further test our method on RoBERTa to evaluate the generality of our method and the results show the biggest improvement in F1 of up to 1.29% (on a dataset from a white supremacist online forum).",Yes,1,21.03.2022,02.01.2023,Yes,Yes,"Toxic comment. ""By ‘‘toxic comment’’, we generally refer to different types of negative, unhealthy or disrespectful user-generated-content, which includes but is not limited to hate speech, abusive language, cyberbullying, etc (Kwok & Wang, 2013; Burnap et al., 2015; Chavan & Shylaja, 2015).""",No,,No,,,,,"WS, Tweet 18k, Tweet 42k, Wiki",4,4,"a white supremacist online forum, Twitter, Wikipedia",Toxicity,English,N/A,N/A,N/A,N/A,N/A,Not discussed,,N/A,Not discussed,,Not discussed,,"10,703; 18,625; 42,314; 159,571",80%,10%,Not discussed,Not discussed,,Not discussed,,Specific,"race, gender, religion, sexuality",Non-targeted,N/A,Yes,No,,,Yes,"muslim, gay, whites, women, jews",No,,,No,Human
115,No,"(Sandaruwan, Lorensuhewa, & Kalyani, 2019)",Sinhala Hate Speech Detection in Social Media using Text Mining and Machine learning,2019 19th International Conference on Advances in ICT for Emerging Regions (ICTer) ,Sri Lanka,"They propose lexicon-based and machine learning-based approaches to automatically detect Sinhala hate and offensive speeches that are being shared through Social Media. In this study, lexicon-based approach was initiated with the lexicon-generating process and corpus-based lexicon gave 76.3% of accuracy for hate, offensive and neutral speech detection. Machine learning approach was begun with building a 3000 comments corpus which is evenly distributed among hate, offensive and neutral speeches. Using this comment corpus, they were able to identify best-fitting feature groups and models for Sinhala hate speech detection. According to the experiments, the character trigram with Multinomial Naïve Bayes gave the highest recall value of 0.84 with 92.33% accuracy.",Yes,18,XX.XX.2019,02.01.2023,Yes,Yes,"Hate speech. ""Hate speech is defined as ―a speech that makes hatred towards any individual or any person or a group based on race, religion, ethnicity, gender, sexual orientation and their believing.”
Offensive speech. ""An offensive speech is defined as ―speech that contains swear words, but that not containing or not spreading hatred towards others”",No,,No,,,,,,1,1,"Facebook, YouTube","Hate Speech, Abusiveness",Sinhala,N/A,N/A,N/A,Not discussed,"Data set was collected by using Facebook API, YouTube API and using an in-house built crawler. Before annotation process, we removed URLs and emojis. Then comments were saved in text files under utf-8 formatting.",Not discussed,,manual,Yes,"In comments annotation process, annotators were asked to label the
comments as offensive and neutral as they feel when the comments are being read. After the annotation process, annotated comments set was received. 
Then they were asked to annotate same comments again according to the hate and offensive definitions that were given to them. With the commitment of three annotators, extracted comments were annotated among three classes such as hate, offensive and neutral. Class for a comment was taken by considering the majority vote among three annotators.",Not discussed,,3000,N/A,N/A,Three annotators,Yes,"Hate speech is defined as ""a speech that makes hatred towards any individual or any person or a group based on race, religion, ethnicity, gender, sexual orientation and their believing.” in our study and
An offensive speech is defined as ""speech that contains swear words, but that not containing or not spreading hatred towards others”",Not discussed,,General,N/A,Targeted,"race, religion, gender, sexuality",Yes,No,,,No,,No,,,No,Human
118,No,"(Baheti, Sap, Ritter, & Riedl, 2021)",Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts,arXiv preprint,USA,"To better understand the dynamics of contextually offensive language, they investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, they create TOXICHAT, a crowd-annotated dataset of 2,000 Reddit threads and model responses labelled with offensive language and stance. The analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. The results highlight the need for further efforts to characterize and analyze inappropriate behaviour in dialogue models, in order to help make them safer.",Yes,25,13.09.2021,03.01.2023,Yes,Yes,"Offensiveness. ""We consider ui offensive if it is intentionally or unintentionally toxic, rude or dis- respectful towards a group or individual following Sap et al. (2020).""",No,,Yes,Github,,https://github.com/abaheti95/ToxiChat,Yes,ToxiChat,2,1,Reddit,Offensiveness,English,N/A,XX.05.2019-XX.09.2019,N/A,N/A,"They gather Reddit posts and comments (Baumgartner et al., 2020) that were written between May and October 2019. From this, they construct threads, each of which comprises a title, post and subsequent comment sequence. They extract threads from two sources: (1) Any SubReddits: threads from all SubReddits, (2) Offensive Sub-Reddits: threads from toxic SubReddits identified in previous studies (Breitfeller et al., 2019) and Reddit community reports. 

They implement a two-stage sampling strategy: (1) Random sample - From both sources, randomly sample 500 threads (total 1000). (2) Offensive sample - From the remaining threads in both sources, sample additional 500 threads (total 1000), whose last comment is predicted as offensive by a classifier. Specifically, they used high-precision predictions (probability ≥ 0.7) from a BERT-based offensive comment classifier (Devlin et al., 2019) that was fine-tuned on the Social Bias Inference Corpus (Sap et al., 2020). This classifier achieves ≈ 85.4 Offend label F1 on the SBIC dev set.",Not discussed,,manual,Not discussed,,Not discussed,,2000,1400,300,Five crowd-workers from the Amazon Mechanical Turk platform,Not discussed,,Not discussed,,Specific,"race, gender, sexuality, religion, age, body, disability, class, Political, other",Non-targeted,N/A,Yes,Yes,"Race/Ethnicity, Gender/ Sexuality, Religion, Age/Body, Victims/Disabilities, Class /Political","race, gender, sexuality, religion, age, body, disabilities, class, Political, other",Yes,List of all the targets in table 7,Yes,"celebrity/ personality, individual/ redditor, comment author","About a person,To a person",No,Human
121,No,"(Fast, Vachovsky, & Bernstein, 2016)",Shirtless and dangerous: Quantifying linguistic signals of gender bias in an online fiction writing community,Tenth International AAAI Conference on Web and Social Media,USA,"In this paper, they present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. They apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors’ use of gender stereotypes is associated with the community’s ratings. They find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in the corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.",Yes,62,XX.XX.2016,03.02.2023,Yes,Yes,"Gender bias. ""Gender bias and its stereotypes retain significant force in modern culture.""",No,,No,,,,,Fiction dataset,1,1,Wattpad,"Gender Bias, Gender Stereotypes ",English,N/A,N/A,N/A,Not discussed,"The dataset contains more than 1.8 billion words selected from a random sample of 600,000 stories, written by more than 500,000 writers across twenty genres. Wattpad provided them with all data, as well as gender annotations for 475,000 authors, 46% of which are by women and 54% of which are by men.",Not discussed,,N/A,Not discussed,,Yes,Random sampling,1.8 million,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,gender,Targeted,gender,No,Yes,Gender,gender,Yes,"Male, Female",No,,,No,Human
124,No,"(Cecillon, Labatut, Dufour, & Linares, 2020)",WAC: A Corpus of Wikipedia Conversations for Online Abuse Detection,arXiv preprint ,France,"In this work, they propose an original framework, based on the Wikipedia Comment corpus, with comment-level abuse annotations of different types. The major contribution concerns the reconstruction of conversations, by comparison to existing corpora, which focus only on isolated messages (i.e. taken out of their conversational context). This large corpus of more than 380k annotated messages opens perspectives for online abuse detection and especially for context-based approaches. They also propose, in addition to this corpus, a complete benchmarking platform to stimulate and fairly compare scientific works around the problem of content abuse detection, trying to avoid the recurring problem of result replication. Finally, they apply two classification methods to the dataset to demonstrate its potential.",Yes,4,13.03.2020,03.01.2023,Yes,Yes,"Online abuse. ""…different types of abuse:
• personal attack: abusive content directed at some- body’s person rather than providing evidence;
• aggression: malicious remark to a person or group on characteristics such as religion, nationality or gender;
• toxicity: comment that can make other people want to leave the conversation.""",Yes,"Personal attack, Aggression, Toxicity",Yes,Other,Figshare,https://figshare.com/articles/dataset/Wikipedia_Abusive_Conversations/11299118,Yes,WAC,4,1,Wikipedia,Abusiveness,English,N/A,N/A,N/A,Not discussed,"WAC is a combination of the first two corpora de- scribed in Section 2. and takes advantage of their comple- mentarity. It is based on the messages and conversations structure from WikiConv (Hua et al., 2018) and the human annotations for 3 different types of abusive content from the WCC (Wulczyn et al., 2017).",Not discussed,,manual,Yes,"The first step is to extract annotations from the WCC. This corpus provides 10 judgments per annotated comment. Each judgement provides multiple annotations de- pending on the dataset. The Personal attack dataset has 5 binary annotations: quoting_attack, recipient_attack, third_party_attack, other_attack and the more general at- tack. The Aggression and Toxicity datasets also provides such a general binary score (aggression and toxicity, respectively). Additionally, they provide an aggression_score and a toxicity_score) ranging from −2 (very abusive) to 2 (very healthy), 0 being neutral. They aggregate these 10 judgments to determine the gold annotation of all the annotated messages. For the binary annotations, they compute the majority annotation among crowdworkers to determine the gold standard. For the scores, they compute the average value among all crowdworkers. In WAC, they call annotated messages the annotated comments extracted from WCC. ",Not discussed,,383k,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"religion, nationality, gender",Yes,No,,,No,,No,,,No,Human
127,No,"(Liu et al., 2018)",Forecasting the presence and intensity of hostility on Instagram using linguistic and social features,International AAAI Conference on Web and Social Media,USA,"In this paper the authors consider the problem of forecasting future hostilities in online discussions, which they decompose into two tasks: (1) given an initial sequence of non-hostile comments in a discussion, predict whether some future comment will contain hostility; and (2) given the first hostile comment in a discussion, predict whether this will lead to an escalation of hostility in subsequent comments. Thus, they aim to forecast both the presence and intensity of hostile comments based on linguistic and social features from earlier comments. To evaluate their approach, they introduce a corpus of over 30K annotated Instagram comments from over 1,100 posts. The approach is able to predict the appearance of a hostile comment on an Instagram post ten or more hours in the future with an AUC of .82 (task 1), and can furthermore distinguish between high and low levels of future hostility with an AUC of .91 (task 2).",Yes,65,XX.06.2018,16.09.2022,Yes,Yes,"""Hostile comments that are common to a number of types of unwanted behavior including harassment and aggression. We deﬁne a hostile comment as one containing harassing, threatening, or offensive language directed toward a speciﬁc individual or group.""",No,,Yes,Github,,https://github.com/tapilab/icwsm-2018-hostility,Yes,N/A,1,1,Instagram,Hostile Comments,English,Implicitly before June 2018,Implicitly before June 2018,N/A,N/A,N/A,Not discussed,,manual,No,,Not discussed,,"30,987",N/A,N/A,Amazon Mechnical Turk workers who had passed certification tests ,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
128,No,"(Blevins et al., 2016)",Automatically Processing Tweets from Gang-Involved Youth: Towards Detecting Loss and Aggression ,International Conference on Computational Linguistics,USA,"The authors present a corpus of tweets from a young and powerful female gang member and her communicators, which the authors have annotated with discourse intention, using a deep read to understand how and what triggered conversations to escalate into aggression. They use this corpus to develop a part-of-speech tagger and phrase table for the variant of English that is used and a classifier for identifying tweets that express grieving and aggression.
",Yes,50,XX.12.2016,16.09.2022,Yes,Yes,"""We use the dataset to develop a system that can automatically classify tweets as expressing either loss, grieving the death of friends or family who were shot, or aggression, threatening to harm others often in retribution for a loss. Tweets that don’t fall into either category are classiﬁed as other.""",No,,No,,,,,N/A,1,1,Twitter,"Agressiveness, Grieving",English,Implicitly before December 2016,"January, March 15th to April 17th of 2014",USA,N/A,Radian6,No,,manual,No,,Yes,They collected all tweets of and adressed to Gakirah Barnes. They started collecting two weeks prior the death of one of her friends and ended one week after her death.,820,616,102,Two annotators,Yes,They ought to perform POS tagging on the given tweets.,Not discussed,,Specific,other,Targeted,other,Yes,No,,,No,,Yes,Tweets directed to Gakirah Barnes,To a person,No,Human
129,No,"(Mathew et al., 2020)",Interaction dynamics between hate and counter users on Twitter,ACM IKDD CoDS,India,"In this paper, the authors investigate the interaction of hatespeech and the responses that counter it (aka counter-speech). One of the prime contribution of this work is that they developed and released a dataset where they annotate pairs of hate and counter users. They perform several lexical, linguistic and psycholinguistic analysis on these annotated accounts and observe that the couterspeakers of the target communities employ different strategies to tackle the hatespeech. The hate users seem to be more popular as they were observed to be more subjective, express more negative sentiment, tweet more and have more followers. While the hate users seem to use words more about envy, hate, negative emotion, swearing terms, ugliness, the counter users use more words related to government, law, leader. Finally, the authors build a classifier to detect if a user is a hateful or counter speaker. ",Yes,7,XX.01.2020,16.09.2022,Yes,Yes,"""We define hatespeech according to the Twitter guidelines. Any tweet that ‘promotes violence against other people on the basis of race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease’ is considered as a hatespeech. [...] We call a tweet as a ‘counterspeech’ if the tweet is a direct reply to a hateful tweet.""",No,,No,,,,,N/A,1,1,Twitter,Counterspeech,English,Implicitly before January 2020,Implicitly before January 2020,N/A,N/A,PHEME script,Not discussed,,manual,No,,Yes,They filtered out all tweets that did not have at least two replies.,1290,1161,129,Two undergraduate students with Majorin Computer Science and a PhD student in Social Computing,Yes,Identify whether a tweet contains hate speech or not. Decide if a reply is counterspeech or not.,Not discussed,,General,N/A,Targeted,"race, nationality, sexuality, gender, religion, age, disability, body",No,No,,,No,,No,,,Yes,Human
130,No,"(Hu & Zhao, 2019)",The Impact of Online Harassment on the Performance of Projects in Crowdfunding ,Pacific Asia Conference on Information Systems,Hong Kong SAR,"This study attempts to investigate to what extent the textual online harassment score and the project creator’s attitude towards textual online harassment might affect project performance. The authors constructed a Kickstarter panel dataset consisting of 388,100 projects and designed a novel framework and an algorithm BiLSTM-CNN to extract the textual online harassment score from comments, which can reach column-wise mean ROC AUC of 0.9463.",Yes,0,15.06.2019,16.09.2022,Yes,Yes,"""Toxic comment is the textual form of offensive speech that are rude, disrespectful, hostile, aggressive or likely to make someone leave an online discussion, which differs from negative comments.""",No,,No,,,,,N/A,2,1,Kickstarter,Toxicity,English,XX.04.2014 - XX.12.2018,Implicitly before December 2018,N/A,N/A,N/A,Not discussed,,N/A,Not discussed,,No,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
132,No,"(Hossain et al., 2020)",A Framework for Political Portmanteau Decomposition,AAAI Conference on Web and Social Media,USA,"Portmanteaus are new words formed by combining the sounds and meanings of two words. Given their sticky nature, portmanteaus are often used to create political and personal attacks by combining a target entity with derogatory terms, which can then be spread online for promoting hate speech and defamation. In this paper, the authors present a framework to decompose political portmanteaus used online into their component words. Using their annotated dataset of political portmanteaus, they train a system that correctly decomposes 76.2% of the political portmanteaus into their component words. Furthermore, for 93.4% of the political portmanteaus, this system finds the correct component words in its top ten results, suggesting that using better ranking methods can lead to stronger results. This work provides a framework for both understanding an intriguing linguistic phenomena and for building hate-speech filters that could catch novel words that would bypass traditional hate speech detection approaches.",Yes,2,XX.05.2020,19.09.2022,Yes,Yes,"""A Political Portmanteau (PP) is a word created by combining two words X and Y such that: (1) at least two consecutive starting or ending letters of X and Y are used, (2) at least one letter from X and/or Y is removed, and (3) X and/or Y refers to a political entity. For example, “trumptanic” (Trump; titanic) and “hilliary” (Hillary; liar) are PPs, but “trumpland” violates (2), and “workoholic” (work; alcoholic) violates (3).""",Yes,"political portmanteau, portmanteau, prefix, suffix, spelling, name, other",Yes,Website,,https://cs.rochester.edu/u/nhossain/ppol-dataset.zip,Yes,PPOL,1,1,Reddit,Political Portmanteau,English,Implicitly between July 2019 and May 2020,January 2016 to July 2019,N/A,N/A,Searching for PP candidates that have at least one word beginning (or ending) with the 4 beginning (or ending) characters from a name in PEL (Political Entity List).,Not discussed,,manual,Not discussed,,Not discussed,,"1,473","1,178",147,Two political science college majors,Yes,"The annotators were asked to (i) categorize each candidate into one of the following classes: political portmanteau, portmanteau, preﬁx, sufﬁx, spelling, name or other (see examples in Table 1), (ii) identify the two component words that form the candidate if applicable, and (iii) label whether each candidate is offensive.",Not discussed,,Specific,political,Targeted,political,Yes,Yes,Political,political,Yes,Republicans / Democrats,Yes,"Trump, Obama, Hillary Clinton",About a person,No,Human
133,No,"(Risch & Krestel, 2018)",Delete or not Delete? Semi-Automatic Comment Moderation for the Newsroom,"Workshop on Trolling, Aggression and Cyberbullying",Germany,"In this paper, the authors studied the task of comment moderation. In contrast to the sub-task of hate speech detection, moderation needs to consider several other types of inappropriate comments, such as insults and defamation, but also unverifiable suspicions, which do not rely on plausible arguments or credible sources. Hence, the authors propose a semi-automatic, holistic approach, which includes comment features but also their context, such as information about users and articles. For evaluation, they present experiments on a novel corpus of 3 million news comments annotated by a team of professional moderators.",Yes,29,XX.08.2018,19.09.2022,Yes,Yes,"""The deﬁnition of inappropriate content is by no means clear and differs between different venues.
Many platforms have individual guidelines that users must adhere to and that moderators employ for content assessment. Even with these guidelines there is no precise boundary between appropriate and inappropriate comments. Not only obviously unlawful content, such as ethnic or racial slurs needs to be removed, but the range is wider: personal attacks against other users and the editors, profanity, spam, or off-topic conversations need to be detected and moderated.""",No,,No,,,,,N/A,1,1,Big German news website,Inappropriateness,German,Implicitly between April 2017 and August 2018,"January 1st, 2016 to March 31st, 2017",Germany,N/A,N/A,Not discussed,,N/A,Not discussed,,,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,race,No,No,,,No,,No,,,No,Human
134,No,"(Amjad et al., 2021)",UrduThreat@ FIRE2021: Shared Track on Abusive Threat Identification in Urdu,FIRE Virtual Conference,"Mexico, USA, Russia","This shared task address the task of abusive and threatening language detection in Urdu language. The authors presented two datasets: (i) Abusive and Non-Abusive language, (ii) Threatening and Non-Threatening language. The abusive dataset contains 1,187 tweets categorized as Abusive and 1,213 as Non-Abusive and the threatening dataset contains 4,929 tweets categorized as Non-Threatening and 1,071 as Threatening. They provided one baseline system for Subtask A (Abusive Language Detection) and three baseline systems for Subtask B (Threatening Language detection). The best performing system achieved an F-score value of 0.88 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based transformer models showed the best performance.",Yes,10,XX.12.2021,19.09.2022,Yes,Yes,Abusive behavior; Violent threats,No,,No,,,,,N/A,1,1,Twitter,"Abusiveness, Threats",Urdu,"Implicitly before December, 2021",Implicitly before December 2021,N/A,N/A,Twitter API,Not discussed,,manual,Not discussed,,Yes,They used a dictionary on abusive and threatening words to crawl tweets.,8400,"8,400 (doesn't add up)","5,050 (doesn't add up)",N/A,Yes,"The annotation guidelines were defined, which were based on Twitter policies to combat abusive behavior and violent threats all the tweets were manually annotated. For example, if a tweet contained abusive words, the tweet was annotated as an abusive tweet. Likewise, if a tweet contained threatening words, the tweet was annotated as a threatening tweet.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
135,No,"(Hammer et al., 2019)",THREAT: A Large Annotated Corpus for Detection of Violent Threats,International Conference on Content-Based Multimedia Indexing,Norway,In this paper the authors present a large dataset consisting of comments from 19 different YouTube videos. The videos were about controversial political and religious themes which created a lot of aggressive discussions. The dataset consists of a total of about 30 000 sentences extracted from circa 10 000 comments. For each of the sentences they provide annotations that indicate if the sentence is a violent threat (or sympathy with such) or not. This is the first publicly available dataset with such an annotation. ,Yes,6,XX.XX.2019,19.09.2022,Yes,Yes,"""Online discussions are often contaminated by abominable behavior like making violent threats, a development which clearly is a cause for concern. Social media providers thus struggle to provide good services to their users and often online threats are directed towards women, kids and vulnerable minorities""",No,,Yes,Website,,https://docs.google.com/forms/d/e/1FAIpQLScQTVDqROxIg4YSq1xJHkCkolhXStPbeW3gricJprNkTQZccw/viewform,Yes,THREAT,1,1,YouTube,Threats,English,Summer 2013,Implicitly before summer 2013,N/A,N/A,For 19 YouTube videos that are related to religious and political topics comments have been collected,Yes,the corpus is anonymized.,manual,Not discussed,,Not discussed,,"9,845",N/A,N/A,Two annotators,Yes,They ought to differentiate between threat and non-threat,Not discussed,,General,N/A,Targeted,"gender, age",Yes,No,,,No,,No,,,No,Human
136,No,"(Karan & Snajder, 2019)",Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context,Workshop on Abusive Language Online,Croatia,"The authors address the task of automatically detecting toxic content in user generated texts. They focus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, they perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments. Using an existing dataset of conversations among Wikipedia contributors as a starting point, they compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.",Yes,21,XX.08.2019,19.09.2022,Yes,Yes,"""While practically very useful, standard models are only applicable in a post-hoc scenario, i.e., to detect a toxic comment after if has already been posted. An alternate approach would be to have models detect situations that are likely to lead to toxic comments.""",No,,Yes,Website,,https://takelab.fer.hr/data/pretox/,Yes,N/A,2,1,Wikipedia,Toxicity,English,"Implicitly before August, 2019","Implicitly before August, 2019",N/A,N/A,They adapted an existing Wikipedia dataset,Not discussed,,N/A,Not discussed,,Yes,"They filtered all threads with less than two participants, they updated all comments to their most recent version, and flagged deleted comments.","949,000","755,000","83,000",N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
137,No,"(Ruhaila et al., 2020)","Automatic Labelling of Malay Cyberbullying Twitter Corpus using Combinations of Sentiment, Emotion and Toxicity Polarities","International Conference on Algorithms, Computing and Artificial Intelligence",Malaysia,"In this work, the authors have constructed a corpus of 219,444 tweets of common Malay cyberbullying words and 54,867 manually labelled by Malay language experts. They have also proposed a method that automatically labels short text data by using a combination of sentiment, emotion and toxicity polarities. Besides these, they have conducted an evaluation of the proposed method which yield high accuracy value, indicating that this method can be a suitable solution to automatic labelling of Malay tweets for cyberbullying context. Future work would include adding more Malay cyberbullying words.",Yes,2,XX.12.2020,20.09.2022,Yes,Yes,"""Labelling large corpuses can be a daunting task and worse, impractical. While existing data annotation platforms are readily available, such as the AmazonTurk which provides access to thousands of paid labelers, there is the important aspect of qualified labelers. Scarcity of qualified labelers in a particular domain or subject can deem these platforms useless. This is especially pronounced in low resource languages such as Malay.""",No,,No,,,,,N/A,1,1,Twitter,Cyberbullying,Malay,XX.01.2018 - XX.02.2019,Implicitly before February 2019,N/A,N/A,"Twitter API, random and keyword-based",Not discussed,,"automated, manual",Not discussed,,,,"54,867 (manually annotated)",N/A,N/A,Malay language experts,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
140,No,"(Del Bosque & Garza, 2014)",Aggressive text detection for cyberbullying,Mexican International Conference on Artificial Intelligence,Mexico,"In the present work, the authors have tackled aggressive text detection as a regression problem that consists of mapping a document to an aggressiveness score. They have defined a simple scale that ranges from zero to ten (where ten is the most aggressive) and assumed that aggressive text detection is a sub-task of sentiment analysis that is closely related to document polarity detection. Taking the former into account, they proposed and explored lexicon-based, supervised, fuzzy, and statistical approaches, which were tested over a Twitter repository. The results show that linear regression seems to be a solid candidate for scoring the documents, and that the use of profane language (swear words) seems also to be a key feature for the task.",Yes,39,XX.11.2014,20.09.2022,Yes,Yes,"Cyberbullying. ""The continuous intentional aggression over an indefense victim via electronic media is known as cyberbullying.""",No,,No,,,,,"f-dataset, b-dataset",2,2,Twitter,Cyberbullying,English,Implicitly before November 2014,Implicitly before November 2014,N/A,N/A,"They searched for the keyword ""school"", and related to this they searched for tweets containing ""bitch"" or ""fuck"".",Not discussed,,manual,Not discussed,,,,"f-dataset 281, b-dataset 110",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,other,Non-targeted,N/A,No,No,,,No,students ,No,,To a person,No,Human
141,No,"(Alenzi & Khan, 2019)",Application of Sentiment Lexicons on Movies Transcripts to Detect Violence in Videos ,International Journal of Advanced Computer Science and Applications,Saudi Arabia,The research work reports performance of two different sentiment lexicons when they were applied on video transcripts to detect violence in YouTube videos. The models were built using the existing sentiment lexicons. The dataset consists of 100 English video transcripts collected from the web and was annotated manually as violent and non-violent. Various experiments were performed on the dataset using English SentiWordNet (ESWN) and Vader Package with different text preprocessing settings. The Vader package outperformed the ESWN by providing 75% accuracy. ESWN results for all POS tagging with 66% accuracy were better than its result for adjectives POS tagging with 58% accuracy.,Yes,1,XX.XX.2019,20.09.2022,Yes,Yes,"""Videos on YouTube carry different content, which may contain many unwanted things such as violence. Violence is the cause of many problems, especially among children like aggression and bullying at home, in school and in public places.""",No,,No,,,,,N/A,1,1,YouTube,Violence,English,Implicitly before November 2019,Implicitly before November 2019,N/A,N/A,They searched the internet for scripts of Anime shows that they found on YouTube,Not discussed,,manual,No,,No,,100 scenes,N/A,N/A,Three persons including the researchers,Not discussed,,Not discussed,,General,N/A,Targeted,age,Yes,No,,,No,,No,,,No,Human
142,No,"(Shulginov et al., 2021)",Automatic Detection of Implicit Aggression in Russian Social Media Comments ,Conference: Computational Linguistics and Intellectual Technologies,Russia,"This article studies the characteristics of implicit and explicit types of aggression in the comments of a Russian social network with the means of machine learning. As it is hypothesized that expression of aggression depends on local norms, the dataset contains the comments collected from a single social media community. These comments were divided into three classes: polite communication, implicit aggression, and explicit aggression. Trying different combinations of data preprocessing, we discovered that lemmatization and replacement emojis with placeholders contribute to better results. The authors tested several models Naive Bayes, Logistic Regression, Linear Classifiers with SGD Training, Random Forest, XGBoost, RuBERT) and compared their results. The study describes the misclassifications and compares the keywords of each class of comments. The results can be helpful while enhancing the algorithm of detection of implicit aggression. ",Yes,1,XX.XX.2021,20.09.2022,Yes,Yes,"""Aggression has not been studied properly because aggression is a complex sociocultural phenomenon. Verbal behaviour cannot be described by the dichotomy of aggression and politeness. It is more likely to be a continuum between two poles: completely rude interaction and polite respectful interaction [Locher, 2006].""",Yes,explicitly and implicitly aggressive,Yes,Github,,https://github.com/alinatl/Implicit-Aggression,Yes,Implicit-Aggression,2,2,VK,Aggressiveness,Russian,Implicitly before 2021,Implicitly before 2021,N/A,N/A,"They searched the ""BORSCH"" community of VK for comments.",Not discussed,,manual,No,,Not discussed,,"7,225 (imbalanced dataset)/ 5,687 (balanced dataset)","5,058 (imbalanced)/ 3,984 (balanced)","2,167 (imbalanced)/ 1,703 (balanced)",N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
143,No,"(Kouvela et al., 2020)",Bot-Detective: An explainable Twitter bot detection service with crowdsourcing functionalities,International Conference on Management of Digital EcoSystems,Greece,"In this work the authors propose Bot-Detective, a web service that takes into account both the efficient detection of bot users and the interpretability of the results as well. The main contributions are summarized as follows: i) they propose a novel explainable bot-detection approach, which, to the best of authors' knowledge, is the first one to offer interpretable, responsible, and AI driven bot identification in Twitter, ii) they deploy a publicly available bot detection Web service which integrates an explainable ML framework along with users feedback functionality under an effective crowdsourcing mechanism; iii) they build the proposed service under a newly created annotated dataset by exploiting Twitter's rules and existing tools. This dataset is publicly shared for further use. In situ experimentation has showcased that Bot-Detective produces comprehensive and accurate results, with a promising service take up at scale.",Yes,6,02.11.2020,21.09.2022,Yes,Yes,"""However, Twitter's large social and economic impact in these sectors, has attracted a dark side of users who act maliciously with efforts to manipulate and influence people and their decisions. Social bots, i.e. code functions which operate as human accounts, proceed with automated actions that resemble those that people would do, are widely used and deployed in Twitter.""",No,,Yes,Website,,https://bot-detective.csd.auth.gr/datasets,Yes,bot-detective-2020,1,1,Twitter,Social Bots,English,Period of 20 days before November 2020,Implicitly before November 2020,N/A,N/A,"They used the Twitter API, and searched hashtags regarding cryptocurrencies such as #crypto, #cryptocurrency, #ico",Not discussed,,automated,No,,Yes,They filtered out all non-English tweets.,N/A,N/A,N/A,"Botometer, a model for giving Tweets a score from 1-10 regarding their likelihood of being a bot.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Non-human
145,No,"(Ruwandika & Weerasinghe, 2018)",Identification of Hate Speech in Social Media  ,International Conference on Advances in ICT for Emerging Regions,Sri Lanka,"An exploration of different approaches to detect hate speech in social media is present in this paper. The experiment was carried out using a local English text dataset created by the authors from an online news website. Hate speech is defined as the usage of language to insult or spread hatred towards a group or individual based on religion, race, gender or social status for the experiment. Then a comparison of both supervised and unsupervised learning techniques with different feature types for the task of hate speech detection was done. From all the supervised and unsupervised models Naïve Bayes classifier with Tf-idf features performed best with an F-score of 0.719.",Yes,30,26.09.2018,21.09.2022,Yes,Yes,"""Hate speech on social media is a common issue seen at present which is growing really fast. Due to the growth of online hate content there’s a huge influence for the increase of hate crimes in the society. So, if an accurate efficient methodology can be found to control the online hate content, it will be a great relief to the society. This research represents a study carried out to compare different techniques for the task of hate speech identification of a local English dataset. """,No,,No,,,,,N/A,1,1,Colombo Telegraph,Hate Speech,English,Implicitly before December 2017,April to May of 2017,N/A,N/A,All articles with more than 25 comments were collected,Not discussed,,manual,No,,,,1000,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
146,No,"(Chatzakou et al., 2017)",Mean Birds: Detecting Aggression and Bullying on Twitter,ACM on web science conference,"Greece, UK","In this paper, the authors present a principled and scalable approach to detect bullying and aggressive behavior on Twitter. They propose a robust methodology for extracting text, user, and network-based attributes, studying the properties of bullies and aggressors, and what features distinguish them from regular users. They find that bullies post less, participate in fewer online communities, and are less popular than normal users. Aggressors are relatively popular and tend to include more negativity in their posts. They evaluate their methodology using a corpus of 1.6M tweets posted over 3 months, and show that machine learning classification algorithms can accurately detect users exhibiting bullying and aggressive behavior, with over 90% AUC.",Yes,366,25.06.2017,22.09.2022,Yes,Yes,Bullying and aggressive behavior on Twitter,Yes,"agressive, bullying, or spammer",Yes,Other,Upon request,,No,N/A,1,1,Twitter,"Bullying, Aggressiveness",English,XX.06.2016 - XX.08.2016,Implicitly before August 2016,N/A,#gamergate,Twitter's Streaming API,Not discussed,,manual,Not discussed,,Yes,They collected 1M random tweets and a set of 650K hate-related tweets.,"1,650,000","1,485,000","165,000","Crowdworkers from ""CrowdFlower"". 30% female, 70% male. From 56 different countries",Yes,"They ought to label each user, based on a batch of their tweets, as normal, aggressor, bully, or spammer.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
149,No,"(Dinu et al., 2021)",A Computational Exploration of Pejorative Language in Social Media,Findings of the Association for Computational Linguistics,"Romania, USA","In this paper the authors study pejorative language. They released a public lexicon of pejorative words in four languages (English, Spanish, Italian, and Romanian), as well as dataset of tweets annotated for pejorative uses of words. They have modelled pejorativity detection as a problem of disambiguation, and performed experiments using state-of-the-art contextual embeddings in order to automatically distinguish pejorative from non-pejorative uses of words, obtaining promising results.",Yes,5,XX.11.2021,22.09.2022,Yes,Yes,"""In this paper we address the question of pejorative words. Pejorative words are words or phrases that have negative connotations or that are intended to disparage or belittle. Pejorativity is closely related to the notion of slurs or insults: “as noun phrases, ‘insult’ and ‘slur’ refer to symbolic vehicles designed by convention to derogate targeted individuals or groups” """,No,,No,,,,,"PEJOR1, PEJOR2",5,3,Twitter,Pejorative Words,"English, Spanish",Implicitly before November 2021,Implicitly before November 2021,N/A,N/A,They searched different datasets for pejorative words to utilize these for their datasets.,Yes,Their use complied with the terms of use of each dataset.,manual,Not discussed,,,,"PEJOR1: 944, PEJOR2: 313",N/A,N/A,Specialists in linguistics,No,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
151,No,"(Alakrot et al., 2018)",Dataset Construction for the Detection of Anti-Social Behaviour in Online Communication in Arabic,International Conference on Arabaic Computational Linguistics,Ireland,"In this paper the authors present a dataset of YouTube comments in Arabic, specifically designed to be used for the detection of offensive language in a machine learning scenario. The dataset contains a range of offensive language and flaming in the form of YouTube comments. The authors document the labelling process they have conducted, taking into account the difference in the Arab dialects and the diversity of perception of offensive language throughout the Arab world. Furthermore, statistical analysis of the dataset is presented, in order to make it ready for use as a training dataset for predictive modelling.",Yes,50,17.11.2018,23.09.2022,Yes,Yes,"""Typically, these studies collect data from various reachable sources, the majority of the datasets being in English. However, to the best of our knowledge, there is no dataset collected from the YouTube platform targeting Arabic text and overall there are only a few datasets of Arabic text, collected from other social platforms for the purpose of oﬀensive language detection. Therefore, in this paper we contribute to this ﬁeld by presenting a dataset of YouTube comments in Arabic, speciﬁcally designed to be used for the detection of oﬀensive language in a machine learning scenario""",No,,No,,,,,N/A,1,1,YouTube,Offensiveness,Arabic,17.07.2017,2015 to July 2017,N/A,N/A,N/A,Not discussed,,manual,No,,Yes,They collected comments from videos about controversial media footage.,15050,N/A,N/A,Three annotators: 1st Iraqi 44 years old. 2nd Egyptian 24 y/o. 3rd from Libya 32 y/o.,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
153,No,"(Van Bruwaene et al., 2020)",A multi-platform dataset for detecting cyberbullying in social media,Lang Resources and Evaluation,Canada,"The authors develop a multi-platform dataset that consists purely of the text from posts gathered from seven social media platforms. They present a multi-stage and multi-technique annotation system that initially uses crowdsourcing for post and hashtag annotation and subsequently utilizes machine-learning methods to identify additional posts for annotation. The authors show that, despite the diversity of examples present in the dataset, good performance is possible for models trained on datasets produced in this manner. This becomes a clear advantage compared to traditional methods of post selection and labeling because it increases the number of positive examples that can be produced using the same resources and it enhances the diversity of communication media to which the models can be applied.",Yes,29,06.04.2020,23.09.2022,Yes,Yes,"""Cyberbullying, which can be deﬁned as ‘an aggressive, intentional act carried out by a group or individual, using electronic forms of contact, repeatedly and over time against a victim who cannot easily defend him or herself (Smith et al. 2008), has become a pernicious social problem in recent years.""",Yes,"bullying, cyberaggression",No,,,,,N/A,1,1,"YouTube, Tumblr, Twitter",Cyberbullying,English,Implicitly before April 2020,Implicitly before April 2020,N/A,N/A,"They sourced a list of relevant hashtags, and used these for data collection.",Not discussed,,manual,No,,,,9126,N/A,N/A,Three Mturk workers,Yes,"The annotators ought to label each post as ‘big meanie’ (if the person who posted it is being mean), ‘not sure’ (in case they were not sure), and ‘not meanie’ (everything else) according to how they would personally respond to the post.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
160,No,"(He et al., 2021)",Racism is a Virus: Anti-Asian Hate and Counterspeech in Social Media during the COVID-19 Crisis,International Conference on Advances in Social Networks Analysis and Mining,USA,"In this work, the authors study the evolution and spread of anti-Asian hate speech through the lens of Twitter. They create COVID-HATE, the largest dataset of anti-Asian hate and counterspeech spanning 14 months, containing over 206 million tweets, and a social network with over 127 million nodes. By creating a novel hand-labeled dataset of 3,355 tweets, they train a text classifier to identify hateful and counterspeech tweets that achieves an average macro-F1 score of 0.832. Using this dataset, they conduct longitudinal analysis of tweets and users. Analysis of the social network reveals that hateful and counterspeech users interact and engage extensively with one another, instead of living in isolated polarized communities. They find that nodes were highly likely to become hateful after being exposed to hateful content in the year 2020. Notably, counterspeech messages discourage users from turning hateful, potentially suggesting a solution to curb hate on web and social media platforms. ",Yes,25,XX.11.2021,26.09.2022,Yes,Yes,"""Hateful incidents throughout the world, such as acts of microaggression, physical and verbal abuse, and online harassment have increased during the pandemic. Following the identiﬁed origin of COVID-19 in China, racially motivated hate crime incidents have increasingly targeted the Chinese and the broader Asian communities, resulting in over 6,603 racially-motivated hateful incidents in the past year.""",Yes,"Anti-Asian COVID-19 Hate Tweets, COVID-19 Counterspeech Tweets, Neutral and Irrelevant Tweets",Yes,Website,,https://www.dropbox.com/sh/g9uglvl3cd61k69/AACEk2O2BEKwRTcGthgROOcWa?dl=0,Yes,COVID-HATE,1,1,Twitter,Anti-Asian Hate,English,15.01.2020 - (Implicitly before) 01.11.2021,January 15 2020 to March 26 2021,N/A,COVID-19 pandemic,"Twitter API, different ranges of keywords regarding covid-19, hate, and counterspeech",Not discussed,,manual,No,,Yes,"Out of the collected tweets, they sampled random tweets for annotation.","3,355",N/A,N/A,Two undergraduate students supervised by one of the co-authors,Yes,"The annotators ought to distinguish between hateful towards Asians, counterspeech, or neutral tweets.",Not discussed,,Specific,race,Targeted,race,Yes,No,,,Yes,Asian people,No,,,Yes,Human
161,No,"(Ghosh et al., 2022)",SEHC: A Benchmark Setup to Identify Online Hate Speech in English,IEEE Transactions on Computational Social Systems,India,"In this article, the authors create a multi-domain hate speech corpus (MHC) of English tweets that includes hate speech against religion, nationality, ethnicity, and gender in general and cover diverse domains, such as current affairs, politics, terrorism, technology, natural disasters, and human/drugs trafficking. Each instance in their dataset is manually annotated as hate or non-hate. The authors use the existing state-of-the-art models and present a stacked-ensemble-based hate speech classifier (SEHC) to identify hate speech from Twitter data. The results indicate that the proposed method may serve as a strong baseline for future studies using this dataset. ",Yes,1,21.03.2022,26.09.2022,Yes,Yes,"""Hate speech across different domains varies in the vocabularies [1], i.e., data distributions are different across domains. [...] While mainstream machine learning techniques can be compelling in narrow domains, they typically suffer from poor transfer learning ability and system reusability. One solution could be to learn a different system for each domain. However, this would imply a considerable cost to annotate training data for many domains and prevent us from exploiting the information shared across domains. An alternative strategy is to learn a single uniﬁed system on a dataset created from multiple domains that will discover intermediate abstractions that are shared and meaningful across domains.""",No,,Yes,Other,,https://docs.google.com/forms/d/e/1FAIpQLSdNgBgx0n-gyvlTGKDW6y34_mNLNvDLhZxgsNdd1gjbLi9C2w/viewform,Yes,MHC,1,1,Twitter,Hate Speech,English,XX.01.2018 - XX.05.2020,Implicitly before May 2020,N/A,N/A,Twython,Not discussed,,manual,No,,Not discussed,,"10,242",N/A,N/A,Three annotators,Yes,They ought to label each tweet as hate or non-hate,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
163,No,"(Suvarna & Bhalla, 2020)",#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media,Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,India,"In this work, the authors established the need to devise a computationally effective method to identify victim blaming language on Twitter. To achieve this, they proposed a single step transfer learning based classification method that effectively captures the unique linguistic structures of twitter data and victim blaming language. On a manually annotated dataset, their proposed approach could achieve significant improvement over existing methods that rely on custom textual features and popular deep learning based methods. ",Yes,4,XX.07.2020,27.09.2022,Yes,Yes,"""Victim blaming occurs when the victim of a crime or any wrongful act is held entirely or partially at fault for the harm that befell them (Coates et al., 2006). Additionally, “slut shaming” is a popular form of victim blaming which refers to attacking a person’s character on the basis of sexual activity, real or perceived (Ringrose and Renold, 2012).""",No,,No,,,,,N/A,1,1,Twitter,Victim Blaming,English,06.11.2019 - 29.11.2019,Implicitly before November 29 2019,N/A,N/A,"They used the Twitter API, and they used hashtags and common words related to sexual harassment.",Not discussed,,manual,No,,Yes,They removed tweets that have less than 33 characters,5070,3042,1014,Three students: Two social science students and a psychology student.,Yes,"The annotators are provided with a palette of victim blaming language, that consists of ""clothing, makeup of victim, victim's psychological state, victim's former/current job as a prostitute, victim's sexual history, victim's upbringing, locations, that suggest victim culpability, self-reporting""",Not discussed,,Specific,sexuality,Targeted,sexuality,Yes,No,,,Yes,rape victims,No,,,No,Human
165,No,"(Mahapatra et al., 2020)",A Novel Approach for Identifying Social Media Posts Indicative of Depression ,"IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security",India,"In this paper, the authors put forward a novel approach for identifying the social media posts that are indicative of depression with the help of Long Short-Term Memory (LSTM) and Natural Language Processing (NLP) methodologies, Word embedding, n-gram tokenization, and word2vec to identify the text that expresses feelings of depression and its related sentiments. The approach accurately predicts sentiment in the text through Deep Learning, which removes false positives by considering the immediate context of words. The data for this experiment has been scraped from public forums on Reddit. Labeling is made before analyzing the data, which allows post about a common topic to be grouped. Posts from multiple groups discussing depression and self-harm are taken as the positive class, while posts from miscellaneous, random groups are taken as the negative class. ",Yes,1,XX.12.2020,27.09.2022,Yes,Yes,"""One such area, where content moderation is the need of the hour, is identifying and flagging social media posts indicative of psychological illness and self-harm. Early and accurate flagging of such content can prevent injury and offer much-needed help, which is especially important in depression and self-harm, where studies have shown that people often openly discuss their problems on social media. Depressive disorders broadly exhibit the standard features, which is the ""presence of sad, empty, or irritable mood, accompanied by somatic and cognitive changes that significantly affect the individual's capacity to function"".""",No,,No,,,,,N/A,1,1,Reddit,Depression,English,Implicitly before December 2020,Implicitly before December 2020,N/A,N/A,"They searched subreddits discussing depression and other, more general, ones using the PushShift API",Not discussed,,N/A,No,,No,,529438,423550,105888,N/A,No,,No,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
166,No,"(Mou & Lee, 2021)","An Effective, Robust and Fairness-aware Hate Speech Detection Framework",IEEE Conference on Big Data,USA,"Existing hate speech detection methods have limitations in several aspects, such as handling data insufficiency, estimating model uncertainty, improving robustness against malicious attacks, and handling unintended bias (i.e., fairness). To bridge the gap, the authors design a data-augmented, fairness addressed, and uncertainty estimated novel framework. As parts of the framework, they propose Bidirectional Quaternion-Quasi-LSTM layers to balance effectiveness and efficiency. To build a generalized model, they combine five datasets collected from three platforms. Experiment results show that this model outperforms eight state-of-the-art methods under both no attack scenario and various attack scenarios, indicating the effectiveness and robustness of the model. ",Yes,0,XX.12.2021,27.09.2022,Yes,Yes,"""In this paper, we embrace the hate speech definition as “abusive speech targeting specific group characteristics” to take both generality and specificity of hate speeches into consideration.""",No,,Yes,Github,,https://github.com/GMouYes/BiQQLSTM,No,BiQLSTM,6,1,"Twitter, Wikipedia, Stormfront",Hate Speech,English,Implicitly before December 2021,Implicitly before December 2021,N/A,N/A,They selected accessible datasets from a variety of platforms,Not discussed,,manual,No,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
167,No,"(Beatty, 2020)",Graph-Based Methods to Detect Hate Speech Diffusion on Twitter,International Conference on Advances in Social Networks Analysis and Mining,USA,"In this paper, the authors investigate models to detect the spread of hate speech on Twitter based on its diffusion in the network graph. They experiment with a dataset of 10,000 tweets manually labelled as hate speech or not and show that classification based solely on the sharing graph yields strong F1 scores for the task and high hate speech detection precision. They also highlight the vulnerability of existing textual hate speech detection methods to adversarial attacks and demonstrate that while their methods do not outperform state-of-the-art text models, graph-based models provide robust detection mechanisms and are able to detect instances of hate speech that fool text classifiers. They find that graph convolutional networks produce the strongest hate speech F1 score of 0.58 and find other success with kernel methods. Finally, the authors also consider the effects of automated bots in the sharing of hate speech content and find the bots are insignificant in their experiments.",Yes,3,XX.12.2020,28.09.2022,Yes,Yes,"""Our research is motivated by a desire to uncover new methods of hate speech detection to complete existing detection methods. Previous research has analyzed online users who frequently post hate speech content, but few studies to our knowledge analyze the diffusion of hate speech online and whether diffusion patterns can offer predictive detection features.""",No,,No,,,,,N/A,2,1,Twitter,Toxicity,English,30.03.2017 - 09.04.2017,"Implicitly before April 9, 2017",N/A,N/A,Twitter API,Not discussed,,manual,No,,Yes,"They sampled parts of an existing Twitter dataset by Founta et al., and in addition, they resampled to gather more ""normal"" tweets.",10074,9066,1008,N/A,No,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
168,No,"(de Gibert et al., 2018)",Hate Speech Dataset from a White Supremacy Forum,Workshop on Abusive Language Online,Spain,"This paper describes a hate speech dataset composed of thousands of sentences manually labelled as containing hate speech or not. The sentences have been extracted from Stormfront, a white supremacist forum. A custom annotation tool has been developed to carry out the manual labelling task which, among other things, allows the annotators to choose whether to read the context of a sentence before labelling it. The paper also provides a thoughtful qualitative and quantitative study of the resulting dataset and several baseline experiments with different classification models. ",Yes,234,12.09.2018,28.09.2022,Yes,Yes,"""Although there is no universal deﬁnition for hate speech, the most accepted deﬁnition is provided by Nockleby (2000): “any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic”.""",No,,Yes,Github,,https://github.com/Vicomtech/hate-speech-dataset,Yes,N/A,1,1,Stormfront,Hate Speech,English,Implicitly between 2017 and 2018,2002 to 2017,N/A,N/A,They used web-scraping techniques,Not discussed,,manual,No,,Yes,They filtered out non-English text,"9,916","7,933","1,983",Three of the authors,Yes,"If the three premises that the given text is a a) deliberate attack, b) directed towards a specific group of people, and c) motivated by aspects of the group's identity, are true, the text is categorized as hate.",Not discussed,,Specific,race,Targeted,"race, gender, sexuality, nationality, religion",Yes,Yes,Race,Race,No,,No,,,No,Human
169,No,"(Alotaibi & Hasanat, 2020)",Racism Detection in Twitter Using Deep Learning and Text Mining Techniques for the Arabic Language ,International Conference of Smart Systems and Emerging Technologies,Saudi Arabia,"This work aims to contribute to the detection of cyber-racism in the Arab region by considering the complicated nature of the Arabic language through tailored data acquisition and multi-staged pre-processing models. Furthermore, it takes into account the specificity of the application of racism detection in comparison to more broad sentiment analysis models that validly call for complicated architectures. The proposed approach utilizes deep learning techniques due to their superiority given the availability of very large datasets. In detail, it employs both convolutional neural network (CNN) and metaheuristic optimization algorithm for the classification of racism-oriented tweets in the Twitter platform.",Yes,6,XX.11.2020,28.09.2022,Yes,Yes,"""In this work, deep learning methods will be used to develop racism detectors for the Twitter platform, one of the most popular homelands of cyber-racism in the Arab region.""",No,,No,,,,,N/A,1,1,Twitter,Racism,Arabic,"Implicitly before November, 2020","Implicitly before November, 2020",N/A,N/A,"Twitter API, using a list of keywords to collect racist content",Not discussed,,manual,No,,No,,N/A,N/A,N/A,A group of native speakers,No,,Not discussed,,Specific,race,Targeted,race,Yes,Yes,Race,Race,No,,No,,,No,Human
170,No,"(Paul et al., 2022)",COVID‑19 and cyberbullying: deep ensemble model to identify cyberbullying from code‑switched languages during the pandemic,Recent Advances on Social Media Analytics and Multimedia Systems: Issues and Challenges,India,"In multilingual societies like India, code-switched texts comprise the majority of the Internet. As a first step towards enabling the development of approaches for cyberbullying detection, the authors developed a new code-switched dataset, collected from Twitter utterances annotated with binary labels. To demonstrate the utility of the proposed dataset, they build different machine learning (Support Vector Machine & Logistic Regression) and deep learning (Multilayer Perceptron, Convolution Neural Network, BiLSTM, BERT) algorithms to detect cyberbullying of English-Hindi (En-Hi) code-switched text. The proposed model integrates different hand-crafted features and is enriched by sequential and semantic patterns generated by different state-of-the-art deep neural network models. Initial experimental results of the proposed deep ensemble model on the code-switched data reveal that their approach yields state-of-the-art results, i.e., 0.93 in terms of macro-averaged F1 score. ",Yes,3,08.01.2022,28.09.2022,Yes,Yes,"""Cyberbullying is broadly categorized into different forms as it appears to be, e.g., racism (facial features, skin colour), sexism (male, female), physical appearance (ugly, fat), intelligence (ass, stupid) and others. [...] There have been quite a few instances where people were also disowned their family members for testing positive with COVID-19. Those people were bullied over social media, instead of spreading awareness, they have been mistreated, refused for medical assistance. Therefore, identifying these instances have become utmost important in order to prevent cyberbullying to occur any further or for taking any appropriate action against the  offender.""",No,,Yes,Github,,https://github.com/95sayanta/COVID-19-and-Cyberbullying,No,COVID-19 and Cyberbullying,1,1,Twitter,Cyberbullying,Hindi-English,28.01.2020 - XX.04.2020,"Implicitly before April, 2020",N/A,N/A,"Twitter's streaming API, Tweepy. They searched the hashtags ""Coronavirus, COVID19, Lockdown, CoronavirusDelhi, CoronavirusOutbreak, and CoronavirusPandemic""",Not discussed,,manual,No,,No,,22680,N/A,N/A,The authors,No,,Not discussed,,Specific,other,Targeted,"race, gender, body, other",Yes,No,,,Yes,COVID-19,No,,,No,Human
171,No,"(Khan et al., 2021)",Hate Speech Detection in Roman Urdu,ACM Transactions on Asian and Low-Resource Language Information Processing,Pakistan,"In this study, the authors have scrapped more than 90,000 tweets and manually parsed them to identify 5,000 Roman Urdu tweets. Subsequently, they have employed an iterative approach to develop guidelines and used them for generating the Hate Speech Roman Urdu 2020 corpus. The tweets in the this corpus are classified at three levels: Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another contribution, they have used five supervised learning techniques, including a deep learning technique, to evaluate and compare their effectiveness for hate speech detection. The results show that Logistic Regression outperformed all other techniques, including deep learning techniques for the two levels of classification, by achieved an F1 score of 0.906 for distinguishing between Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate speech tweets.",Yes,12,09.03.2021,29.09.2022,Yes,Yes,"""Hate speech is a specific type of controversial content that is defined as “an abusive speech targeting specific group characteristics, such as ethnicity, religion, or gender”. [...]  It [Roman Urdu] is the official language of Pakistan, which is a country of 220 million inhabitants. Apart from Pakistan, Urdu is spoken in many countries, including India, the UK, the United States, Canada, and the Middle East. However, there are several challenges of generating digital content in Urdu.""",Yes,"Hate Speech, Offensive",Yes,Other,kaggle,https://www.kaggle.com/datasets/drkhurramshahzad/hate-speech-roman-urdu,Yes,HS-RU-20,1,1,Twitter,Hate Speech,Urdu,"Implicitly before March, 2021","Implicitly before March, 2021",South Asia,N/A,Twitter Python API. ,Not discussed,,manual,No,,Yes,"They searched hashtags, that are likely to contain hate; such as #IndianHitlerModi","5,000",N/A,N/A,The authors,Yes,"They gave definitions for Hate Speech (""A hostile or sweeping sentence that exhibits a clear desire to be cruel, promote hatred, incite harm, or attacks a group, on the bases of their characteristics, such as religion, race, sex, national origin, ethnic origin, disability, sexual orientation, gender originality, or politics, and may provoke them to take revenge""), Offensive Language (""This includes a sentence that often conveys the purpose of insulting groups and can include disrespectful, hurtful, or abusive language""), and Neutral (""A statement to express happiness or does not contain hurtful content is a neutral sentence"")",Not discussed,,General,N/A,Targeted,"race, religion, gender",Yes,No,,,No,,No,,,No,Human
173,No,"(Pandey et al., 2018)",Distributional Semantics Approach to Detect Intent in Twitter Conversations on Sexual Assaults,International Conference on Web Intelligence,USA,"This multidisciplinary study investigates Twitter posts related to sexual assaults and rape myths (e.g., 'women lie about rape') for characterizing the types of malicious intent, which leads to the beliefs on discrediting women and rape myths. Specifically, the authors first propose a novel malicious intent typology for social media using the guidance of social construction theory from policy literature that includes Accusational, Validational, or Sensational intent categories. They then present and evaluate a malicious intent classification model for a Twitter post using semantic features of the intent senses learned with the help of convolutional neural networks. Lastly, they analyze a Twitter dataset of four months using the intent classification model to study narrative contexts in which malicious intents are expressed and discuss their implications for gender violence policy design.",Yes,14,03.12.2018,29.09.2022,Yes,Yes,"""This multidisciplinary study investigates Twitter posts related to sexual assaults and rape myths for characterizing the types of malicious intent, which leads to the beliefs on discrediting women and rape myths.""",Yes,"accusational, validational, and sensational",No,,,,,myth dataset,1,1,Twitter,"Rape, Sexual Assault",English,01.08.2016 - 01.12.2016,Implicitly before December of 2016,N/A,N/A,"Twitter Streaming API, using the keywords 'rape' and 'sexual assault'. In addition using another list of keywords for positive cases: 'lie, lying, lied, liar, hoax, fake, false, fabricated, made up'",Not discussed,,manual,No,,Yes,"They randomly sampled 2,500 tweets out of 112,369 tweets.",2500,N/A,N/A,Three annotators,Yes,"'Accusational': ""messages express doubts about or undermine accusers; express more concern for the accused than the accuser; and/or perpetuate the idea that women lie about rape."" / 'Validational': ""messages express belief in the accuser; and /or point out the injustice of the crime for an accuser or accused, and/or the inadequacy of the punishment."" / 'Sensational': ""messages focus more politics or provocation than on the issue of rape or sexual assault; intent may be primarily to frighten, politicize or sensationalize with these terms, but not to afﬁrm, accuse or meaningfully inform regarding rape or sexual assault.""",Not discussed,,Specific,sexuality,Targeted,"gender, sexuality",Yes,No,,,Yes,Rape / Sexual Assault victims,No,,,No,Human
174,No,"(Trana et al., 2020)",Fighting Cyberbullying: An Analysis of Algorithms Used to Detect Harassing Text Found on YouTube,International Conference on Applied Human Factors and Ergonomics,USA,"Given the increase in cyberbullying, the goal of this study is to develop a machine learning classification schema to minimize incidents specifically involving text extracted from image memes. To provide a current corpus for classification of the text that can be found in image memes, the authors collected a corpus containing approximately 19,000 text comments extracted from YouTube. Three MT workers classified each comment in the corpus as bullying or non-bullying. The authors report on the efficacy of three machine learning classifiers, naive Bayes, Support Vector Machine, and a convolutional neural network applied to a YouTube dataset, and compare the results to an existing Formspring dataset. Additionally, they investigate algorithms for detecting cyberbullying in topic-based subgroups within the YouTube corpus.",Yes,6,04.07.2020,29.09.2022,Yes,Yes,"""Cyberbullying is a form of harassment that occurs through online communication with the intention of causing emotional distress to the intended target(s). Given the increase in cyberbullying, our goal is to develop a machine learning classiﬁcation schema to minimize incidents speciﬁcally involving text extracted from image memes.""",No,,No,,,,,N/A,2,1,YouTube,Cyberbullying,English,October 2019 to January 2020,"Implicitly before January, 2020",N/A,N/A,"YouTube API, searching comments of videos regarding controversial topics",Not discussed,,manual,No,,No,,18776,N/A,N/A,Crowdworkers fom Amazon Mechanical Turk,No,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
176,No,"(Pramanick et al., 2021)",MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets,Findings of the Association for Computational Linguistics,"USA, India, Romania, Qatar","The authors focus on two tasks: (i)detecting harmful memes, and (ii) identifying the social entities they target. They further extend the recently released HarMeme dataset, which covered COVID-19, with additional memes and a new topic: US politics. To solve these tasks, they propose MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a novel multimodal deep neural network that uses global and local perspectives to detect harmful memes. MOMENTA systematically analyzes the local and the global perspective of the input meme (in both modalities) and relates it to the background context. MOMENTA is interpretable and generalizable, and the experiments show that it outperforms several strong rivaling approaches.",Yes,21,22.09.2021,30.09.2022,Yes,Yes,"""Following Pramanick et al. (2021b), we abridge the deﬁnition of harmful meme as follows: a multimodal unit consisting of an image and an embedded text that has the potential to cause harm to an individual, an organization, a community, or society.""",Yes,"1st harmfulness, 2nd target, organization, community, society: individual, ",Yes,Github,,https://github.com/lcs2-iiitd/momenta,Yes,Harm-C / Harm-P,2,2,Google Image,Harmful Memes,English,"Implicitly before September, 2021","Implicitly before September, 2021",N/A,N/A,"They used an extension for Google Chrome, and searched Google Images by keywords regarding US politics and COVID-19",Not discussed,,manual,No,,Not discussed,,"Harm-C 3,544 / Harm-P 3,552",N/A,N/A,"15 annotators, all of them experts in NLP or linguistics, 22-40 years old, 10 male, 5 female",Yes,"1st the annotators ought to distinguish between harmful and non-harmful. If harmful, they distinguish between individual, organization, community, and society as possible targets",Yes,payment according to the standard local pay rate,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
180,No,"(Chowdhury et al., 2020)",A Multi-Platform Arabic News Comment Dataset for Offensive Language Detection,Proceedings of the 12th Language Resources and Evaluation Conference,Qatar,"In this paper, the authors introduce and make publicly available a new dialectal Arabic news comment dataset, collected from multiple social media platforms, including Twitter, Facebook, and YouTube. We follow two-step crowd-annotator selection criteria for low-representative language annotation task in a crowdsourcing platform. Furthermore, they analyze the distinctive lexical content along with the use of emojis in offensive comments. They train and evaluate the classifiers using the annotated multi-platform dataset along with other publicly available data. The results highlight the importance of multiple platform dataset for (a) cross-platform, (b) cross-domain, and (c) cross-dialect generalization of classifier performance.",Yes,33,11.05.2020,21.09.2022,Yes,Yes,"Current conceptualization categorizes offensive language usage, for social media, mostly as hate speech, including remarks attacking particular race, religion, nationality among others; vulgar or obscene and pornographic comments, including explicit and rude sexual references",Yes,Offensive: hate speech and vulgar (but not hate),Yes,Github,,https://github.com/shammur/Arabic-Offensive-Multi-Platform-SocialMedia-Comment-Dataset,Yes,"New dataset: Multi Platforms Offensive Language Dataset (MPOLD);

Previously published datasets: Egyptian Tweets and corresponding Comment Dataset (ETCD),  Levantine dataset (L-HSAB),  Deleted Comments Dataset (DCD)",4,1,"Twitter, Facebook, YouTube",Offensiveness,Arabic,Implicitly before 11.05.2020 (publication),The news are posted from 2011 to 2019; No specified time for the production of comments.,N/A,N/A,"The authors collected over 100k comments from Facebook, Twitter, and YouTube, for a well-reputed international news agency. They first collected all news content posted, from 2011 to 2019, by the news agency in their social media accounts. They collected the contents from each platform through their own API (YouTube, Facebook, and Twitter). Then, using each content ID, the comments for the content are collected. As Twitter does not provide the API for retrieving comments directly, they used the Standard Search API of Twitter which only provides comments for past 7 days only. To overcome this challenge, they periodically collected the comments in every 6 hours for a certain period to extract the complete comment history of the news post.",Yes,The comments are anonymized by replacing any user mentions with a ‘USER.IDX’ tag and urls with ‘URL’ tags.,manual,Yes,"The authors used Amazon Mechanical Turk (AMT), a well-known crowdsourcing platform, to obtain manual annotations of these 4000 news comments. For each comment, they collected 3 judgments. The pilot study helped to identified the reasonable incentive for the annotation task.

Two evaluation criteria of the annotator:

(1) answering a series of multiple-choice questions - designed by experts - that reflects the annotators’ language proficiency and understanding of the questions. The annotator must provide correct answers to 80% of the questions (i.e., 8 out of 10) to pass the qualification test. Once the annotators pass the test, they can attempt the main classification task.

(2) they used gold standard instances hidden in the designed Human Intelligence Tasks (HITs). For each HIT, they assigned 25 comments for the annotation and out of them, 5 were randomly assigned as the test questions. A threshold of 80% is again set for the selection criteria implying that 4 out of 5 gold instances must be correct in order for the assignment by an annotator to be accepted. These test questions (comments) are selected randomly from a pool of 60 comments and are annotated by the domain experts.

They assigned the label agreed by the majority (i.e. 2/3 ) of the annotator. Fleiss’s kappa was calculated: κ = 0.72. To assess the reliability of the crowd-annotations, they randomly selected 500 comments to be annotated by a domain expert. Using the expert annotation as the reference, they observed that the accuracy of our crowdsourced dataset is 94%.

They manually annotated the obtained 675 (16.88%) offensive comments, in the dataset, for hate speech and vulgar (but not hate) categories.",Yes,"They retained comments that includes 5 or more Arabic tokens apart from emojis, removed duplicate comments based on textual content. They then selected a random subset of 4000 comments.",4000,5-fold cross-validation,5-fold cross-validation,Recruited via Amazon Mechanical Turk (AMT),Yes,"The annotators were instructed to rely on their instinct and suggested to ignore their personal biases such as political, religious belief or their cultural background. To make the decision-making process easier, the authors provided elaborated and detailed instructions to the annotators with examples.

For the task, they asked the annotators to consider instances as offensive, if comment contain (a) abusive words; (b) explicit or implicit hostile intention that threats or project violence; (c) contempt, humiliate or to underestimate groups or individuals using – animal analogy, name calling, attacking their political ideologies or their disabilities, cursing, insulting religious beliefs, incitement racial and ethnic hatred, or other forms of insulting/profanity. Details of the annotation guideline and examples presented to the annotator, in each cases, are made publicly available: https://github.com/shammur/Arabic-Offensive-Multi-Platform-SocialMedia-Comment-Dataset/blob/master/annotation_guideline/annotation_guideline.pdf",Yes,1 cent per comment,General,N/A,Targeted,"race, religion, nationality, sexuality",Yes,No,,,No,,No,,,No,Human
181,No,"(Sangwan & Bhatia, 2022)",Denigrate Comment Detection in Low-Resource Hindi Language Using Attention-Based Residual Networks,ACM Transactions on Asian and Low-Resource Language Information Processing,India,"This research puts forward a model to detect online denigration bullying in low-resource Hindi language using attention residual networks. The proposed model Hindi Denigrate Comment–Attention Residual Network (HDC-ARN) intends to uncover defamatory posts (denigrate comments) written in Hindi language which stake and vilify a person or an entity in public. Data with 942 denigrate comments and 1499 non-denigrate comments is scraped using certain hashtags from two recent trending events in India: Tablighi Jamaat spiked Covid-19 (April 2020, Event 1) and Sushant Singh Rajput Death (June 2020: Event 2). Only text-based features are considered. The pre-trained word embedding for Hindi language from fastText is used. The model has three ResNet blocks with an attention layer that generates a post vector for a single input, which is passed through a sigmoid activation function to get the final output as either denigrate (positive class) or non-denigrate (negative class). An F-1 score of 0.642 is achieved on the dataset.",Yes,1,31.01.2022,21.09.2022,Yes,Yes,"Denigration is one of the most frequently used cyberbullying ploys to actively damage, humiliate, and disparage the online reputation of target by sending, posting, or publishing cruel rumours, gossip, and untrue statements.",No,,No,,,N/A,No,The dataset,1,1,Twitter,"Denigration, Cyberbullying",Hindi,"After two events (April 2020, Event 1) (June 2020, Event 2), implicitly before 31.01.2022 (publication)","After two events (April 2020, Event 1) (June 2020, Event 2), implicitly before 31.01.2022 (publication)",N/A,"Tablighi Jamaat spiked Covid-19 (April 2020, Event 1); Sushant Singh Rajput Death (June 2020, Event 2)","For extraction of tweets from Twitter, Tweepy Python library was used, which is a convenient way to access the Twitter API. Data was scraped using certain hashtags from two recent trending events in India.

Tablighi Jamaat spiked Covid-19 (April 2020, Event 1): Tweets were scraped using the most trending hashtags such as “#NizammudinIdiots”, “#TabligiJamaat”, “#MaulanaSaad”, and “#coronajihad”.

Sushant Singh Rajput Death (June 2020, Event 2): Tweets were collected using the most trending hashtags, such as “#JusticeForSSR”, #boycottkaranjohar”, “#nepotisminbollywood”, “KanganaRanaut”, “#AdityaChopra”, “#MaheshBhatt”, and “#AliaBhatt”.

Posts in any language other than Hindi were filtered out manually. Finally, a dataset with 942 denigrate and 1499 non-denigrate comments was created",Yes,User mentions (@) are removed,manual,Not discussed,,Not discussed,,2441,"898 or 1543 (leave-one-event-out approach, one event is used as a test set, while the other is used for training)","898 or 1543 (leave-one-event-out approach, one event is used as a test set, while the other is used for training)",N/A,Not discussed,,Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,"Tablighi Jamaat (blamed for contributing to the spread of the coronavirus); Sushant Singh Rajput (controversial, denigrate and toxic comments due to his unnatural death)",About a person,No,Human
182,No,"(Chen et al., 2017)",Aggressivity detection on social network comments,ACM International Conference Proceeding Series,Hong Kong SAR,"In this article, the authors introduce a text mining system that can detect whether a certain paragraph contains the aggressive sentiment, and demonstrate its performance with different classification models. In addition, it is observed that the system works well on both the manually curated dataset and the de facto dataset. Extensive experiments are conducted to validate the effectiveness of the proposed system and highlight some possible factors that contribute to the robust results.",Yes,6,25.03.2017,21.09.2022,Yes,Yes,"Aggressive comments…1) Comments contain personal remarks, insulters, critic, condemnation or sarcasm, etc. 2) Comments refer to sensitive topics. 3) Comments contain too much swear words or a curse.",No,,No,,,N/A,No,manually curated dataset and the de facto dataset,2,1,"Twitter, Facebook",Aggressiveness,English,Implicitly before 25.03.2017 (publication),Implicitly before 25.03.2017 (publication),N/A,N/A,"The authors manually add Tweets and Facebook comments to their datasets while their emotions and stickers are not taken into consideration. Besides their hand labelled comments, we also collect the social network comment data from ‘Sentiment140 Corpus’ (classified by the positivity should be modified). After the modification, polarities of the Tweets are tagged aggressive or unaggressive.",Yes,removing noisy information such as ‘@’ followed by the username,manual,Not discussed,,Not discussed,,N/A,5-fold cross validation,5-fold cross validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
183,No,"(Nobata et al., 2016)",Abusive Language Detection in Online User Content,Proceedings of the 25th International Conference on World Wide Web,"USA, India","In this work, the authors develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. They also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, they use the detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior.",Yes,963,11.04.2016,21.09.2022,Yes,Yes,"For our work, abusive language encompasses hate speech, profanity and derogatory language.",Yes,"Abusive: Hate Speech, Derogatory, Profanity",Yes,Website,,https://webscope.sandbox.yahoo.co,Yes,"Primary Data Set, Temporal Data Set, WWW2015 Data Set, Evaluation Data Set",4,3,Yahoo!,Abusiveness,English,Primary Data Set: between October 2012 and January 2014  Temporal Data Set: between April 2014 and April 2015  Evaluation Data Set: between March and April 2015,Primary Data Set: between October 2012 and January 2014  Temporal Data Set: between April 2014 and April 2015  Evaluation Data Set: between March and April 2015,N/A,N/A,"Primary Data Set: Data for our primary training models is sampled from comments posted on Yahoo! Finance and News during the period between October 2012 and January 2014. A completely random 10% subset of the comments that are posted each day on Yahoo! Finance and News articles are sent for review by Yahoo’s in-house trained raters. Further, all comments which are reported as “abusive” for any reason by visitors to the Finance and News portals are also sent to the raters for review and judgment. Such reports are termed community moderation.

Temporal Data Set: The data used in our temporal experiments (§5.4) are also sampled from comments posted on Yahoo! Finance and News; the period is between April 2014 and April 2015.

Evaluation Data Set: Several thousand comments were extracted between March and April 2015 for the raters to label.",Not discussed,,manual,Yes,"(1) Trained raters: A completely random 10% subset of the comments that are posted each day on Yahoo! Finance and News articles are sent for review by Yahoo’s in-house trained raters. Further, all comments which are reported as “abusive” for any reason by visitors to the Finance and News portals are also sent to the raters for review and judgment. Such reports are termed community moderation.

(2) The authors posted text judgment challenges as Human Intelligence Tests (HITs) on Amazon’s Mechanical Turk (AMT) and offered financial incentives for workers to participate in the study. Each HIT consisted of a set of 10 text passage instances that had to be attempted. The authors set the per worker HIT limit to 5, and so each worker could only judge at most 50 text passages (5 HITs x 10 text passage instances). In order for the Turkers to be able to judge the text passages as accurately as done by the in-house raters, the authors distilled the annotation guidelines used internally at Yahoo and provided example use-cases as well as how they should be potentially judged.",Yes,Random 10%; all comments which are reported as “abusive” for any reason by visitors to the Finance and News portals,"Primary Data Set: 2,150,176  Temporal Data Set: 1,174,509  Evaluation Data Set: 2000 comments are used",Primary Data Set: 80%  WWW2015 Data Set: 5-fold cross validation  Evaluation Data Set: -  Temporal Data Set: each training set consists of only 20k comments (in consecutive 20k slices),Primary Data Set: 20%,"(1) Yahoo’s in-house trained raters.
(2) Turkers recruited on Amazon’s Mechanical Turk (AMT)",Yes,"Annotation Instructions

Given the passage below, first rate the passage on whether the passage is clean or abusive. And if abusive,

• whether it contains hate speech

• whether it contains derogatory language

• whether it contains profanity

(1) Hate Speech: language which attacks or demeans a group based on race, ethnic origin, religion, disability, gender, age, disability, or sexual orientation/gender identity. Some examples:

• The JEWZ are Market Manipulators Hate them freaking Jews !! To bad Hitler did not finish them off !!!

• I hate the mall because there are too many unemployed apefricans and mudsharks.

• 300 missiles are cool! Love to see um launched into TelAviv! Kill all the gays there!

• EGYPTIANS ARE DESCENDED FROM APES AND PIGS. NUKE EGYPT NOW Ah me tu se ku sh

(2) Derogatory: language which attacks an individual or a group, but which is not hate speech. Some examples:

• All you perverts (other than me) who posted today, needs to leave the O Board. Dfasdfdasfadfs

• yikes...another republiCUNT weighs in....

(3) Profanity: language which contains sexual remarks or profanity. Some examples:

• T.Boone Pickens needs to take a minimum wage job in FL for a week. I guarantee he shuts the f up after that.

• Somebody told me that Little Debbie likes to take it up the A.$.$.

• So if the pre market is any indication Kind of like the bloody red tampons that you to suck on all day??",Yes,"(1) Yahoo’s in-house trained raters: not discussed

(2) Turkers recruited on Amazon’s Mechanical Turk (AMT): Workers who completed a HIT would be awarded US$ 0.20/HIT (this translates to US$0.02 for each text passage attempted).",General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
184,No,"(Moon et al., 2020)",BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection,Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,South Korea,"In this work, the authors first present 9.4K manually labeled entertainment news comments for identifying Korean toxic speech, collected from a widely used online news platform in Korea. The comments are annotated regarding social bias and hate speech since both aspects are correlated. The inter-annotator agreement Krippendorff's alpha score is 0.492 and 0.496, respectively. They provide benchmarks using CharCNN, BiLSTM, and BERT, where BERT achieves the highest score on all tasks. The models generally display better performance on bias identification, since the hate speech detection is a more subjective issue. Additionally, when BERT is trained with bias label for hate speech detection, the prediction score increases, implying that bias and hate are intertwined.",Yes,44,30.07.2020,21.09.2022,Yes,Yes,"""Social bias is defined as a preconceived
evaluation or prejudice towards a person/group
with certain social characteristics: gender, political affiliation, religion, beauty, age, disability, race,
or others. … Drawing upon those, we define hate speech in our
study as follows:
• If a comment explicitly expresses hatred
against individual/group based on any of the
following attributes: sex, gender, sexual orientation, gender identity, age, appearance, social
status, religious affiliation, military service,
disease or disability, ethnicity, and national
origin
• If a comment severely insults or attacks individual/group; this includes sexual harassment,
humiliation, and derogation … We often
see comments that are offensive to certain individuals/groups in a qualitatively different manner. We
identify these as offensive and set the boundary as
follows:
• If a comment conveys sarcasm via rhetorical
expression or irony
• If a comment states an opinion in an unethical,
rude, coarse, or uncivilized manner
• If a comment implicitly attacks individual/group while leaving rooms to be considered as freedom of speech""",Yes,"bias: gender, other, none  hate speech: hate, offensive, none",Yes,Github,,https://github.com/kocohub/korean-hate-speech,Yes,Korean HateSpeech Dataset,1,1,Korean entertainment news aggregation platform,Toxicity,Korean,"Implicitly after 29.02.2020, before 30.07.2020",01.01.2018 - 29.02.2020,N/A,N/A,The authors constructed the Korean hate speech corpus using the comments from a popular domestic entertainment news aggregation platform.,Not discussed,,manual,Yes,"Every comment was provided to three random annotators to assign the majority decision. Annotators are asked to answer two three-choice questions for each comment:

1. What kind of bias does the comment contain?

• Gender bias, Other biases, or None

2. Which is the adequate category for the comment in terms of hate speech?

• Hate, Offensive, or None

They are allowed to skip comments which are too ambiguous to decide.",Yes,"The authors draw 1,580 articles using stratified sampling and extract the top 20 comments ranked in the order of Wilson score (Wilson, 1927) on the downvote for each article. Then, they remove duplicate comments, single token comments (to eliminate ambiguous ones), and comments composed with more than 100 characters (that could convey various opinions). Finally, 10K comments are randomly selected among the rest for annotation.","9,381","7,896",974,32 annotators consisting of 29 workers from a crowdsourcing platform DeepNatural AI and three natural language processing (NLP) researchers.,Yes,"A. Existence of social bias

The first property is to note which social bias is implicated in the comment. Here, social bias means hasty guess or prejudice that ‘a person/group with a certain social identity will display a certain characteristic or act in a biased way’. The three labels of the question are as follows.

1. Is there a gender-related bias, either explicit or implicit, in the text?

• If the text includes bias for gender role, sexual orientation, sexual identity, and any thoughts on gender-related acts (e.g., “Wife must be obedient to her husband’s words”, or “Homosexual person will be prone to disease.”)

2. Are there any other kinds of bias in the text?

• Other kinds of factors that are considered not gender-related but social bias, including race, background, nationality, ethnic group, political stance, skin color, religion, handicaps, age, appearance, richness, occupations, the absence of military service experience, etc.

3. A comment that does not incorporate the bias

B. Amount of hate, insulting, or offense

The second property is how aggressive the comment is. Since the level of “aggressiveness” depends on the linguistic intuition of annotators, we set the following categorization to draw a borderline as precise as possible.

1. Is strong hate or insulting towards the article’s target or related figures, writers of the article or comments, etc. displayed in a comment?

• In the case of insulting, it encompasses an expression that can severely harm the social status of the recipient.

• In the case of hate, it is defined as an expression that displays aggressive stances towards individuals/groups with certain characteristics (gender role, sexual orientation, sexual identity, any thoughts on gender-related acts, race, background, nationality, ethnic group, political stance, skin color, religion, handicaps, age, appearance, richness, occupations, the absence of military service experience, etc.).

• Additionally, it can include sexual harassment, notification of offensive rumors or facts, and coined terms for bad purposes or in bad use, etc.

• Just an existence of bad words in the document does not always fall into this category.",Not discussed,,Specific,"gender, other",Targeted,"gender, political, religion, body, age, disability, race, sexuality, class, religion, nationality, organization/institution, other, ",Yes,Yes,"Gender, Other","gender, other",No,,No,,,No,Human
185,No,"(Kennedy et al., 2017)",Hack Harassment: Technology Solutions to Combat Online Harassment,Proceedings of the First Workshop on Abusive Language Online,"USA, Ireland","This work is part of the #HackHarassment initiative to use machine learning to identify online harassment in social media and comment streams. The authors introduce an improved cross-platform harassment dataset and a machine learning model built on the dataset. In this work, they build upon our initial results using version 1.0 of their dataset (Bastidas et al., 2016). They followed a supervised classification method that uses a data with gold-standard labeled comments and a set discriminating linguistic properties, or features, of each comment to predict the class membership of new or untrained comments. The features consisted primarily of n-gram and a small set of linguistic features on datasets drawn from The Guardian, Reddit, and Twitter.",Yes,50,30.07.2017,21.09.2022,Yes,Yes,Online harassment,No,,No,,,N/A,No,harassment dataset,1,1,"Guardian, Reddit, Twitter",Harassment,English,Summer 2016,Implicitly before summer 2016 (data collection)  Reddit:  from reddit_comments_all_2015;  Twitter: Additional tweets were scraped directly from Twitter during July 2016;,N/A,N/A,"(1) Reddit: Comments from Reddit were downloaded from a publicly available dataset on Google BigQuery, reddit comments all 2015. These comments were then filtered to those that had received at least 100 down votes. The initial version of the classifier was used to label these comments.
(2) Twitter: Data were comprised of two sources: manual curation and annotation of a pre-existing machine annotated dataset and a set of scraped tweets using proprietary sampling methods. The sampling should not be considered unbiased. The initial 5000 tweets were sourced from an online repository of tweets at. Additional tweets were scraped directly from Twitter during July 2016 using a custom twitterbot that queried on hot-button topics as keywords to the Twitter API.
(3) The Guardian: Comments were scraped from 15 articles covering hot-button or polarizing topics.",Not discussed,,"automated, manual",Yes,"(1) Reddit: The resulting 5700 harassing comments were then further manually labeled by an in-house team of analysts. Analysts were given instructions and examples for annotation of harassment or non-harassment. In addition, the raters were provided with an additional set of more fine-grained labels but instruction on annotation was not provided.). Each post was labeled independently by at least five Intel Security Web Analysts.
(2) Twitter: These additional tweets were first labeled by the early classifier and then manually labeled by the team.
(3) The Guardian",Not discussed,,"20,432 comments",80% (~16346),20% (~4086),"For manual annotation, the annotators are in-house team of Intel Security Web Analysts.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
187,No,"(Torregrosa et al., 2020)",Analyzing the relationship between relevance and extremist discourse in an alt-right network on Twitter,Social Network Analysis and Mining,Spain,"With the aim of analyzing this effect of the use of an extremist discourse in relation to the relevance of these users on their online network, this work studies the relationship between the use of indicators of extremist discourse from users belonging to an alt-right network on Twitter and their relevance on it. The network of alt-right users is created using the retweets of 96 accounts where the user relevance is measured by five different types of centrality metrics, including in-degree, eigenvector, k-shells, betweenness, and closeness. Both the linguistic indicators and the tone were analyzed using LIWC and VADER software. The network analysis outcomes show that user relevance on the network is indeed related to the use of an extremist discourse. Finally, this relationship is also tested on different corpus of texts and about different topics, being found that this relationship is more clear on retweets made by the users and when discussing about hate speech topics.",Yes,16,13.08.2020,21.09.2022,Yes,Yes,"Extremist discourse. ""The extremist discourse is understood as the particular terminology used by extremist groups on their messages, which reflects their ideas, cognitive processes, and emotional state, and which is measured using different linguistic indicators (Alizadeh et al. 2019; Figea et al. 2016; Forscher and Kteily 2017; Greven 2016; Grover and Mark 2019; Kaati et al. 2016; Lyons 2017; Panizo-LLedot et al. 2019; Torregrosa et al. 2019; Zhang et al. 2018).""",Yes,"Conspiracy, Hate Speech, Media, Neutral and Politics",No,,,N/A,No,The dataset,1,1,Twitter,Extremist Discourse,English,01.07.2019 - 01.11.2019,01.07.2019 - 01.11.2019,N/A,N/A,"The authors built a dataset, periodically checking Twitter’s API, which contains 116, 387 tweets posted by 96 alt-right Twitter accounts between the 1st of July and the 1st of November of 2019 (4 months). The 96 alt-right accounts were picked from the dataset created in Thorburn et al. (2018) work. That work used twelve twitter accounts advertised in posters of 2017 Unite the Right Rally in Charlottesville (Virginia) as seeds. Next, 546 accounts were randomly selected from all the accounts that followed six or more of these twelve aforementioned accounts and have published at least 20 tweets. Finally, those 546 accounts were manually labeled, reading their published tweets, to ensure that they were a valid representation of the alt-right ending with a dataset of 422 alt-right accounts.",Not discussed,,manual,Not discussed,,,,"422 alt-right accounts (116, 387 tweets)",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,political,No,No,,,No,,No,,,No,Human
188,No,"(Jambak & Setiawan, 2018)",The development of bahasa Indonesia corpora for machine learning model in combating cyber bullying: A case study of the Indonesian 2017 capital city governor election,Journal of Theoretical and Applied Information Technology,Indonesia,"The ultimate goal is to propose a machine learning algorithm for combating cyber bullying in Bahasa Indonesia. Bahasa Indonesia corpora have been developed and will be tested using the existing machine learning algorithms that work in Indonesia, English, and Hindi. The data had been scraped and derived from social media during the 2017 Indonesian capital city governor election. In this paper, the authors discussed the cyber bullying problems in Indonesia and why Indonesian need a tool to combat them. We compared the result of previous similar researches and we followed some recommendations in our research context and scope. This paper has produced feature space design and proposed algorithm that will be model or tool for detecting the cyber bullying. The best accuracy of this model is SVM kernel RBF (80.8%).",Yes,0,15.03.2018,21.09.2022,Yes,Yes,Cyberbullying,No,,No,,,N/A,No,Bahasa Indonesia corpora,1,1,"Facebook, Path, Twitter, Instagram, YouTube, comment of blogs",Cyberbullying,Indonesian,Implicitly before 15.03.2018 (publication),Implicitly before 15.03.2018 (publication),N/A,2017 Indonesian capital city governor election,"The corpora has theme about people reaction or respond to capital city governor election. The corpora would come from social media (Facebook, Path, Twitter, Instagram, YouTube, and comment of blogs) derived by R Studio and other in-house application, database and Comma-Separated Values (CSV) file.

With R Studio, the authors can filter many feeds from twitter. They get twitter feeds with their attribute such as Created-At, From-User, From-User-Id, To-User, To- User-Id, Language, Source, Text, Geo-Location-Latitude, Geo-Location-Longitude, Retweet-Count, and Id. In this research, the attribute used is only the Text. So the storage will be filtered until it has text attribute only.

As for the feeds from Facebook, Path, Instagram, YouTube, and comment of blogs, they collected the feeds by an in-house application. The attribute feeds for this feed will be same as the twitter feeds which was saved in the storage. In this research, there are 3 attributes: date, source, and text. Date is about when the feed is published in the social media. The source says about where the feeds is published whether it is twitter, Facebook, or others. Than text says about the feed or the sentence published. The all feeds that are stored or the corpora that will be assessed manually by linguist then the corpora will be values whether it is positive or negative.",Not discussed,,manual,Not discussed,,Not discussed,,N/A,2/3 of the dataset,1/3 of the dataset,"Every corpora must be assessed by linguist. Linguist is a person who has many experiences of writing and editing document that have selling price. In this research, there is a linguist and an editor who has been working in a media office group in 20 years. He has assessed many corpora.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
190,No,"(Åkerlund, 2020)",The importance of influential users in (re)producing Swedish far-right discourse on Twitter,European Journal of Communication,Sweden,"Using a combination of descriptive statistics, sentiment analysis and close readings of a collection of 74,336 Swedish tweets, this article explores the platform usage patterns of users who are influential in a Swedish far-right discourse on Twitter and how these users help to (re)produce far-right discourse. Specifically, it focuses on their use of platform functions and on language use. The analysis shows that influential users have a narrow focus in terms of the content they post and how they profile themselves. They are highly active, have more followers and produce more original content than other users. Surprisingly, while previous research has found that emotionally charged tweets are retweeted more and that highly popular and influential Twitter users tend to express more emotion while tweeting, influential users in this dataset often posted far-right content concealed as neutral, factual statements. This use of seemingly neutral language creates an inclusive far-right context, lets influential users evade responsibility for their content as well as facilitates more overtly hateful interpretations.",Yes,19,01.12.2020,21.09.2022,Yes,Yes,"Far-right discourse. ""Far-right views are rapidly being mainstreamed into established politics throughout Europe, and also in Sweden. Far-right discourse is also highly prominent on social media, where it successfully engages many social media users.""",Yes,Influential users; other users,No,,,N/A,No,the dataset,1,1,Twitter,Far-Right Discourse,Swedish,02.09.2018 - 22.11.2018,02.09.2018 - 22.11.2018,N/A,"Swedish general election (While not specifically focusing on the Swedish general election, this article takes advantage of a timespan when political discussions were particularly prominent in Swedish social media settings. )","Tweets were sampled via hashtags aimed at capturing Swedish far-right discourse on Twitter. The hashtags were selected via manual searches on Twitter, starting, due to speculation regarding the potential electoral success of Sweden’s largest far-right party, the Sweden Democrats (SD), with the hashtag ‘#SD’. Additional hashtags were then selected through snowball sampling, where tweets with one hashtag directed the search to other co-occurring hashtags. In total, 23 hashtags were sampled because of their re-occurring use in far-right discussions, ranging from those with party political affiliations and relating to the election, to those that more generally concerned anti-immigration perspectives, as well as those that were more or less covertly racist.",Yes,"As this article explores a smaller group of users in depth, particular care was taken not to reveal these users’ identities. Even though the dataset contains public figures, disclosing their identity could still have repercussions for them. Thus, a minimal number of quotes are presented in the analysis. The few quotes that have been used are, in all instances, translated from Swedish and have been slightly altered so they cannot be traced back to their source. Most often, data are presented at an aggregated and abstracted level to protect anonymity, and statistics concerning users’ accounts are presented using centrality measures so that no individuals can be identified.",N/A,Not discussed,,,,"74,336 tweets, retweets, replies and quote tweets, posted by 6809 users",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,political,No,No,,,No,,No,,,No,Human
191,No,"(Kanessa & Tulu, 2021)",Automatic Hate and Offensive speech detection framework from social media: the case of Afaan Oromoo language,2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),Ethiopia,"This research proposed the SVM with TF-IDF, N-gram, and W2vec feature extraction to construct dataset which is binary classifier to detect hate speech for Afaan Oromoo language. To construct dataset for this study, the authors crawled data from Facebook posts and comments by using Face pager and scrap storm API, then labeled the collected data to two class: hate and neutral. The general objective of this research is to design a framework which classify hate and neutral speech. The experiment is evaluated based on accuracy, F-score, recall and precision measurements. The framework based on SVM with n-gram combination with TF-IDF achieve 96% in all metrics.",Yes,1,22.11.2021,21.09.2022,Yes,Yes,"Hate speech. ""Hate speech is a defined in case of dimension, field of occurrence and phenomenon which associate with individuals, groups.""",No,,No,,,N/A,No,Afaan Oromoo hate speech dataset,1,1,Facebook,Hate Speech,Afaan Oromoo,Implicitly before 22.11.2021 (publication),Implicitly before 22.11.2021 (publication),N/A,N/A,"The authors constructed a dataset of comment and posts retrieved from Facebook public pages of Afaan Oromoo newspapers, individual politicians, activist and Afaan Oromoo TV. They have used versatile Facebook crawler tools, such as Scrap storm and Face pager.

During dataset preparation the following tasks are performed:

Eliminating all non Afaan Oromoo text and non-textual post and comment; Confiscating all null values; Filtering using keywords that are an indicator of hate and normal; Merging data of each page into one dataset; Eradicating duplication to ensure the uniqueness of each text in a dataset.",Not discussed,,manual,Yes,The two annotators were asked to analyses the content of crawled comments and to classify them as hate or neutral. In case of hate speech were considered only if both annotators agree but for neutral either of them were considered.,Not discussed,,2780,80% (~2224),20% (~556),Annotators are Afaan Oromoo language experts.,Yes,"The following are guidelines for labeled a given post or comment as Hate speech:

If the post/comment contain inferiority or superiority of specific groups.

If the post/comment affects different characteristics of the person and motivates audiences to take action or making a violation.

If post/comment contains stereotype.

If post/comment accusing people based on their target groups.

The following are guideline for labeled a given post or comment as Neutral speech

If the posts/comments is free from all phenomena.

If the posts/comments is from authorized agency which is licensed agency of government.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
192,No,"(Sheng et al., 2021)","“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,USA,"The authors propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. They specifically compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, they propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. The results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) constrained decoding techniques can be used to reduce ad hominems in generated dialogue responses.",Yes,11,06.06.2021,21.09.2022,Yes,Yes,"Ad hominems attack an opponent’s character or identity instead of the points the opponent is making, and can exist in any conversational setting between two or more entities.",Yes,"ad hominems categories: stupidity, ignorance, trolling/lying, bias, condescension, and other",Yes,Other,request,Please contact Emily at ewsheng at gmail.' see  https://github.com/ewsheng/ad-hom-in-dialogue,Yes,ADHOMINTWEETS,1,1,"Twitter, DialoGPT (dialogue model)",Ad hominems,English,07.08.2020 - 29.10.2020,07.08.2020 - 29.10.2020,N/A,"BLM (from #blacklivesmatter posts) and MeToo (from #metoo posts)  WFH (wfh or #workingfromhome) and Vegan (vegan, #veganism, #govegan, or #veganlife)","The authors extract English [post, response] pairs on different topics from Twitter and also use DialoGPT to generate responses for all collected posts.

Relevant topics are divided into polarizing (i.e., controversial) and non-polarizing.

For this study, the authors choose the topic WFH (“work from home”) as a non-polarizing topic and collect Twitter posts that include the hashtag #wfh or #workingfromhome. Polarizing topics can further be divided into those that are directly relevant to marginalized communities and those that are not. For the latter, they choose the topic Vegan and collect posts that include any of the hashtags: #vegan, #veganism, #govegan, or #veganlife.

For polarizing topics that are directly relevant to marginalized groups, they focus on the topics BLM (from #blacklivesmatter posts) and MeToo (from #metoo posts).

In total, they collect 14,585 [post, response] pairs of Tweets posted between Aug. 7 and Oct. 29, 2020.

Because of the natural imbalance of ad hominem responses for different topics, ad hominem responses for topics like WFH are relatively sparse compared to those for topics like BLM. The authors automatically augment the training set to combat this sparsity. First, they accumulate all posts and responses not present in the dev and test sets. Next, they choose a random post to pair with a random labeled response to form a new sample. They generate these new data samples to roughly balance the number of samples across topics and across ad hominems versus non-ad hominems for each topic. These new combinations of [post, response] pairs help de-emphasize spurious correlations between topics and classifier labels",Yes,All usernames and urls are replaced with special placeholders to better anonymize the data,"automated, manual",Yes,"The authors collect human annotations that can then be used for analysis and training a classifier to automatically label ad hominems.

(1) Human Annotation - first round

Annotators on Mechanical Turk are asked to read a post and response and determine whether the response contains any ad hominem(s) towards the person who made the post. Ad hominems are devided into the following categories: stupidity, ignorance, trolling/lying, bias, condescension, and other. The goal for the first round of human annotation is to collect enough data to train an ad hominem classifier.

(2) Additional Annotations - 2~4th round

For the second and third rounds, they use an ad hominem classifier trained on data from all previous rounds to label unseen samples in ADHOMINTWEETS. They then select a balanced amount of automatically-labeled ad hominems and non-ad hominems from each [topic, response source] pair to annotate.

Some topics (e.g., WFH and Vegan) prompt fewer ad hominem responses. The solution is to manually take the responses annotated as ad hominems and pair them with WFH or Vegan posts. A fourth round of annotation was run on these pairs and only keep the ones where the majority of annotators label the response as an ad hominem to the post. The authors combine majority annotations across all rounds of annotations to train the final ad hominem classifier used for analysis.",Yes,"For more effective annotation, the authors use heuristics to choose [post, response] pairs where the response is likely to be an ad hominem. In preliminary analyses, the authors find that responses that contain certain “you”-phrases such as “you are” are more likely to have ad hominems.

To balance targeted and random samples, for each topic (BLM, MeToo, Vegan, WFH) and response source (human, DialoGPT) pair, the authors randomly select 150 [post, response] pairs with you-responses and another 150 pairs without you-responses for annotation. In total, they gather 2,400 [post, response] pairs that are then annotated through Mechanical Turk.",human-annotated ad hominem labels for 2K pairs; automatically augmented training data with another 2K pairs,N/A,N/A,Annotators are from the U.S. or Canada.,Yes,"The guidelines are shown to annotators and are available in Appendix of the paper.  The guidelines provided explanations/examples for ‘ad hominem' and for each sub-categories: stupidity, ignorance, trolling/lying, bias, condescension, other, and non-ad hominem posts.",Yes,Annotators are paid $0.05 to label the ad hominems in a sample.,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Non-human
194,No,"(Chen et al., 2020)",A General Methodology to Quantify Biases in Natural Language Data,CHI '20: CHI Conference on Human Factors in Computing Systems,USA,"In this paper, the authors propose a general methodology to quantify potential data biases. Instead of requiring a specification of sensitive attributes and key concepts, they leverage recent advances of statistical tools to measure the discrepancy of data distribution regarding a reference dataset. For the case of natural language data, they show that lexicon-based features quantify explicit stereotypes, while deep learning-based features further capture implicit stereotypes represented by complex semantics. Their exploration shows that approaches based on stochastic confidence ranking and deep generative models can mitigate data biases at the level of datasets and the level of samples. Their method provides a more flexible way to detect potential biases, and can help domain experts to uncover unexpected biases of different types.",Yes,5,25.04.2020,21.09.2022,Yes,Yes,"Biases of both explicit stereotypes and implicit stereotypes. Explicit stereotypes are overtly expressed judgments, and reflect what people deliberately think about; Implicit stereotypes, on the other hand, reflect more unconscious associations between a social group and certain attributes.",Yes,"Ideology: left, right",No,,,N/A,No,"(1-2) Twitter: Explicit Attitudes (Twitter-Left; Twitter-Right)

(3-4) News Headlines: Implicit Attitudes (News Headlines-Left; News Headlines-Right)",4,4,"Twitter, ABC, BBC, the Guardian, Forbes, Russia Today","Bias, Stereotypes",English,Implicitly before 25.04.2020 (publication),Implicitly before 25.04.2020 (publication),N/A,N/A,"(1-2) Twitter: Explicit Attitudes (Twitter-Left; Twitter-Right)

The authors collected Twitter data of opposing ideology from two seed users, @TheDemocrats and @GOP, who are all high-profile figures actively involved in political ideology debates. For each of them, up to 3,200 of its latest tweets were crawled using Twitter API. For each tweet, up to 100 retweeters were indentified as likely supporters of @TheDemocrats or @GOP. Based on these identified supporters, 2 Twitter datasets were developed.

Twitter-Left: 120,690 tweets posted by supporters of @TheDemocrats that mentioned @TheDemocrats

Twitter-Right: 61,267 tweets posted by supporters of @GOP that mentioned @TheDemocrats

(3-4) News Headlines: Implicit Attitudes (News Headlines-Left; News Headlines-Right)

The authors crawled news headlines from May to July 2018 from more than 500 majority English-language newspapers and TVs. For each media source, follow the approach in other studies, they checked it against the list on “mediabiasfactcheck.com”, which provides the political bias of the most popular medias. Based on the media sources, 2 news headlines datasets were developed.

News Headlines-Left: 110,266 headlines from left-centered media sources, such as ABC News, BBC, and the Guardian

News Headlines-Right: 62,175 headlines from right-centered media sources, such as Forbes and Russia Today (RT)",Not discussed,,manual,Yes,The authors recruit crowd workers on CrowdFlower (https://www.crowdflower.com) to evaluate the ideology attitudes of 200 random samples from each dataset respectively.,Yes,200 random samples from each dataset,"800 (200 random samples from each dataset), to validate that the Twitter and news headlines datasets contain ideology biases",N/A,N/A,Crowd workers on CrowdFlower,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
195,No,"(Roy et al., 2018)",Analyzing Abusive Text Messages to Detect Digital Dating Abuse,2018 IEEE International Conference on Healthcare Informatics (ICHI),USA,"The ultimate goal of this project is to create a mobile phone application that can detect digital dating abuse. As a step toward this goal we are investigating the use of machine learning algorithms and natural language processing techniques to flag text messages as abusive or non-abusive in the context of dating abuse. Due to the lack of a publicly available dataset that could be used to create training and testing sets for the classifiers, the authors first had to create and validate a dataset of abusive text messages. This paper describes the dataset creation process and the results of an evaluation of different classification and feature extraction techniques used to detect abusive texts.",Yes,8,04.06.2018,21.09.2022,Yes,Yes,"Digital dating abuse, where a dating partner leaves repeated threat messages over the phone or social media",Yes,"Physical abuse, Physiological /Emotional abuse, Sexual abuse, Stalking",No,,,N/A,No,"SPAM, Critical, Mixed",3,3,"MTV’s website (athinline.org), resulting text message created by participants, SMS Spam Corpus, Mobile Forensics Text Message Corpus",Digital Dating Abuse,English,Implicitly before 04.06.2018 (publication),Implicitly before 04.06.2018 (publication),N/A,N/A,"(1) MTV’s website, athinline.org is a crowdsourced feedback and advice forum for distressed young adults. Teenagers and adolescents anonymously share stories and when a story is posted, everyone can read it and vote on the severity of the story. There are three levels - over the line (severe), on the line (moderate to mild) and under the line (not very serious). The topics include sexting, online harassment, social network bullying, dating abuse, physical abuse and sexual abuse. The authors analyzed a completely anonymous corpus of 728 of these personal stories and specifically picked 70 stories that were closely aligned to different scenarios of dating abuse. These stories were then used as a starting point to create scenarios to motivate creation of abusive text messages.

(2) Forty-four (44) participants from Clemson University were recruited and each was given a demographic questionnaire, followed by a dating abuse awareness questionnaire to establish a knowledge level baseline. Participants were then allotted five stories each from the abuse scenarios and asked to create abusive text messages by pretending to be the abusive romantic partner. This activity was conducted using google forms and the participants typed out their responses. They were also given instructions to create responses as if they were actually texting someone so that the researchers could capture unique linguistic patterns and short forms that young adults use while texting. A total of 170 responses were collected. Some of these responses contained multiple text messages, which were split into individual units of 182 text messages. Before the participants created their messages, they were verbally instructed (during the informed consent stage) on what DDA is and were shown examples of abusive text messages. They were also given instructions about including general characteristics such as acronyms, spelling and grammatical errors. The researchers then reviewed the messages for their relevance before giving them to the domain experts for validation.

(3) To train a machine-learning classifier to recognize abusive versus non-abusive text messages the authors created a balanced dataset of abusive and non-abusive text messages. Two different corpuses were used to collect non-abusive text messages: the SMS Spam Corpus v.0.1 and the Mobile Forensics Text Message Corpus. To validate consistent performance they created three different datasets for training and testing purposes. The text message combinations were as follows:

• SPAM - One hundred and sixty-one (161) abusive messages with one hundred and forty (140) non-abusive text messages from the SPAM corpus

• Critical - One hundred and sixty-one (161) abusive messages with one hundred and forty (140) non-abusive text messages from the Mobile Forensics Text Message Corpus

• Mixed - One hundred and sixty-one (161) abusive messages with seventy (70) non-abusive text messages from the Mobile Forensics Text Message Corpus and seventy (70) non-abusive text messages from the SPAM corpus.",Not discussed,,manual,Yes,"The dataset of text messages created by participants was given to a group of four annotators. The annotators were each given 182 text messages. They then labeled each text message as either abusive or non-abusive. If a message was judged abusive, they were then asked to rate their confidence with respect to the type of abuse or abuses it represented on a scale of 1 to 5.  The annotators were also provided with standard definitions of each abuse categories. Joint agreement between the annotators was calculated using Light’s Kappa as an inter-rater agreement statistical consistency measure.  The authors chose 157 (of 182) text messages that were labeled as abusive by at least three out of four annotators. In four cases where there was a tie between annotators as to whether or not the text was abusive the researchers broke the tie by labeling the text as abusive. The final dataset consists of 161 abusive text messages.",,,"161 abusive text messages, 140 non-abusive messages collected from corpus",70% (~211),30% (~90),"Four annotators. The criteria for choosing these annotators were that either they were victims of abusive relationships, family members of victims of interpersonal violence, or they were researchers in this domain.",Yes,"The definitions of the categories of dating abuse provided to the annotators were as follows:

• Physical abuse - Occurs when a partner threatens to pinch, hit, shove, slap, punch, or kick a partner.

• Physiological /Emotional abuse - Threatening a partner or harming his or her sense of self-worth. Examples include name-calling, shaming, bullying, embarrassing on purpose, or keeping him/her away from friends and family.

• Sexual abuse - Forcing a partner to engage in a sex act when he or she does not or cannot consent. Can be physical or non-physical, like threatening to spread rumors if the partner refuses to have sex.

• Stalking - Refers to a pattern of harassing or threatening tactics that are unwanted and cause fear in the victim.",Not discussed,,Specific,organization/institution,Targeted,other,Yes,No,,,No,,Yes,Digital Dating Abuse (DDA) is targeted specifically towards dating partners,To a person,No,Human
196,No,"(Cuzcano & Ayma, 2020)",A Comparison of Classification Models to Detect Cyberbullying in the Peruvian Spanish Language on Twitter,International Journal of Advanced Computer Science and Applications (IJACSA),Peru,"In this work, the authors aim to compare four traditional supervised machine learning methods performances in detecting cyberbullying via the identification of four cyberbullying-related categories on Twitter posts written in the Peruvian Spanish language. Specifically, they trained and tested the Naive Bayes, Multinomial Logistic Regression, Support Vector Machines, and Random Forest classifiers upon a manually annotated dataset with the help of human participants. The results indicate that the best performing classifier for the cyberbullying detection task was the Support Vector Machine classifier.",Yes,1,XX.10.2020,21.09.2022,Yes,Yes,"Harassment in social networking sites, better known as cyber bullying, …The most prominent acts of virtual harassment occur through rumors, insults, threats, humiliation, and sexual harassment",Yes,"no harassment, direct harassment, hate speech, and sexual harassment",Yes,Github,,https://github.com/ximenamar/sp_tweets_cyberbullying,Yes,"sp_tweets_cyberbullying (Imbalanced Dataset,  Balanced Dataset)",2,2,Twitter,Cyberbullying,Spanish,08.2019 - 01.2020,08.2019 - 01.2020,Peru (geographical delimitation filter via coordinates),N/A,"The authors constructed and made publicly available a dataset consisting of a collection of 10,096 tweets in Spanish from comments and interactions between Peruvian Twitter users with the help of the Streaming API tool. They collected the dataset during August 2019 and January 2020 from users with an age range between 14 and 60 years old. To ensure class discriminability among tweets, they included common words, jargons relative to Peruvian people, and offensive words during the tweet retrieval process. Furthermore, they have added a geographical delimitation filter after the tweets retrieval process to ensure that the collected tweets belong to Peruvian users only. The filter is part of the Streaming API tool, which is composed of delimiting quadrants with the latitude and longitude coordinates of the different regions of Peru.",Yes,The participants evaluated the tweets via a website specially created to guarantee anonymous sessions not to reveal the participant’ identities.,manual,Yes,"The participants evaluated a set of twenty randomly selected tweets via a website specially created to guarantee anonymous sessions not to reveal the participant’ identities. In one session, a participant assigns a class label to each tweet from the set of twenty tweets according to the four cyberbullying categories. Moreover, the authors made cyberbullying categories definitions available throughout the labeling process, and they also ensured that a tweet gets evaluated by at least three different participants to avoid labeling conflicts.  ",Not discussed,,10096,70% (~7067),30% (~3029),"Annotators were mostly undergraduate students from the last year of Psychology, Communications, and Law schools from different universities in Peru.",Yes,The authors made cyberbullying categories definitions available throughout the labeling process.,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
197,No,"(Ishmam & Sharmin, 2019)",Hateful Speech Detection in Public Facebook Pages for the Bengali Language,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),Bangladesh,"In this paper, the authors develop Machine Learning (ML) algorithms based model, as well as Gated Recurrent Unit (GRU), based deep neural network model for classifying users' comments on Facebook pages. They have collected, annotated 5,126 Bengali comments and classified them into six classes - Hate Speech, Communal Attack, Inciteful, Religious Hatred, Political Comments, and Religious Comments. The produced corpus is the first contribution to the field of hateful speech detection in the Bengali language for social media. Finally, they employ several machine learning algorithms, compare the performance, and attained 52.20% accuracy in Random Forest. The accuracy is improved in the case of GRU based model (70.10% accuracy) about 18%.",Yes,32,16.12.2019,21.09.2022,Yes,Yes,"Hate Speech. ""In general, hate Speech is a form of comment that is intended to attack a person regarding sex, ethnicity, race, disability, sexual orientation or any kind of personal attack.""",Yes,"Hateful: hate, inciteful, communal hatred, religious hate;  Non-hateful: political, religious",Yes,Github,,https://github.com/IshmamAlvi/Hate-Speech-for-Bengali-language,Yes,Hate speech in Bengali language (Stemmed Corpus),1,1,Facebook,Hate Speech,Bengali,22.12.2017 - 04.2018 (API)  05.2018 - 11.2018 (manual collection),22.12.2017 - 04.2018 (API)  05.2018 - 11.2018 (manual collection),N/A,N/A,"The authors select noon chatterjee5, Basherkella, Awami League, Sakib Al Hasan as sample pages for developing the corpus. These pages belong to popular Facebook celebrities, extremist political parties, the official page of the ruling party and the famous cricketer of Bangladesh. They used Facebook graph API (version 2.9) from 22 December 2017 to April 2018 and collected about 3000 comments from these pages along with various metadata such as replies of comments, reactions, etc. However, Facebook recent policies have reduced the access of Graph API for gathering metadata of pages. Therefore, from May 2018 to November 2018, they manually collected about 2000 comments from these pages. They classified the Facebook comments into hateful and non-hateful comments and these are later classified into six categories- hate, inciteful, communal hatred, religious hate, political, religious. The first four categories belong to the hateful speech and later two are non hateful speech.",Not discussed,,manual,Yes,"(available at https://doi.org/10.1145/3309700.3338457)  The authors engaged 3 annotators. Example of an annotation task: One annotate a comment with weight factors for six classes such as 0, 0.2, 0, 0.8, 0, 0 (the summation must be 1). The other two have also annotated like this. The authors summed up the factors from three annotators for each class respectively and then annotate the class with the maximum weighted factor class.",Not discussed,,4753,81% (~3850),10% (~475),"Three university students, who are from different disciplines such as history, language. Most importantly, all of them are active users of Facebook. Among the three, one is from an ethnic minority since a religious minority can identify the communal attack better than others.",Yes,"Hate Speech: In general, hate Speech is a form of comment that is intended to attack a person regarding sex, ethnicity, race, disability, sexual orientation or any kind of personal attack.

Inciteful: Some comments are not directly harmful rather trigger mass people to advocate violence against a certain group of people like religious minority groups or political parties. The comments having a similar meaning are classified as Inciteful.

Religious Hatred: To standardize the religious hate, we follow The Racial and Religious Hatred Act 2006. The comments showing offensive words or conveying hatred against any religious minority groups are categorized into this class.

Religious Comments: Religious comments are non-hateful comments which people use from different holy books of different religions mostly from the Islamic holy books. Besides, good words, well-wishing using the name of God are considered as religious comments.

Communal Attack: Communal attack is a form of the verbal attack showed against different ethnicity or different community based on race or religion.

Political Comments: We need to classify a major portion of our collected comments as political comments. The comments directly or indirectly related to political parties, criticism, discussion, commentary are tagged with such categories. In Table 1, sample comments from each of the categories are enlisted.

The four classes (hate, religious hatred, communal hatred, inciteful) are hateful speech while the rest are non-hateful speech. A particular comment may be considered as religious hatred by one, on the other hand, communal hatred by another. Moreover, a sound understanding of the social, cultural, historical context of a geographical territory should be necessary for annotation of the comments. Therefore, the lexicons are selected with the social context from our socio-cultural perspective. For example, মালু(a slang based on religious belief), is considered a hate speech against the Hindus. Therefore, if we find this lexicon, in anywhere of the sentence, we classify it as a communal hatred. Moreover, if we find name of any political party leader, it resembles political comment or hate speech.",Not discussed,,Specific,"religion, political",Targeted,"religion, race, political, gender, disability, sexuality",Yes,No,,,No,,No,,,No,Human
198,No,"(Gao et al., 2020)",Offensive Language Detection on Video Live Streaming Chat,Proceedings of the 28th International Conference on Computational Linguistics,Japan,"This paper presents a prototype of a chat room that detects offensive expressions in a video live streaming chat in real time. Focusing on Twitch, one of the most popular live streaming platforms, the authors created a dataset for the task of detecting offensive expressions. They collected 2,000 chat posts across four popular game titles with genre diversity (e.g., competitive, violent, peaceful). To make use of the similarity in offensive expressions among different social media platforms, they adopted state-of-the-art models trained on offensive expressions from Twitter for our Twitch data (i.e., transfer learning). They investigated two similarity measurements to predict the transferability, textual similarity, and game-genre similarity. Our results show that the transfer of features from social media to live streaming is effective. However, the two measurements show less correlation in the transferability prediction.",Yes,7,08.12.2020,21.09.2022,Yes,Yes,Offensive language,No,,No,,,N/A,No,"Offensive Language Identification Dataset (OLID) (existing dataset);

live chat datasets: Fortnite (LCFN), Grand Theft Auto V (LCGT), Hearthstone (LCHS), Minecraft (LCMC)",5,4,"Twitch, Metacritic, GamePressure",Offensiveness,English,XX.01.2020 (for live chat datasets),January 2020 (for live chat datasets),N/A,N/A,"For the live chat dataset, the authors collected posts from the chat rooms of the following four games in January 2020; moreover, they collected keywords of the games from the game review sites Metacritic and GamePressure to measure the game-genre similarity. They finally obtained 2,000 posts from the four game titles and annotated them as offensive or not offensive.",Not discussed,,manual,Not discussed,,Not discussed,,"14,100 tweets (OLID)  2,000 posts (live chat datasets)","13,240 tweets (+optional: posts that are not in test set)",one of the live chat dataset,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
200,No,"(Vidgen et al., 2020)",Recalibrating classifiers for interpretable abusive content detection,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"UK, Germany","The authors investigate the use of machine learning classifiers for detecting online abuse in empirical research. They show that uncalibrated classifiers (i.e. where the `raw' scores are used) align poorly with human evaluations. This limits their use for understanding the dynamics, patterns and prevalence of online abuse. They examine two widely used classifiers (created by Perspective and Davidson et al.) on a dataset of tweets directed against candidates in the UK's 2017 general election. A Bayesian approach is presented to recalibrate the raw scores from the classifiers, using probabilistic programming and newly annotated data. The authors argue that interpretability evaluation and recalibration is integral to the application of abusive content classifiers.",Yes,6,20.11.2020,21.09.2022,Yes,Yes,"Toxicity: “a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.”",No,,Yes,Other,Zenodo,https://zenodo.org/record/4075461#.X4Cc9i2ZOu4,Yes,Perspective-annotations; Davidson-annotations,2,2,Twitter,Abusiveness,English,16.05.2017 - 08.06.2017,16.05.2017 - 08.06.2017,N/A,2017 UK general election,"The authors studied tweets directed at candidates on Twitter in the run up to the 2017 UK general election. They collected all mentions and replies to the 2,620 candidates from 16 May to 8 June 2017, creating a dataset of 8.93 million tweets. They apply both classifiers to the dataset, from which we construct two samples to be annotated.",No,,manual,Yes,"The 1,000 tweets in each sample were given to annotators with the definitions of toxicity and hate provided by the original authors. Annotators were not given the classifier scores, tweets were presented in random order, and they were not told the distribution of scores. Each sample was annotated by 5 different independent annotators (i.e. 10 in total). Annotators had all taken part in at least 6 weeks of hateful content annotation as part of other projects, and received 2 additional weeks of training and underwent regular discussion/training sessions.

To decide the true/false labels for recalibration, the authors100 take the majority vote from the five annotators for both Samples 1 and 2.",Yes,"Sample 1 contains 1,000 tweets, sampled uniformly from the probability distribution for the Perspective classifier scores (i.e. with 50 tweets from each 0.05 increment). Sample 2 contains 1,000 tweets, sampled uniformly from the probability distribution for the Davidson et al. classifier scores, also with 50 tweets from each 0.05 increment.","1,000 tweet sample x 2",N/A,1000 (recalibration for each classifier),"Annotators were all fluent in English (8 out of 10 were native speakers). They were equally split between male and female genders, all aged between 18 and 30, university educated and from a range of European countries.",Yes,"The 1,000 tweets in each sample were given to annotators with the definitions of toxicity and hate provided by the original authors (Davidson, Perspective).

The authors also mentioned: …reflect the relatively short guidelines provided by the creators of the Davidson classifier and the fact that for the Perspective classifier we had only the definition of toxicity: “a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.”",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
201,No,"(Ritu et al., 2021)",Bangla Abusive Language Detection using Machine Learning on Radio Message Gateway,2021 6th International Conference on Communication and Electronics Systems (ICCES),Bangladesh,"This paper presents a mechanism for detecting Bangla abusive language from a realtime radio message gateway. This paper has created a dataset with more than 45000 Bangla sentences, which are labeled as abusive and non-abusive. Sample online radio message gateway has been been introduced and machine learning algorithms such as multinomial naive bias (MNB), logistic regression (LR), and random forest (RF) classifiers are utilized to predict the abusive languages. One of the significant prospects of this work would be applied during live radio programs where listeners try to communicate by sending live messages. The proposed mechanism can check and map the live messages with the dataset and segregate the positive comments or messages only, by filtering the abusive comments. Among the applied classifiers, it has been found that the random forest classifier has performed better than the other two classifiers by leveraging approximately 76% accuracy.",Yes,0,08.07.2021,21.09.2022,Yes,Yes,Abusive language,No,,No,,,N/A,No,Bangla abusive language dataset,1,1,"Facebook, news portals, online shopping sites",Abusiveness,Bengali,"for 3-4 months, implicitly before 08.07.2021 (publication)",implicitly before 08.07.2021 (publication),N/A,N/A,"The authors collected these data from different celebrities’ social media accounts and radio station programs on social media platforms. For the analysis, they gathered different remarks on posts from Facebook pages, Prothom Alo, Bangladesh Pratidin, Kaler Kantho, jagonews24, somoynews.tv, Jamuna Television, independent24.tv, Hero Alom, Just Ripon video, Sefat Ullah Sefuda, Salman Mohammad Muqtadir, Rafiath Rashid Mithila, Naila Nayem, Jaya Ahsan, Mahiya Mahi, Taslima Nasrin, Daraz Online Shopping. Starting from film actor, actress to social media celebrities, online news portals and online shopping sites were covered for collecting data.",Not discussed,,manual,Not discussed,,Not discussed,,45000+ Bangla sentences,10-fold cross validation,10-fold cross validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
203,No,"(Tang et al., 2020)","Categorizing Offensive Language in Social Networks: A Chinese Corpus, Systems and an Explanation Tool",Chinese Computational Linguistics,China,"In this work, the authors present a densely annotated data-set COLA (Categorizing Offensive LAnguage), consists of fine-grained insulting language, antisocial language and illegal language. They study different strategies for automatically identifying offensive language on COLA data. Further, they design a capsule system with hierarchical attention to aggregate and fully utilize information, which obtains a state-of-the-art result. Results from experiments prove that the hierarchical attention capsule network (HACN) performs significantly better than existing methods in offensive classification with the precision of 94.37% and recall of 95.28%. They also explain what our model has learned with an explanation tool called Integrated Gradients. Meanwhile, the system’s processing speed can handle each sentence in 10 ms, suggesting the potential for efficient deployment in real situations.",Yes,4,12.11.2020,21.09.2022,Yes,Yes,"“Offensive” is pretty much something people identify as against morals, very inappropriate, or disrespectful. However, “offensive” is a broad general term and does not define the precise extent or the limits of its application. Thus, we classify the term “offensive” into three categories: “insulting,” “antisocial” and “illegal” through stepwise refinement. “Insulting” is something rude, insensitive and/or offensive, directed at another person or group of people. This emphasizes that the content is a direct attack against specific others. “Antisocial” is harmful to organized society, or the language describes a behavior deviating from the social norm for long. “Illegal” language means it violates the language policy. Where the language policy refers to the government through legislation or policies to formally decide how languages are used.",Yes,"offensive: insulting, antisocial, and illegal",No,,,N/A,No,COLA (Categorizing Offensive LAnguage),1,1,"YouTube, Weibo",Offensiveness,Chinese,Implicitly before 12.11.2020 (publication),Implicitly before 12.11.2020 (publication),N/A,N/A,"With more than 1000 comments and more than 10000 views as the thresholds, the authors selected 20 popular Chinese videos from YouTube. Furthermore, from the comments below the video are crawled through Google YouTube V3 API, which is offered by Google for researchers to collect comments.

Social media companies all have some methods to prevent crawlers. These methods can be divided into three categories: analyzing the headers of web page requests, monitoring the behavior of users visiting the website, and adjusting the directory and data loading methods. Corresponding to that, the authors adopt three approaches to crawl the data. F

(1) directly add HEADERS and REFERER to the code to bypass the check.

(2) use the IP proxy.

(3) use a browser to analyze the requests. If we can obtain the AJAX request, then we can use the above two methods to resolve and obtain the corresponding data. However, if we cannot get AJAX requests, we can call the selenium + phantomjs framework and call its browser kernel to simulate human operations and JS scripts that trigger the page.",Yes,URLs and mentioned users are removed,manual,Yes,"More than 18,707 sentences are selected.

The authors remove invalid tokens in the text, like HTML tags and emoticons, and treat the text as the preliminary data for hand-operated annotation. The vocabulary was divided into three categories: insulting language, antisocial language, illegal language.

Three annotators categorised these texts in four classes: neutral, insulting, antisocial, and illegal. Three people are asked to annotate the same text to ensure accuracy. Inter-annotator agreement and intra-annotator agreement have been considered for the coherence of annotations While annotating.",Not discussed,,18707,11600,3759,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
204,No,"(Park et al., 2021)",KOAS: Korean Text Offensiveness Analysis System,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,South Korea,"In this paper, the authors present KOAS, a system that fully exploits both contextual and linguistic features and efficiently estimates the offensiveness score of Korean text. They carefully designed KOAS with a multi-task learning framework and constructed a Korean dataset for offensive analysis from various domains.",Yes,0,07.11.2021,21.09.2022,Yes,Yes,"The score of offensiveness represents the degree of negative feelings (e.g. anger, annoyance and fear) that the text may arouse in readers.",Yes,"binary classes whether they contain abusive language and three classes for sentiment polarity: positive, neutral and negative",Yes,Other,Google Drive,https://drive.google.com/file/d/1YZ_tuJzs5CBaO0pNY7Cb1Xa0rRR3GQX-/view?usp=sharing,No,"KOAS: Data_org, Data_aug",2,2,"YouTube, Naver Movie review, dcinside.",Offensiveness,Korean,Implicitly before 07.11.2021 (publication),Implicitly before 07.11.2021 (publication),N/A,N/A,"The authors gathered the training data from three different sources, which covers various domains – YouTube, Naver Movie review and dcinside. They scraped comments from a popular Korean online community, expecting the train dataset to be close to a raw expression for practicality. Then, they removed duplicate comments and filtered out non-Korean sentences. The collected dataset consists of 46,853 Korean sentences.",Not discussed,,manual,Not discussed,,Not discussed,,"46,853","28,111","9,370",N/A,Yes,"The dataset was labeled by three annotators on predefined criteria for abusive language (Koo and Seo, 2012) and sentiment following the instruction (Nakov et al., 2016).",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
205,No,"(Guo et al., 2021)",BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),"France, China","The authors introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. The model is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and named entity recognition. The dataset used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in French. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.",Yes,2,11.11.2021,21.09.2022,Yes,Yes,"In our labeling procedure, we consider a tweet as offensive in cases where personal attacks are detected.",No,,No,,,N/A,No,"Pre-training Dataset;

Downstream Task Datasets: Offensive Language dataset (new), dataset from CAp 2017",3,2,Twitter,Offensiveness,French,"Pre-training Dataset (January 2016 - December 2019, September 2020 - April 2021, 2014 - 2018);  Downstream Task Datasets: Offensive Language dataset (new), dataset from CAp 2017 (The time span is not mentioned, implicitly before 11.11.2021)","Pre-training Dataset (January 2016 - December 2019, September 2020 - April 2021, 2014 - 2018);  Downstream Task Datasets: Offensive Language dataset (new), dataset from CAp 2017 (The time span is not mentioned, implicitly before 11.11.2021)",N/A,COVID-19 pandemic,"(1) Pre-training Dataset: aggregation of three corpora.

• The authors start by downloading tweets from the general Twitter Stream grabbed by the Archive Team, containing of tweets streamed from January 2016 to December 2019. Selecting only the French tweets with Twitter’s built-in feature, the authors obtain a corpus of 34M unique tweets.

• They also build a COVID-19 related corpus of French tweets relevant to the COVID-19 pandemic posted between September 2020 and April 2021. In this case, the filters are focused on tweets that include the hashtags “covid19” and “coronavirus”. Through Twitter’s public streaming API, they extracted tweets in French marked with either or both of the two above hashtags. This corpora consists of 19M unique tweets.

• Finally, they make use of a previous Twitter dataset constructed for socioeconomic analysis(Abitbol et al., 2018).. This corpus includes a collection of tweets in French between the years 2014 and 2018. They extract 173M unique tweets form this corpus.

(2) Offensive Language dataset

The authors created a human annotated dataset for general offensiveness detection in Tweets collected during the COVID-19 pandemic. The dataset contains 5786 French tweets among which 1301 have been labeled as offensive.

(3) Named Entity Recognition dataset: data from the CAp 2017 challenge (Lopez et al., 2017)",Not discussed,,manual,Not discussed,,Not discussed,,5786,70% (~4050),15% (~868),N/A,Yes,"In our labeling procedure, we consider a tweet as offensive in cases where personal attacks are detected. For example, ""The chinese virus is tiring"" would not be considered offensive, while ""I hate the chinese for bringing us the chinese virus"" would be. This is a binary sequence classification task.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
207,No,"(Dahiya et al., 2021)",Would Your Tweet Invoke Hate on the Fly? Forecasting Hate Intensity of Reply Threads on Twitter,Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,"India, France, UK","In this paper, the authors define a novel problem – given a source tweet and a few of its initial replies, the task is to forecast the hate intensity of upcoming replies. To this end, they curate a novel dataset constituting ∼ 4.5k contemporary tweets and their entire reply threads. The preliminary analysis confirms that the evolution patterns along time of hate intensity among reply threads have highly diverse patterns, and there is no significant correlation between the hate intensity of the source tweets and that of their reply threads. They employ seven state-of-the-art dynamic models (either statistical signal processing or deep learning based) and show that they fail badly to forecast the hate intensity. They then propose DESSERT, a novel deep state-space model that leverages the function approximation capability of deep neural networks with the capacity to quantify the uncertainty of statistical signal processing models. Exhaustive experiments and ablation study show that DESSERT outperforms all the baselines substantially. Further, its deployment in an advanced AI platform designed to monitor real-world problematic hateful content has improved the aggregated insights extracted for countering the spread of online harms.",Yes,8,14.08.2021,21.09.2022,Yes,Yes,Hate intensity,Yes,"hate, fake, and controversial",Yes,Github,,https://github.com/LCS2-IIITD/DESSERt_KDD21,Yes,DESSERT_KDD21,1,1,Twitter,Hate Speech,English,October 2020 - January 2021,Implicitly during/before January 2021,N/A,"2020 US presidential elections (e.g., #MagaTerrorists, #StopTheSteal), COVID-19 (e.g., #ChinaVirus, #TrumpVirus), the Brexit referendum in the UK (e.g., #brexitshambles, #stopbrexit), and other political issues in the US, the UK and India (e.g., #BorisResign, #ToryCovidCatastrophe, #CAA, #NRC)","Since existing Twitter API does not provide the entire reply thread of a tweet, the authors first identified source tweets and systematically scraped timelines of users who replied to these tweets. To identify the source tweets, they searched with hashtags related to 2020 US presidential elections (e.g., #MagaTerrorists, #StopTheSteal), COVID-19 (e.g., #ChinaVirus, #TrumpVirus), the Brexit referendum in the UK (e.g., #brexitshambles, #stopbrexit), and other political issues in the US, the UK and India (e.g., #BorisResign, #ToryCovidCatastrophe, #CAA, #NRC). The complete dataset contains 4, 533 reply threads.",Not discussed,,manual,Not discussed,,Yes,1830 randomly selected source tweets were annotated,"4, 533 reply threads;  1830 source tweets (annotated for different types)",First 70% replies of each reply thread,Remaining 30% replies of each reply thread,Logically’s content moderators,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
208,No,"(Salawu et al., 2022)",A Mobile-Based System for Preventing Online Abuse and Cyberbullying,International Journal of Bullying Prevention,UK,"In this paper, the authors present BullStop, a mobile application that can use different machine learning models to detect cyberbullying. A new cyberbullying dataset containing 62,587 tweets annotated using a taxonomy of different cyberbullying types was created to facilitate the classifier’s training. BullStop was developed using a participatory and user-centred design approach involving young people, parents, educators, law enforcement and mental health professionals. Additionally, the application incorporates online training for the ML models using ground truth supplied by the user as additional training data, and in this way, it can create a personalised classifier for each user. Furthermore, on detecting online abuse, the application automatically initiates punitive actions such as deleting offensive messages and blocking cyberbullies on behalf of the user. BullStop is freely available on the Google Play Store and has been downloaded by hundreds of users.",Yes,0,01.03.2022,21.09.2022,Yes,Yes,"Cyberbullying can be defined as wilful and repeated harm inflicted through the use of computers, cell phones, and other electronic devices",Yes,"Cyberbullying, Insult, Profanity, Sarcasm, Threat, Exclusion, Spam and Porn",No,,,N/A,No,Annotated cyberbullying and offensive language dataset (denoted as C in experiment setup); Davidson dataset (D); Kaggle Toxic Comments dataset (K),3,1,Twitter,"Abusiveness, Cyberbullying",English,Implicitly before 01.03.2022 (publication),Implicitly before 01.03.2022 (publication),N/A,N/A,"The dataset contained 62,587 tweets extracted from Twitter using a variety of query terms, including the 15 most frequently used profane words on Twitter (Wang et al., 2014) as well as hashtags including #sarcasm, #stayinyourlane #maga, #killyou, #rape, #cops, #blm, #alllivesmatter, #idontlikeyou and #notinvited to capture different forms of online abuse, hate speech and bullying.",Not discussed,,manual,Yes,"Seventeen annotators were used to label the dataset, and each tweet was labelled by three different annotators. The inter-rater agreement as measured by Krippendorff’s alpha is 0.67, which can be interpreted as ‘moderate’. The tweets were annotated for the labels: Cyberbullying, Insult, Profanity, Sarcasm, Threat, Exclusion, Spam and Porn, which are the classes predicted by the trained models used in the ADM.",Not discussed,,"62,587  tweets",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
210,No,"(Ousidhoum et al., 2019)",Multilingual and Multi-Aspect Hate Speech Analysis,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),Hong Kong SAR,"In this paper, the authors presented a multilingual (English, French, and Arabic) multi-aspect (Directness, Hostility, Target, Group, Annotator) hate speech tweets dataset. They analyzed in details the difficulties related to the collection and annotation of this dataset. They performed multilingual and multitask learning on the corpora and showed that deep learning models perform better than traditional BOW-based models in most of the multilabel classification tasks. Multilingual multitask learning also helped tasks where each label had less annotated data associated with it. They also discuss how to leverage the annotations in order to improve hate speech detection and classification in general.",Yes,139,03.11.2019,21.09.2022,Yes,Yes,"Hate speech. ""Hate speech may not represent the general opinion, yet it promotes the dehumanization of people who are typically from minority groups (Soral et al., 2017; Martin et al.,2012) and can incite hate crime (Ross et al., 2017).""",Yes,"Five attributes: Directness, Hostility, Target, Group, Annotator  Directness: direct, indirect  Hostility: abusive, hateful, offensive, disrespectful, fearful, normal  Target: origin, gender, sex_orient, religion, disability, other  Group: individual, other, women, spec_needs, african  Annotator: disgust, shock, anger, sadness, fear, confusion",Yes,Github,,https://github.com/HKUST-KnowComp/MLMA_hate_speech,Yes,MLMA (Multilingual and Multi-Aspect) hate speech,1,1,Twitter,Hate Speech,"English, French, Arabic",Implicitly before 03.11.2019 (publication),Implicitly before 03.11.2019 (publication),N/A,N/A,"Considering the cultural differences and commonly debated topics in the main geographic regions where English, French, and Arabic are spoken, searching for equivalent terms in the three languages led to different results at first. Therefore, after looking for 1,000 tweets per 15 more or less equivalent phrases in the three languages, the authors revised the search words three times by questioning the results, adding phrases, and taking off unlikely ones in each of the languages.

In fact, they started data collection by searching for common slurs and demeaning expressions such as “go back to where you come from”. Then, they observed that discussions about controversial topics, such as feminism in general, illegal immigrants in English, Islamo-gauchisme (“Islamic leftism”) in French, or Iran in Arabic were more likely to provoke disputes, comments filled with toxicity and thus, notable insult patterns that we looked for in subsequent search rounds.",Not discussed,,manual,Yes,"(1) Pilot: The authors initially put samples of 100 tweets in each of the three languages on Amazon Mechanical Turk. They showed the annotators the tweet along with lists of labels describing (a) whether it is direct or indirect hate speech; (b) if the tweet is dangerous, offensive, hateful, disrespectful, confident or supported by some URL, fearful out of ignorance, or other; (c) the target attribute based on which it discriminates against people, specifically, race, ethnicity, nationality, gender, gender identity, sexual orientation, religious affiliation, disability, and other (“other” could refer to political ideologies or social classes.); (d) the name of its target group, and (e) whether the annotators feel anger, sadness, fear or nothing about the tweets.
Each tweet has been labeled by three annotators. Additional text fields are provided to annotators to fill in with labels or adjectives that would (1) better describe the tweet, (2) describe how they feel about it more accurately, and (3) name the group of people the tweet shows bias against.
The researchers kept the most commonly used labels from the initial label set, took off some of the initial class names and added frequently introduced labels.
(2) Final dataset: Each of the annotated aspects represents a classification task of its own, that could either be evaluated independently, or tested on how it impacts other tasks. The different labels are designed to facilitate the study of the correlations between the explicitness of the tweet, the type of hostility it conveys, its target attribute, the group it dehumanizes, how different people react to it, and the performance of multitask learning on the five tasks.
The authors assigned each tweet to five annotators, then applied majority voting to each of the labeling tasks. Given the numbers of annotators and labels in each annotation sub-task, multilabel annotations were allowed in the most subjective classification tasks, namely the hostility type and the annotator’s sentiment labels, in order to keep the right human-like approximations. If there are two annotators agreeing on two labels respectively, both labels are added to the annotation.
***
To prevent scams, the authors also prepared three annotation guideline forms and three aligned labelsets written in English, French, and Modern Standard Arabic with respect to the language of the tweets to be annotated. The researchers requested native speakers to annotate the data and chose annotators with good reputation scores (more than 0.90). They have divided the corpora into smaller batches on Amazon Mechanical Turk in order to facilitate the analysis of the annotations of the workers and, fairly identify any incoherence patterns possibly caused by the use of an automatic translation system on the tweets, or the repetition of the same annotation schema. If the authors reject the work of a scam, they notify them, then reassign the tasks to other annotators.",Not discussed,,"13,014 (5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets)",80% (~10411),10% (~1301),Native-speaking annotators with good reputation scores (more than 0.90) recruited via Amazon Mechanical Turk,Yes,"Three annotation guideline forms and three aligned labelsets written in English, French, and Modern Standard Arabic  were provided with respect to the language of the tweets to be annotated.

The guidelines are available at https://github.com/HKUST-KnowComp/MLMA_hate_speech/blob/master/guidelines.tar",Not discussed,,Specific,"race, gender, sexuality, religion, disability, other",Non-targeted,N/A,Yes,Yes,"Origin, Gender, Sexual orientation, Religion, Disability, Other","race, gender, sexuality, religion, disability, other",Yes,"women, special needs, African, other",No,,,No,Human
211,No,"(Pérez-Arredondo & Graells-Garrido, 2021)",Twitter and abortion: Online hate against pro-choice female politicians in Chile,Journal of Language Aggression and Conflict,"Chile, Spain","This paper explores the misogynistic abuse against female Chilean politicians who openly supported a pro-choice bill that allowed the access to abortion in limited circumstances. We analysed the verbal abuse targeted at these politicians during the legislation of the abortion bill (2015–2017) and the linguistic and discursive patterns of online abuse. To that end, we collected tweets from this legislation period and created a subset with specific milestones of the parliamentary debate. Further, we undertook a corpus-assisted analysis of the data, focusing on collocations and keywords, which were then analysed in the light of van Leeuwen’s (2008) framework on the representation of social actors and legitimation strategies. Results evidence that violence against women in power can take forms other than the explicit sexual, physical, and psychological threats that are commonly identified. Violence targets these women as it is claimed that they are unsuitable to legislate for allegedly having tolerated and protected crime. Therefore, the corrective function of abuse takes the form of legal actions against their crimes.",Yes,7,04.03.2021,21.09.2022,Yes,Yes,"Violence against women aims to exercise social control over their actions, bodies, and overall existence. …Disciplining and/or punishing women who challenge the male-dominated public sphere also emerges in the literature on online harassment.",Yes,"Topic/Discursive strategy: Psychological traits, Physical traits, Ideology (politics and religion), Morality and values, Crime",No,,,N/A,No,"Legislation and Implementation of Restrictive Abortion (LIRA) Corpus;

MILESTONES corpus",2,2,Twitter,Misogyny,Spanish,31.01.2015 - 31.10.2017,31.01.2015 - 31.10.2017,Chile (according to the self-reported location on the profile),"Abortion Bill 21,030","(1) Legislation and Implementation of Restrictive Abortion (LIRA) Corpus

The data was collected through the Twitter Streaming API using a crawler tailored for Chilean Spanish content during the legislation period of the Abortion Bill 21,030, from the day the bill was introduced in Parliament (31 January 2015) through the end of the first three months the law was implemented (31 October 2017). The crawler downloaded tweets that complied with query keywords related to abortion and other political issues such as migration and (political and economic) centralization. For this analysis, the authors restricted the search to tweets that included the word aborto ‘abortion’ and/or abortista ‘abortionist’ and were published by users who self-reported Chile as location on their profiles. Each tweet includes the screen name of the user who tweeted the message, date, and the text (including any mentions the user incorporated in the original message). t Retweets (i.e. reposting someone’s Tweet so that your followers can see it, too), links, and multimodal texts were excluded from the analysis. In total, the corpus includes 220,815 tweets and mentions, which accounts for 4,801,809 word tokens.

(2) MILESTONES corpus

MILESTONES was created to evaluate and analyse users’ reactions throughout the different stages of the legislation and implementation of the law. These stages are also correlated to rising peaks in tweet activity throughout the period, meaning that Twitter users were particularly active those days. Thus, the authors considered both the day and the day after these milestones took place, resulting in a 48-hour coverage from its occurrence. This corpus totals 38,236 tweets and mentions, which accounts for 799,999-word tokens.

Both corpora were explored in relation to online hate and misogyny against pro-choice female politicians. To this aim, the authors identified thirteen female politicians who actively participated throughout the legislation of the project and used their usernames to search for abuse against them.",Yes,Examples provided in this article have been anonymized to protect the users’ identities and comply with ethical research uses of Twitter data,N/A,Not discussed,,,,"LIRA Corpus: 20,815 tweets and mentions, which accounts for 4,801,809 word tokens.  MILESTONES corpus: 8,236 tweets and mentions, which accounts for 799,999-word tokens.",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,"gender, political",Targeted,gender,No,Yes,Gender,gender,Yes,Women,Yes,Thirteen female politicians,To a person,No,Human
213,No,"(Cercas Curry & Rieser, 2018)",#MeToo Alexa: How Conversational Systems Respond to Sexual Harassment,Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing,UK,"Conversational AI systems, such as Amazon's Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, the authors establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #MeTooAlexa corpus. The results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on “clean” data, which suggests that inappropriate system behaviour is not caused by data bias.",Yes,71,05.06.2018,21.09.2022,Yes,Yes,"According to the UK’s Equality Act (U.K. Government, 2010), sexual harassment is unwanted behaviour of a sexual nature that is meant to violate the victims’ dignity; make them feel intimidated, degraded or humiliated; or creates a hostile working environment. Similarly, the Linguistic Society of America defines sexual harassment as “unwelcome sexual advances, requests for sexual favours, and other verbal or physical conduct of a sexual nature”.",Yes,"Prompts: Gender and Sexuality, Sexualised Comments, Sexualised Insults, Sexual Requests and Demands;  Responses:  Nonsensical Responses: Non-grammatical, Non-coherent, No-answer, Search results, Don’t know  Negative Responses: Humorous refusal, Polite refusal, Deflection, Chastising, Retaliation, Avoids answering directly  Positive Responses: Play-along, Joke, Flirtation",No,,,N/A,No,#MeToo corpus; clean Reddit data (old),2,1,"conversational systems, Reddit",Sexual Harassment,English,Implicitly before 05.06.2018 (publication),Implicitly before 05.06.2018 (publication),N/A,N/A,"(1) Prompt Design

The authors collected a total of 360K conversations from an open-domain conversational system built by themselves. The use these real-life examples of abuse to source stimuli for data collection. They randomly sampled a number of sexually-explicit customer utterances from our corpus and summarised them to a total of 35 utterances, which they categorised based on the Linguistic Society’s definition of sexual harassment.

A) Gender and Sexuality, e.g. “What is your gender?”

B) Sexualised Comments, e.g. “I love watching porn.”

C) Sexualised Insults, e.g. “You stupid bitch.”

D) Sexual Requests and Demands, e.g. “Will you have sex with me.”

They repeated the insults multiple times to see if system responses varied and if defensiveness increased with continued abuse. In this case, all responses were included in the study.

(2) #MeToo corpus

Using the 35 prompts, the authors collected a total of 689 responses which were then manually annotated into categories.",Not discussed,,manual,Yes,"689 responses were manually annotated according to the following categories.

Nonsensical Responses: Non-grammatical, Non-coherent, No-answer, Search results, Don’t know

Negative Responses: Humorous refusal, Polite refusal, Deflection, Chastising, Retaliation, Avoids answering directly

Positive Responses: Play-along, Joke, Flirtation

The authors measured the inter-annotator agreement between the two expert annotators to be substantial (κ = 0.66)",Not discussed,,689 responses,N/A,N/A,Two expert annotators who were both Western women of roughly similar age groups.,Yes,Definitions and examples for each category were provided.,Not discussed,,Specific,sexuality,Targeted,sexuality,Yes,Yes,"Gender, Sexuality","gender, sexuality",No,,No,,,No,Non-human
214,No,"(Alhashmi & Darem, 2021)",Consensus-Based Ensemble Model for Arabic Cyberbullying Detection,Computer Systems Science and Engineering,Saudi Arabia,"This study focuses on improving the efficacy of the existing cyberbullying detection models for Arabic content by designing and developing a Consensus-based Ensemble Cyberbullying Detection Model. A diverse set of heterogeneous classifiers from the traditional machine and deep learning technique have been trained using Arabic cyberbullying labeled dataset collected from five different platforms. The outputs of the selected classifiers are combined using consensus-based decision-making in which the F1-Score of each classifier was used to rank the classifiers. Then, the Sigmoid function, which can reproduce human-like decision making, is used to infer the final decision. The outcomes show the efficacy of the proposed model comparing to the other studied classifiers. The overall improvement gained by the proposed model reaches 1.3% comparing with the best trained classifier. Besides its effectiveness for Arabic language content, the proposed model can be generalized to improve cyberbullying detection in other languages.",Yes,0,08.10.2021,21.09.2022,Yes,Yes,"Cyberbullying can be described as the use of digital means, such as smart electronic devices that are connected to the internet, to post content intending to harm or embarrass others",No,,No,,,N/A,No,"Twitter, WhatsApp, Vine, Instagram, Packet",5,5,"Twitter, WhatsApp, Vine, Instagram, Packet",Cyberbullying,Arabic,Implicitly before 08.10.2021 (publication),Implicitly before 08.10.2021 (publication),N/A,N/A,"Due to the lack of availability of Arabic Dataset for cyberbullying detection, the dataset from other languages has been translated to Arabic language in two stages. In the first stage, the google translation of API was used to generate the first draft of the translation. In the second stage, the dataset is corrected manually using human-based translation. The polarity of the words in cyberbullying is modified according to the spoken Arabic language. Similarly, the implicit sentiment has been corrected as well.

Five datasets were combined and used in this research work to train and evaluate the proposed model. Each dataset was extracted from different platforms, namely Twitter, WhatsApp, Vine, Instagram, and Packet. In total, 23462 samples have been considered in the experiments.

Due to the class imbalance between the normal and cyberbullying samples, the cyberbullying samples have been oversampled using Synthetic Minority Oversampling Technique (SMOTE). Thus, the total number of samples was 34,244 samples.",Not discussed,,N/A,Not discussed,,Not discussed,,"34,244 samples",70% (~23971),30% (~10273),N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
216,No,"(Onabola et al., 2021)",hBERT + BiasCorp - Fighting Racism on the Web,"Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion","Canada, USA","In this work, the authors present how they tackle racism with Natural Language Processing. They are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually annotated) is ready for publication. They are currently in the final phase of manually labeling the remaining dataset using Amazon Mechanical Turk. In this work, we present hBERT, where they modify certain layers of the pretrained BERT model with the new Hopfield Layer. hBert generalizes well across different distributions with the added advantage of a reduced model complexity. They are also releasing a JavaScript library 3 and a Chrome Extension Application, to help developers make use of our trained model in web applications (say chat application) and for users to identify and report racially biased contents on the web respectively.",Yes,3,19.04.2021,21.09.2022,Yes,Yes,Racially biased content in this context refers to the attitudes or stereotypes expressed against marginalized races. This is often as a result of implicit bias resulting into hate speech.,No,,Yes,Website,,https://doi.org/10.7910/DVN/KPBRLC,Yes,BiasCorp,1,1,online news media,Racism,English,Implicitly before 19.04.2021 (publication),Implicitly before 19.04.2021 (publication),N/A,N/A,"The datasets used for training were obtained from discussion channels of online news media.

First, sentences containing neural racial words from a curated list were selected. Second, the sentiment score of each comment was calculated according to two lookup tables: a combined and augmented (Jockers, 2015) and Rinker’s augmented Hu and Liu (Tyler Rinker, 2016) (Hu and Liu, 2004) positive/negative word list as sentiment lookup values, and a racial-related English lookup table from Hatebase. To guarantee these two tables influence the sentiment score consistently, the lookup values of the Hatebase table were adjusted by percentage. Then the authors extracted the data with bottom 20 percent of the sentiment score, and matched them up with other randomly selected comments appearing under the same articles or videos as random control. Finally, equal numbers of random controls are added into the data set, to ensure that approximately half of the data is racially discriminatory.

The data were obtained by programmed web crawler based on Scrapy framework with all crawled data stored in PostgreSQL database. The web crawler parsed keys for each article after completing a list with URLs of all articles waiting to be further crawled and then matched the keys with their corresponding API to retrieved stored comments for each article.",Not discussed,,"automated, manual",Yes,"The data with bottom 20 percent of the sentiment score are considered as racially discriminatory. Other randomly selected comments appearing under the same articles or videos are considered as random control.

The first batch of the dataset were manually annotated using Amazon Mechanical Turk.",Not discussed,,"139,090 rows (45,000 manually annotated in the first batch)",N/A,The test set contains only comments from YouTube,N/A,Not discussed,,Not discussed,,Specific,race,Targeted,race,Yes,Yes,Race,Race,No,,No,,,No,Human
217,No,"(von Boguszewski et al., 2021)",How Hateful are Movies? A Study and Prediction on Movie Subtitles,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),Germany,"In this research, the authors investigate techniques to detect hate speech in movies. They introduce a new dataset collected from the subtitles of six movies, where each utterance is annotated either as hate, offensive or normal. They apply transfer learning techniques of domain adaptation and fine-tuning on existing social media datasets, namely from Twitter and Fox News. They evaluate different representations, i.e., Bag of Words (BoW), Bidirectional Long short-term memory (Bi-LSTM), and Bidirectional Encoder Representations from Transformers (BERT) on 11k movie subtitles. The BERT model obtained the best macro-averaged F1-score of 77%. Hence, they show that transfer learning from the social media domain is efficacious in classifying hate and offensive speech in movies through subtitles.",Yes,3,06.09.2021,21.09.2022,Yes,Yes,"Hate speech is defined as verbal communication that denigrates a person or a community on some characteristics such as race, color, ethnicity, gender, sexual orientation, nationality, or religion.",Yes,"hate, offensive, normal",Yes,Github,,https://github.com/uhh-lt/hatespeech,Yes,"Novel Movie Dataset, Twitter (old),  Fox news corpus (old)",3,1,"opensubtitles.org, IMDB, Fox News website, Twitter",Hate Speech,English,Implicitly before 06.09.2021 (publication),Implicitly before 06.09.2021 (publication)  Fox news: 2016;  The six movies were released during 1994 - 2018,N/A,N/A,"The novel movie dataset consists of six movies, chosen based on keyword tags provided by the IMDB website. The tags hate-speech and racism were chosen because we assumed that they were likely to contain a lot of hate and offensive speech. The tag friendship was  chosen to get contrary movies containing a lot of normal subtitles, with less hate speech content. In addition, the authors excluded movie genres like documentations, fantasy, or musicals to keep the movies comparable to each other. Namely they have chosen the movies BlacKkKlansman (2018) which was tagged as hate-speech, Django Unchained (2012), American History X (1998) and Pulp Fiction (1994) which were tagged as racism whereas South Park (1999) as well as The Wolf of Wall Street (2013) were tagged as friendship.

The downloaded subtitle files are provided by the website www.opensubtitles.org and are free to use for scientific purposes. The files are available in the SRT-format that have a time duration along with a subtitle, which while watching appears on the screen in a given time frame. The authors performed the following operations to create the movie dataset: (1) Converted the SRT-format to CSV-format by separating start time, end time, and the subtitle text, (2) Fragmented subtitles which were originally single appearances on the screen and spanned across multiple screen frames were combined, by identifying sentence-ending punctuation marks, (3) Combined single word subtitles with the previous subtitle because single word subtitles tend to be expressions to what has been said before.",Not discussed,,manual,Yes,"For the annotation of movie subtitles, the authors have used Amazon Mechanical Turk (MTurk) crowdsourcing. Before the main annotation task, they have conducted an annotation pilot study, where 40 subtitles texts were randomly chosen from the movie subtitle dataset. Each of them has included 10 hate speech, 10 offensive, and 20 normal subtitles that are manually annotated by experts. In total, 100 MTurk workers were assigned for the annotation task. They have used the built-in MTurk qualification requirement (HIT approval rate higher than 95% and number of HITs approved larger than 5000) to recruit workers during the Pilot task. Each worker was assessed for accuracy and the 13 workers who have completed the task with the highest annotation accuracy were chosen for the main study task. The rest of the workers were compensated for the task they have completed in the pilot study and blocked from participating in the main annotation task.

For the main task, the 13 chosen MTurk workers were first assigned to one movie subtitle annotation to further look at the annotator agreement. Two annotators were replaced during the main annotation task with the next-best workers from the identified workers in the pilot study. This process was repeated after each movie annotation for the remaining five movies. One batch consists of 40 subtitles which were displayed in chronological order to the worker. Each batch has been annotated by three workers.",Not discussed,,"Novel Movie Dataset: 10,688 = 1,747+1,645+1,565+1,622+1,046+3,063;  Twitter (old): 24,802 tweets;  Fox news corpus (old): 1,528 annotated comments",Novel Movie Dataset: 6-fold cross-validation;  Twitter (old): 80% (~19842);  Fox news corpus (old): 80% (~1222).,Novel Movie Dataset: 6-fold cross-validation;,Turk workers,Yes,"In the annotation guidelines, the authors defined hateful speech as a language used to express hatred towards a targeted individual or group or is intended to be derogatory, to humiliate, or to insult the members of the group, based on attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender. Although the meaning of hate speech is based on the context, they provided the above definition agreeing to the definition provided by Nockleby et al. (2000); Davidson et al. (2017). Offensive speech uses profanity, strongly impolite, rude, or vulgar language expressed with fighting or hurtful words to insult a targeted individual or group (Davidson et al., 2017). They used the same definition also for offensive speech in the guidelines. The remaining subtitles were defined as normal.",Yes,40 cents per HIT for the pilot and/or the main annotation task,General,N/A,Targeted,"race, gender, sexuality, nationality, religion",Yes,No,,,No,,No,,,No,Human
218,No,"(Taradhita & Putra, 2021)",Hate Speech Classification in Indonesian Language Tweets by Using Convolutional Neural Network,Journal of ICT Research and Applications,Indonesia,"This paper proposes a convolutional neural network method for classifying hate speech in tweets in the Indonesian language. Datasets for both the training and testing stages were collected from Twitter. The collected tweets were categorized into hate speech and non-hate speech. The authors used TF-IDF as the term weighting method for feature extraction. The most optimal training accuracy and validation accuracy gained were 90.85% and 88.34% at 45 epochs. For the testing stage, experiments were conducted with different amounts of testing data. The highest testing accuracy was 82.5%, achieved by the dataset with 50 tweets in each category.",Yes,6,23.02.2021,21.09.2022,Yes,Yes,"Hate speech is an act of communication by a particular person or group that aims to insult a person or a group based on their ethnicity, race, religion, gender, sexual orientation, or class",No,,No,,,N/A,No,The dataset,1,1,Twitter,Hate Speech,Indonesian,Implicitly before 23.02.2021 (publication),Implicitly before 23.02.2021 (publication),N/A,2017 Jakarta governor election; the 2019 Indonesian presidential election,"This research was conducted with a text dataset containing tweets related to two political events, namely the 2017 Jakarta governor election and the 2019 Indonesian presidential election. The authors constructed a Python script using the Tweepy library in order to utilize the Twitter Streaming API for scraping tweets automatically based on keywords/hashtags and the date range of the tweet posts. Batches of tweets collected in this process were saved into a JSON file. The majority of tweets used in this dataset were collected from the Twitter Streaming API. Tweets related to the 2017 Jakarta governor election in the training dataset were also taken from the hate speech dataset created by Alfina, et al. with a total of 520 tweets. Each tweet in the dataset was manually labeled into two categories, namely ‘HS’ for tweets containing hate speech and ‘Non_HS’ for tweets not containing hate speech. The dataset for training included 630 tweets in each category. The datasets we used for testing had various numbers of tweets, i.e. 25, 50, 75, 100, 150, and 200 tweets in each category. All the datasets used for training and testing were saved in text (.txt) file format.",Not discussed,,manual,Not discussed,,Not discussed,,1660 (1260 train + 400 test),70% (882 tweets),50 - 400 (25 - 200 for each category),N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"race, religion, gender, sexuality, class",Yes,No,,,No,,No,,,No,Human
219,No,"(Saha et al., 2019)",Prevalence and Psychological Effects of Hateful Speech in Online College Communities,Proceedings of the 10th ACM Conference on Web Science,USA,"This work studies the online aspect of hateful speech in a dataset of 6 million Reddit comments shared in 174 college communities. To quantify the prevelence of hateful speech in an online college community, the authors devise College Hate Index (CHX) and  examine its distribution across the categories of hateful speech, behavior, class, disability, ethnicity, gender, physical appearance, race, religion, and sexual orientation. They then employ a causal-inference framework to study the psychological effects of hateful speech, particularly in the form of individuals’ online stress expression. Finally, they characterize their psychological endurance to hateful speech by analyzing their language– their discriminatory keyword use, and their personality traits. The results show that hateful speech is prevalent in college subreddits, and 25% of them show greater hateful speech than non-college subreddits. We also find that the exposure to hate leads to greater stress expression. However, everybody exposed is not equally affected; some show lower psychological endurance than others. Low endurance individuals are more vulnerable to emotional outbursts, and are more neurotic than those with higher endurance.",Yes,67,26.06.2019,21.09.2022,Yes,Yes,"In the specific setting of college campuses, we adopt Kaplin’s definition as a way to operationalize hateful speech in the online college communities: ..verbal and written words, and symbolic acts, that convey a grossly negative assessment of particular persons or groups based on their race, gender, ethnicity, religion, sexual orientation, or disability, which is not limited to a face-to-face confrontation or shouts from a crowd, but may also appear on T-shirts, on posters, on classroom blackboards, on student bulletin boards, in flyers and leaflets, in phone calls, etc.",Yes,"pattern (keyword) categories: behavior, class, disability, ethnicity, gender, physical, race, religion, sexual orientation, and other",No,,,N/A,No,college subreddit dataset,1,1,"Reddit, U.S. News (usnews.com) website,  SnoopSnoo (snoopsnoo.com) website",Hate Speech,English,XX.12.2017,August 2008 - November 2017,N/A,N/A,"The authors began by compiling a list of 200 major ranked colleges in the U.S. by crawling the U.S. News (usnews.com) website. Next, they crawled the SnoopSnoo (snoopsnoo.com) website, which groups subreddits into categories, one of which is “Universities and Colleges”. For 174 of these 200 colleges, they found a corresponding subreddit. As of December 2017, these subreddits had 3010 members on an average, and the largest ones were r/UIUC, r/berkeley, r/aggies, r/gatech, r/UTAustin, r/OSU, and r/ucf with 13K to 19K members.

Next, they built the dataset by running nested SQL-like queries on the public archives of Reddit dataset hosted on Google BigQuery. The final dataset for 174 college subreddits included 5,884,905 comments, posted by 453,781 unique users between August 2008 and November 2017. Within this dataset, 4,144,161 comments were posted by 425,410 unique users who never cross-posted across subreddit communities.",Yes,"This study works with public de-identified data from Reddit. The authors do not report any information that associates hateful speech and its psychological effects with specific individuals or college campuses. To describe the approach and to ground our research better, this paper includes paraphrased and partially masked excerpts of hateful comments, for which the authors suggest caution to readers.","automated, manual",Yes,"The authors adopt a pattern (keyword) matching approach by using a high-precision lexicon from two research studies on hateful speech and social media. This lexicon was curated after multiple iterations of filtering through automated classification, followed by crowdsourced and expert inspection. It consists of 157 phrases that are categorized into: behavior, class, disability, ethnicity, gender, physical, race, religion, sexual orientation, and other.

Using the above hate lexicon, for every subreddit in our dataset, they obtain a normalized occurrence of hateful speech, given as the fraction of keywords that matched the lexicon, to the total number of words in the subreddit’s comments. We obtain both category-specific and category-aggregated measures of hateful speech given in the lexicon.

They also manually annotated a random sample of 200 college subreddit comments to check concurrent validity of the approach.",Yes,200 college subreddit comments were randomly sampled for manual annotation.,"5,884,905 comments (200 manually annotated comments)",N/A,N/A,"Two researchers familiar with the literature on online hateful speech, independently rated if using the lexicon-based approach, these comments were correctly identified to have hateful content.",Not discussed,,Not discussed,,Specific,"class, disability, race, gender, body, religion, sexuality, other",Targeted,"race, gender, religion, sexuality, disability",No,Yes,"Behavior, Class, Disability, Ethnicity, Gender, Physical appearance, Race, Religion, Sexual orientation","class, disability, race, gender, body, religion, sexuality, other",No,,No,,,No,Human
220,No,"(Omar & Hashem, 2022)",An Evaluation of the Automatic Detection of Hate Speech in Social Media Networks,International Journal of Advanced Computer Science and Applications (IJACSA),"Saudi Arabia, Egypt","This study is concerned with evaluating the performance of the automatic detection and tracking of hate speech in Arabic content on Facebook. As an example, the study evaluates the period in October 2020 that came to be known as France’s cartoon controversy. Two different corpora were designed. The first corpus comprised 347 posts deleted by Facebook, now known as Meta. The second corpus was composed of 1,856 posts that were randomly selected using the hashtag إلا رسول الله (except the Prophet of Allah). The results indicate that there is a considerable amount of hate speech taken from or influenced by the Islamic religious discourse, but that automatic detection systems are unable to address the peculiar linguistic features of Arabic. There is also a lack of clarity in defining what constitutes “hate speech”. The study suggests that social media networks, including Facebook, need to adopt more reliable automatic detection systems that consider the linguistic properties of Arabic. Political thinkers and religious scholars should be involved in defining what constitutes hate speech in Arabic.",Yes,0,XX.02.2022,21.09.2022,Yes,Yes,"For the purposes of the study, Facebook’s Policy Rationale developed for the definition of hate speech is adopted: We define hate speech as a direct attack against people - rather than concepts or institutions- on the basis of what we call protected characteristics: race, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity and serious disease. We define attacks as violent or dehumanizing speech, harmful stereotypes, statements of inferiority, expressions of contempt, disgust or dismissal, cursing and calls for exclusion or segregation. We also prohibit the use of harmful stereotypes, which we define as dehumanizing comparisons that have historically been used to attack, intimidate, or exclude specific groups, and that are often linked with offline violence.",Yes,"4 lexical clusters. The most distinctive lexical features of Cluster 1 included words such as coexistence, tolerance, understanding, values, peace, and mercy. The second cluster included words such as ""terrorists"" ""murderers"" ""bloody"", and ""beasts"". The third cluster included words such ""bloody"" as ""pigs"", ""Jews"", ""Christians"", and ""enemies. Finally, the last cluster included almost all the words in the third cluster and encompassing different writing styles.  Clusters 2, 3, and 4 are classified as hate speech and harmful content.",No,,,N/A,No,the first corpus (deleted posts); the second corpus (based on the hashtag الله رسول إال [except the Prophet of Allah]),2,2,Facebook,Hate Speech,Arabic,18.10.2020 - 05.11.2020,Implicitly before 05.11.2020,N/A,France’s cartoon controversy in October 2020,"This study is based on two different corpora built from Facebook posts covering France’s cartoon controversy in October 2020. The first corpus is composed of 1,347 posts deleted by Facebook, now known as Meta. The second corpus comprises 1,856 posts that were randomly selected using the hashtag الله رسول إال [except the Prophet of Allah). Data were collected from October 18 through November 5, 2020. The study is limited to posts in Arabic.",Not discussed,,automated,Yes,"In the second corpus (based on the hashtag الله رسول إال [except the Prophet of Allah]), posts were clustered using vector space clustering methods. The posts were classified into four main groups (clusters). To identify the thematic features of each group, a centroid-based lexical analysis was carried out. The most distinctive lexical features of Cluster 1 included words such as coexistence, tolerance, understanding, values, peace, and mercy. The second cluster included words such as ""terrorists"". ""murderers"". ) ""bloody"". and ""beasts"". The third cluster included words such as ""pigs"", ""Jews"", ""Christians"", and ""enemies"". Finally, the last cluster included almost all the words in the third cluster and encompassing different writing styles. Based on Facebook's policies and definition of hate speech, Clusters 2, 3, and 4 are classified as hate speech and harmful content. Posts in these clusters constitute around 67% of the overall posts in the corpus.",,,"the first corpus (deleted posts): 1,347 posts;  the second corpus (based on the hashtag الله رسول إال [except the Prophet of Allah]): 1,856 posts, ~67% of the overall posts were classified into 4 clusters.",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,religion,Targeted,"race, nationality, disability, religion, sexuality, gender, body",No,Yes,Religion,religion,Yes,Islam,No,,,Yes,Human
221,No,"(Masud et al., 2021)",Hate is the New Infodemic: A Topic-aware Modeling of Hate Speech Diffusion on Twitter,2021 IEEE 37th International Conference on Data Engineering (ICDE),India,"In this work, the authors focus on exploring user behavior, which triggers the genesis of hate speech on Twitter and how it diffuses via retweets. They crawl a large-scale dataset of tweets, retweets, user activity history, and follower networks, comprising over 161 million tweets from more than 41 million unique users. They also collect over 600k contemporary news articles published online. They characterize different signals of information that govern these dynamics. The analyses differentiate the diffusion dynamics in the presence of hate from usual information diffusion. For predicting the initiation of hate speech for any given hashtag, they propose multiple feature-rich models, with the best performing one achieving a macro F1 score of 0.65. Meanwhile, to predict the retweet dynamics on Twitter, they propose RETINA, a novel neural architecture that incorporates exogenous influence using scaled dot-product attention. RETINA achieves a macro F1-score of 0.85, outperforming multiple state-of-the-art models. The analysis reveals the superlative power of RETINA to predict the retweet dynamics of hateful content compared to the existing diffusion models.",Yes,17,19.04.2021,21.09.2022,Yes,Yes,"hate speech. ""Hate speech is often characterized by the formation of echochambers, i.e., only a small group of people engaging with
such contents repeatedly.""",No,,Yes,Github,,https://github.com/LCS2-IIITD/RETINA,No,tweet corpus; online news articles corpus,2,2,"Twitter, News websites",Hate Speech,English,03.02.2020 - 14.04.2020,03.02.2020 - 14.04.2020,N/A,"JV: jamiaviolence, MOTR: MigrantsOn- The Road, TTS: timetosackvadras, JUA: jamiaunderat- tack, IBN: IndiaBoycotts/PR, ZNBK: ZeeNewsBanKaro, SCW: SaluteCoronaWarriors, IPIM: IslamoPhobicIndi- anMedia, DR2020: delhiriots2020, S4S: Seva4Society, PMCF: PMCaresFunds, C_19: COVID_19, HUA: Hin- dus_Under_Attack, WP: WarisPathan, LE: lockdownexten- sion, JCCTV: JamiaCCTV, TVI: TrumpVisitIndia, PNOP: PutNationOver Publicity, DE: DelhiExodus, DER: Del- hiElectionResults, ASMR: amitshahmustresign, R4GK: Restore4GinKashmir, DV: DelhiViolance, SNPR: Stop- NPR, 1C4DH: ICrore4DelhiHindu, NV: Nirbhaya Verdict, NM: NizamuddinMarkaz, 90DSB: 90daysofshaheenbagh, DEM: Demonetisation, NHR: NorthDelhiRiots, PMP: PM- Panuti, HLM: HinduLivesMatter, CV: ChineseVirus, UM: UmarKhalid.","They initially started collected data based on topics which led to a tweet corpus spanning across multiple years. To narrow down our time frame and ease the mapping of tweets to news, they restricted the time span from 2020-02-03 to 2020-04-14 and made use of trending hashtags. Using Twitter's official API, they tracked and crawled for trending hashtags each day within this duration. Overall, they obtained 31, 133 tweets from 13, 965 users. They also crawled the retweeters for each tweet along with the timestamps. To build the information network, they collected the followers of each user up to a depth of 3, resulting in a total of 41, 151, 251 unique users in the dataset. They also collect the activity history of the users, resulting in a total of 163, 042, 612 tweets in the dataset.

They also crawled the online news articles published within this span using the News-please crawler. They managed to collect a total of 683, 419 news articles for this period. After filtering for language, title and date, they were left with 319, 179 processed items. There headlines were used as the source of the exogenous signal.",Not discussed,,manual,Not discussed,,Not discussed,,"17, 877 tweets","15, 225 (80%)","3, 807 (20%)","Three professional annotators who have experience in analyzing online hate speech to annotate the tweets manually.

All of these annotators belong to an age group of 22-27 years and are active on Twitter. As the contextual know- edge of real-world events plays a crucial role in identifying hate speech, the authors ensure that the annotators are well-aware of the events related to the hashtags and topics.",Yes,Annotators were asked to follow Twitter’s policy as guideline for identifying hateful behavior. (https://help.twitter.com/en/rules-and-policies/hateful-conduct-policy),Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
222,No,"(Burtenshaw & Kestemont, 2021)",A Dutch Dataset for Cross-lingual Multilabel Toxicity Detection,Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021),Belgium,"This paper describes a cross-lingual approach to annotating multi-label toxicity text classification on a newly developed Dutch language dataset, using a model trained on English data. The authors present an ensemble model of one Transformer model and an LSTM using Multilingual embeddings. The combination of multilingual embeddings and the Transformer model improves performance in a cross-lingual setting.",Yes,0,06.09.2021,21.09.2022,Yes,Yes,"TOXICITY: Rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.

(Toxicity draws its origins from chemistry, referring to how a substance can damage an organism.)",Yes,"sub-labels: Toxicity, Severe Toxicity, Identity Attack, Insult, Profanity, and Threat.",No,,,,,"Multi-label toxicity (new); 
English data by Wulczyn et al.",2,1,AMiCA (dataset initially developed by Van Hee et al.),Toxicity,"Dutch, English",N/A,N/A,N/A,N/A,"The authors use a newly annotated version of the AMiCA dataset, initially developed by Van Hee et al., for cyberbullying tasks (through anonymous donation and simulation outlined by Emmery et al.). In addition, they performed further annotation for multi-label toxicity, following the label guidelines of Wulczyn et al..",Not discussed,,manual,Not discussed,,Not discussed,,10189 Dutch (New) and 35099 English (Wulczyn 2017) data.,10-fold cross-validation,10-fold cross-validation,Dutch native speakers,Yes,"The authors used the annotation instructions outlined in (Wulczyn et al., 2017c). They translated the instructions into Dutch, the native language of the annotators, and gave detailed guidance with an introductory tutorial and handout. Description and Example of labels from the Wikipedia Talk Labels: Toxicity Dataset

TOXICITY: Rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.

SEVERE TOXICITY: A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion.

IDENTITY ATTACK: Negative or hateful comments targeting someone because of their identity.

INSULT: Insulting, inflammatory, or negative comment to- wards a person or a group of people.

PROFANITY: Swear words, curse words, or other obscene or profane languages.

THREAT: Describes an intention to inflict pain, injury, or violence against an individual or group.",Not discussed,,Specific,other,Non-targeted,N/A,No,Yes,Identity attack,other,No,,No,,,No,Human
223,No,"(Akhter et al., 2020)",Automatic Detection of Offensive Language for Urdu and Roman Urdu,IEEE Access,China,This study is about the detection of offensive language from the user's comments presented in a resource-poor language Urdu. The authors propose the first offensive dataset of Urdu containing user-generated comments from social media. They use individual and combined n-grams techniques to extract features at character-level and word-level. They apply seventeen classifiers from seven machine learning techniques to detect offensive language from both Urdu and Roman Urdu text comments. Experiments show that the regression-based models using character n-grams show superior performance to process the Urdu language. Character-level tri-gram outperforms the other word and character n-grams. LogitBoost and SimpleLogistic outperform the other models and achieve 99.2% and 95.9% values of F-measure on Roman Urdu and Urdu datasets respectively.,Yes,28,15.05.2020,21.09.2022,Yes,Yes,"Offensive comments. ""People who are parted from each other based on geographic, religion, skin color, and culture (like division of Indian Subcontinent into India, Pakistan) often attack each other using offensive language.""",No,,Yes,Github,,https://github.com/pervezbcs/Urdu-Abusive-Dataset,Yes,Urdu Abusive Dataset (UOD); Roman Urdu dataset,2,1,YouTube,Offensiveness,Urdu,N/A,N/A,"India, Pakistan",N/A,"Urdu Abusive Dataset (UOD): The authors collect 2,171 comments and design a dataset of Urdu language from YouTube videos. All the comments are manually collected from political, entertainment, sports, and religion videos uploaded by India and Pakistan. They clean the dataset by removing non-Urdu words and characters, URLs, numbers and special characters from each comment.",Not discussed,,manual,Not discussed,,Not discussed,,2171,10-fold cross-validation,10-fold cross-validation,Three annotators that are graduate students and are local speakers of the Urdu language annotate Dataset.,Not discussed,,Not discussed,,General,N/A,Targeted,"nationality, religion, race, other",Yes,No,,,No,,No,,,No,Human
224,No,"(Tarwani et al., 2019)",Cyberbullying Detection in Hindi-English Code-Mixed Language Using Sentiment Classification,Advances in Computing and Data Sciences,India,"The authors created the Hinglish Cyberbullying Comments (HCC) labeled dataset consisting of comments from social media networks such as Instagram and YouTube. They also developed eight different machine learning models for sentiment classification in-order to automatically detect incidents of cyberbullying. Performance measures namely accuracy, precision, recall and f1 score are used to evaluate these models. Eventually, a hybrid model is developed based on top performers of these eight baseline classifiers which perform better with an accuracy of 80.26% and f1-score of 82.96%.",Yes,8,19.07.2019,21.09.2022,Yes,Yes,"Cyberbullying. The problem of hate inducing speech, offensive and abusive posts on social media has come into the picture which can be referred as cyberbullying.",No,,Yes,Github,,https://github.com/mananjethanandani/HCC-Dataset,No,Hinglish Cyberbullying Comments (HCC) dataset,1,1,"Instagram, YouTube",Cyberbullying,Hindi-English,N/A,N/A,N/A,N/A,"The authors gathered user comments from different viral and popular posts on Instagram and YouTube. All these posts consisted comments written in different languages like Hindi, English, and Hinglish. Out of all these comments, 1010 were filtered comments written in Hinglish along with usernames.",Not discussed,,manual,Not discussed,,Not discussed,,1010,757 (75%),253 (25%),Three NLP researchers,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
225,No,"(Pronoza et al., 2021)",Detecting ethnicity-targeted hate speech in Russian social media texts,Information Processing & Management,"Russia, Spain","This work focuses on hate speech detection in the Russian language targeted at ethnic minorities. The authors use a dataset of over 2,6M user messages mentioning ethnic groups to construct a representative sample of 12K instances (ethnic group, text) that are further thoroughly annotated via a special procedure. They then experiment with four types of machine learning models, from traditional classifiers such as SVM to deep learning approaches, notably the recently introduced BERT architecture, and interpret their predictions in terms of various linguistic phenomena. In addition to hate speech detection with a text-level two-class approach (hate, no hate), they also justify and implement a unique instance-based three-class approach (positive, neutral, negative attitude, the latter implying hate speech). The best results in this study are achieved by using fine-tuned and pre-trained RuBERT combined with linguistic features, with F1-hate=0.760, F1-macro=0.833 on the text-level two-class problem comparable to previous studies, and F1-hate=0.813, F1-macro=0.824 on the unique instance-based three-class hate speech detection task. Finally, they perform error analysis, and it reveals that further improvement could be achieved by accounting for complex and creative language issues more accurately, i.e., by detecting irony and unconventional forms of obscene lexicon.",Yes,13,01.11.2021,21.09.2022,Yes,Yes,"Hate speech usually include the incitement of violence and / or usage of language that attacks or diminishes groups such as ethnic minorities. The broader definition further includes statements that have any negative bias against certain groups, even if they are expressed in subtle forms.  The authors of this study adopt the broader definition of hate speech.",Yes,"three classes: negative, positive, neutral",Yes,Other,SCILA HSE,https://scila.hse.ru/data/2021/05/25/1438275158/RuEthnoHate.zip  https://scila.hse.ru/data/2021/05/25/1438273746/RuEthnoHateExtended.zip,Yes,RuEthnics dataset; RuEthnoHate dataset,2,1,Vkontakte,Hate Speech,Russian,N/A,XX.01.2014 - XX.12.2015,N/A,N/A,"A. The initial annotated collection

1 The authors formed a list of ethnonyms based on the Russian Census [2010] and other sources. It represents a nested array of 115 Russian and post-Soviet ethnic groups, where each group is represented by a list of unigrams and bigrams (e.g. ""Jew"", ""Jewish girl"", ""Jewish nation"" etc), including ethnophaulisms (ethnic slurs) and pseudo-ethnicities (Caucasian, Asian).

2 The authors obtained a collection of all messages containing at least one ethnonym from our list ever posted on all Russian language social media during two years (from January 2014 to December 2015). Having been purchased from a commercial social media aggregator, IQBuzz', this collection turns out to be composed mainly (by 80%) of messages from Vkontakte, a replica of Facebook and the most popular social network in Russia. After filtering out duplicates the collection numbered 2,660,222 messages; hereafter this dataset is referred to as the RuEthnics dataset.

3 Next, the authors formed our first collection for annotation which was substantially smaller than RuEthnics. As the distribution of ethnic groups in the dataset was very unbalanced, the authors over-represented infrequent groups based on manually-derived balancing quotas adopted for each ethnic group. They also limited message length to the range [20; 90] words. In all other respects, the sampling was random. This ensured realistic class distribution in the collection.

This initial annotated collection comprised 14,998 texts as described in more detail in [Koltsova, Alexeeva et al., 2017].

B. modified for this work

1 For this, out of 14,998 texts they obtained 27,165 attitude instances (ethnic group, text) and then selected 11,067 instances on which at least two annotators agreed. The negative class comprised 12% of the sample with 1,365 instances.

2 To increase the quality of the dataset, the authors enriched the sample by the following steps (see [Hernandez et al., 2013] for discussion): They trained a set of simple classifiers on 11,067 instances and obtained the best precision by Gradient Boosting (GB) with n-gram and linguistic features. It was then run on the full RuEthnics dataset. From the negative instances identified by GB the authors randomly selected 985 instances, according to the balancing quota approach. For these, they repeated the entire procedure of the annotation. The instances were added to our dataset, the annotators' labels being used as ground truth. The proportion of the negative class has increased by 5%, although the annotators disagreed with the classifier in 31% of cases.",Not discussed,,manual,Yes,"A. annotation in the initial dataset

Each text was annotated by at least three independent specially trained annotators who were asked to select answers for a list of questions, including the filtering questions about text interpretability and the presence of ethnonyms. Among other things, this resulted in adding instances of ethnonyms that had not occurred in our initial list.

As their main task, the annotators were asked to annotate the overall attitude of the text author to the ethnic group or one indi- vidual (negative/neutral/positive) making special emphasis on negative ones that implied hate speech. The main question sounded as follows: ""what is the overall attitude of the text author to the ethnic group or its representative?"" (negative/neutral/positive).

B. Error annotation

First, a sample output of the models was selected randomly from the test datasets (each test fold in the 10-fold cross-validation). The authors selected four representative models for the analysis. Having four models, they resulted in 24 possible error combinations of correct/incorrect predictions. For each of these 16 types of combinations, they randomly selected three instances (one for each of the three true classes:positive, negative and neutral) from each of the ten test folds. Since the dataset is unbalanced, for some error combinations there were not all the three classes present in the examples. In this case in the error sample they gave preference to the negative class, implying hate speech. The resulting error annotation dataset consists of 473 instances: 182 negative, 176 neutral and 115 positive instances.

Second, they engaged four annotators: two researchers in Social Informatics (one PhD, one BSc-level), and two in Computational Linguistics (one PhD, one MSc-level). The task included annotating each instance represented by a pair (ethnic group, text) in the sample output with labels of relevant linguistic phenomena present in the corresponding text. More than one label could be assigned to one instance. Each annotator also assigned a specific mark to each instance indicating their agreement with the initial class annotation.

Next, the authors calculated the inter-annotator agreement in the second annotation between the label sets assigned by four annotators. Krippendorff's alpha was applied.",Yes,"A. annotation in the initial dataset

The authors enriched the sample by the following steps: They trained a set of simple classifiers on 11,067 instances and obtained the best precision by Gradient Boosting (GB) with n-gram and linguistic features. It was then run on the full RuEthnics dataset. From the negative instances identified by GB the authors randomly selected 985 instances, according to the balancing quota approach. For these, they repeated the entire procedure of the annotation.

B. Error annotation

First, a sample output of the models was selected randomly from the test datasets (each test fold in the 10-fold cross-validation). The authors selected four representative models for the analysis. Having four models, they resulted in 24 possible error combinations of correct/incorrect predictions. For each of these 16 types of combinations, they randomly selected three instances (one for each of the three true classes:positive, negative and neutral) from each of the ten test folds.","A. 5,594 texts (12,052 instances)  B. 473 instances (in error annotation)",10-fold cross-validation,10-fold cross-validation,"A. three independent specially trained annotators

B. (in error annoation) four annotators: two researchers in Social Informatics (one PhD, one BSc-level), and two in Computational Linguistics (one PhD, one MSc-level).",Not discussed,,Not discussed,,Specific,race,Targeted,race,Yes,Yes,Ethnicity,race,No,,No,,,Yes,Human
226,No,"(Bagga et al., 2021)",“Are you kidding me?”: Detecting Unpalatable Questions on Reddit,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,Canada,"First, the authors propose a novel task of detecting a subtler form of abusive language, namely unpalatable questions. Second, they publish a context-aware dataset for the task using data from a diverse set of Reddit communities. Third, they implement a wide array of learning models and also investigate the benefits of incorporating conversational context into computational models. The results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive.",Yes,3,19.04.2021,21.09.2022,Yes,Yes,"For our task, we define an unpalatable question as a negatively phrased question designed to antagonise its recipient.

Unpalatable questions (UQ) is a subtler form of abusive language.",No,,Yes,Github,,https://github.com/networkdynamics/unpalatable-questions,Yes,Unpalatable Questions (UQ) dataset,1,1,Reddit,"Unpalatable Questions, Abusiveness",English,N/A,N/A,N/A,N/A,"The authors collect data from a diverse set of 15 online communities (or subreddits) belonging to different genres:

• Politics:- r/The _Donald - r/politics - r/PoliticalDiscussion - r/Conservative

• Sports: - r/nfl - r/sports - r/hockey

• Hate and Toxic: - r/cringepics - r/cringe - r/4chan - r/CringeAnarchy - r/KotakulnAction - r/ImGoing ToHellForThis - r/TumblrInAction

The subreddits were carefully selected to prevent the dataset from being heavily skewed towards not unpalatable samples since these topics are more likely to involve opinionated and antagonistic discussions. They also preserve conversational context in the dataset by including the preceding comment in the discussion.

During data collection, the authors filter out comments that did not contain a question using the simple rule-based approach where we tokenize the comment and extract sentences that end with a ""?'.",Not discussed,,manual,Yes,"The authors use Amazon Mechanical Turk for crowdsourcing the data annotations. The coders were shown the main comment and also the preceding comment for context. They were asked to label the main comment for whether it contained an unpalatable question or not. Each comment in our dataset is labeled by at least five different coders.

The authors employ three measures to ensure high quality annotations. First, they were able to provide high-quality training to the coders through the use of clear instructions that laid out detailed tips, examples, and counter examples. Second, the allowed only qualified coders to contribute to the task - they were required to achieve a perfect score on a quiz which had a total of 10 questions. Third, the inserted secret test questions throughout the task to address the issue of spam responses. The coders were disqualified and blocked if their accuracy on the test questions fell below the predefined threshold of 90%.

The authors aggregated the five annotations by taking the majority as the final label - a data sample is considered unpalatable if at least 3 coders labeled it as unpalatable. In order to not lose useful information, they added a confidence dimension to the dataset which is the ratio of the number of annotations with the majority label and the total number of annotators: confidence ∈ {0.6, 0.8, 1.0}.

The authors compute two mea- sures of inter-annotator reliability: (1) Cohen's Kappa, and (2) Krippendorff's alpha.",Yes,"the authors filter out comments that did not contain a question using the simple rule-based approach where we tokenize the comment and extract sentences that end with a ""?'.",10909 Reddit comments,5-fold cross-validation,5-fold cross-validation,"Annotators recruited via Amazon Mechanical Turk, under quality control measures implemented by the authors",Yes,"Instructions for the crowdsourcing task that can be seen by Mechanical Turk Workers include definition of unpalatable questions, tips to determine if a questions is unpalatable, example of unpalatable questions, and example of NOT unpalatable questions.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
227,No,"(Gao & Huang, 2017)",Detecting Online Hate Speech Using Context Aware Models,"Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017",USA,"In this paper, the authors provide an annotated corpus of hate speech with context information well kept. Then they propose two types of hate speech detection models that incorporate context information, a logistic regression model with context features and a neural network model with learning components for context. The evaluation shows that both models outperform a strong baseline by around 3% to 4% in F1 score and combining these two models further improve the performance by another 7% in F1 score.",Yes,178,04.09.2017,21.09.2022,Yes,Yes,"Hateful speech: the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.",No,,Yes,Github,,https://github.com/sjtuprog/fox-news-comments,Yes,The Fox News User Comments corpus,1,1,Fox News website,Hate Speech,English,N/A,N/A,N/A,N/A,"The authors created the Fox News User Comments corpus that were posted by 678 different users in 10 complete news discussion threads in the Fox News website. The 10 threads were manually selected and represent popular discussion threads during August 2016. All of the comments included in these 10 threads were annotated. The number of comments in each of the 10 threads is roughly equal. Rich context information was kept for each comment, including its user screen name, the comments and their nested structure and the original news article.",Not discussed,,manual,Yes,"The two annotators first discussed and practices before they started annotation. They achieved a surprisingly high Kappa score Cohen (1960) of 0.98 on 648 comments from 4 threads.

For those comments which annotators disagreed on, the authors label them as hateful as long as one annotator labeled them as hateful. Then one annotator continued to annotate the remaining 880 comments from the remaining six discussion threads.",Not discussed,,1528,10-fold cross-validation,10-fold cross-validation,Two native English speakers,Yes,"The annotation guidelines are similar to the guidelines used by Nobata et al. (2016). The authors define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. The labeling of hateful speech in the corpus is binary. A comment will be labeled as hateful or non-hateful.

The annotation guidelines with examples are available at:

https://github.com/sjtuprog/fox-news-comments/blob/master/AnnotationGuidelineswithExamples.pdf",Not discussed,,General,N/A,Targeted,"gender, race, sexuality",Yes,No,,,No,,No,,,No,Human
228,No,"(Rodríguez-Sánchez et al., 2020)",Automatic Classification of Sexism in Social Networks: An Empirical Study on Twitter Data,IEEE Access,Spain,"In this work, the authors propose a new task that aims to understand and analyze how sexism, from explicit hate or violence to subtle expressions, is expressed in online conversations. To this end, they have developed and released the first dataset of sexist expressions and attitudes in Twitter in Spanish (MeTwo) and investigate the feasibility of using machine learning techniques (both traditional and novel deep learning models) for automatically detecting different types of sexist behaviours. The results show that sexism is frequently found in many forms in social networks, that it includes a wide range of behaviours, and that it is possible to detect them using deep learning approaches. They discuss the performance of automatic classification methods to deal with different types of sexism and the generalizability of our task to other subdomains, such as misogyny.",Yes,41,04.12.2020,21.09.2022,Yes,Yes,"The Oxford English Dictionary defines sexism as prejudice, stereotyping or discrimination, typically against women, on the basis of sex. Similarly, the Real Academia Española de la Lengua defines it as discrimination of people on the basis of sex. In general, sexist behaviours and discourses underestimate the role of women.

The Oxford English Dictionary defines misogyny as hatred or dislike of, or prejudice against women. Sexism does not always imply misogyny. (Sexism includes misogyny but is not limited to it.)",Yes,"Three labels for sexiam annotation: Sexist, non-sexist, and doubtful  Terms selected to build the corpus were grouped into categories: Ideological discredit, Role stereotyping, Inferiority/incapacity Sexualization, Male dominance, Hate and violence, Physical stereotyping",Yes,Github,,https://github.com/franciscorodriguez92/MeTwo,Yes,"MeTwo: Machismo and Sexism Twitter Identification dataset (new)

AMI dataset (published in IberEval competition)
",2,1,Twitter,Sexism,Spanish,XX.07.2018 - XX.12.2018,XX.07.2018 - XX.12.2018,N/A,N/A,"The authors first collected a number of popular expressions and terms commonly used to underestimate the role of women in our society, encourage the harassment towards them or limit their freedom of speech. The main source for such expressions was the Twitter account of the Spanish journalist Ana Isabel Bernal-Triviño, which collects phrases and expressions that women (Twitter users) have received on a day-to-day basis, and that have made them feel belittled and undermined because of their genre. They manually inspected them to select both expressions that may be clearly offensive, or even violent, and expressions that are subtle or even normalized. Initially, a total of 29 Spanish terms and expressions were considered as keywords to create the corpus.

Starting from these expressions, they used the Twitter API to search for tweets containing all selected keywords. Data was collected between July and December 2018, gathering 181792 tweets for the 29 terms. The initial setup of the crawler implies collecting 100 tweets for each term daily, thus, ideally they would collect 2900 tweets per day. Using this methodology for corpus construction, they ensure that we obtain both sexist and non-sexist tweets for each keyword expression.

They established two constraints to the data collection process. (1) They limited the collection of tweets to 15000 per keyword expression. (2) They set a minimum threshold of 150 tweets per expression. After collecting the information, they took a random sample of 150 tweets per term. They verified that tweets had a difference of at least one day to avoid conversations and to ensure data is spread over the six months. They discarded 5 terms since they do not reach 150 tweets. As mentioned above, they sample the original dataset to build the final corpus composed by 3600 tweets.",Not discussed,,manual,Yes,"MeTwo dataset was annotated with three labels (Sexist, non-sexist, and doubtful) and was labeled based on a majority vote by three annotators. In case of total disagreement, a fourth annotator decided the final label. Of the 3600 tweets, the fourth annotator was only required in 20 tweets.

The authors opted for the Cohen’s kappa coefficient to evaluate the inter-annotator agreement.",Yes,"They established two constraints to the data collection process. (1) They limited the collection of tweets to 15000 per keyword expression. (2) They set a minimum threshold of 150 tweets per expression. After collecting the information, they took a random sample of 150 tweets per term. They verified that tweets had a difference of at least one day to avoid conversations and to ensure data is spread over the six months. They discarded 5 terms since they do not reach 150 tweets. As mentioned above, they sample the original dataset to build the final corpus composed by 3600 tweets.",3600,10-fold cross-validation,10-fold cross-validation,N/A,Yes,The authors developed an annotation guide in which they provided a clear explanation of each label along with a number of examples.,Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,"Sex, Gender (Sexiam)",gender,Yes,Women,No,,,No,Human
229,No,"(Gitari et al., 2015)",A Lexicon-based Approach for Hate Speech Detection,International Journal of Multimedia and Ubiquitous Engineering,China,"The authors explore the idea of creating a classifier that can be used to detect presence of hate speech in web discourses such as web forums and blogs. In this work, hate speech problem is abstracted into three main thematic areas of race, nationality and religion. The goal of this research is to create a model classifier that uses sentiment analysis techniques and in particular subjectivity detection to not only detect that a given sentence is subjective but also to identify and rate the polarity of sentiment expressions. They begin by whittling down the document size by removing objective sentences. Then, using subjectivity and semantic features related to hate speech, they create a lexicon that is employed to build a classifier for hate speech detection. Experiments with a hate corpus show significant practical application for a real-world web discourse.",Yes,393,30.04.2015,21.09.2022,Yes,Yes,“Hate speech is a kind of writing that disparages and is likely to cause harm or danger to the victim” ,Yes,"annotated with 3 point scale: Not-Hateful (NH, both positive and neutral), weakly hateful (WH), and strongly hateful (SH);  semantic classes: religion, ethnicity and race",No,,,,,FIRST corpus; SECOND corpus ,2,2,"10 different websites from a list provided in the Hate Directory, one paragraph snippets of quotes relating to the Israel-Palestinian conflict ",Hate Speech,English,N/A,N/A,N/A,Israel-Palestinian conflict ,"FIRST corpus was crawled on diverse dates a total of 100 blog postings (documents) from 10 different websites, 10 for each site, from a list provided in the Hate Directory. This is a directory compiled by Raymond Franklin of sites that are considered to be generally offensive. We were only concerned with blogs that were thematically related to areas of ethnicity, religion and nationality. 

SECOND corpus consists of largely one paragraph snippets of quotes relating to the Israel-Palestinian conflict.  ",Not discussed,,manual,Yes,"The authors asked two graduate students in our university to undertake sample annotation representing approximately 30% of our corpus.

For the FIRST corpus, they randomly selected 3 blog postings from each of the 10 websites for a total of 30 blogs. Similarly for the SECOND corpus, out of a 150 page document, they divided the document into three sections of 50 pages each and selected the first 15 pages in each section for a total of 45 pages.

To ease the annotation process, they asked that the ratings be done on a paragraph basis in case of essay-like documents and use snippets of sentences for the quotes. They rated using a 3 point scale of Not-Hateful (NH, both positive and neutral), weakly hateful (WH) and strongly hateful (SH). In total 180 paragraphs were labeled for the FIRST corpus and 320 paragraphs for the SECOND corpus. The agreement scores for the annotators were calculated.

To resolve cases of disagreement among our two annotators, a committee of the two and one author relabeled the contentious paragraphs strictly on the basis of our definition of hate speech resulting in our gold corpus. In difficult situations a consensus was arrived at using a majority vote.",Yes,"For the FIRST corpus, they randomly selected 3 blog postings from each of the 10 websites for a total of 30 blogs.  

For the SECOND corpus, out of a 150 page document, they divided the document into three sections of 50 pages each and selected the first 15 pages in each section for a total of 45 pages. ",FIRST corpus: 180 paragraphs  SECOND corpus: 320 paragraphs,N/A,FIRST corpus: 180 paragraphs,"Two graduate students in the authors’ university.  

A committee of the two and one author re-labeled the contentious paragraphs in the case of disagreement.  ",Not discussed,,Not discussed,,Specific,"race, nationality, religion",Non-targeted,N/A,Yes,Yes,"Race, Nationality, Religion","race, nationality, religion",No,,No,,,No,Human
230,No,"(García-Díaz et al., 2021)",Detecting misogyny in Spanish tweets. An approach based on linguistics features and word embeddings,Future Generation Computer Systems,"Spain, Norway","The authors apply Sentiment Analysis and Social Computing technologies for detecting misogynous messages in Twitter. They compiled the Spanish MisoCorpus-2020, a balanced corpus regarding misogyny in Spanish, and classified it into three subsets concerning (1) violence towards relevant women, (2) messages harassing women in Spanish from Spain and Spanish from Latin America, and (3) general traits related to misogyny. The proposal combines a classification based on average word embeddings and linguistic features in order to understand which linguistic phenomena principally contribute to the identification of misogyny. The authors have evaluated our proposal with three machine-learning classifiers, achieving the best accuracy of 85.175%. Finally the proposed approach is also validated with existing corpora for misogyny and aggressiveness detection such as AMI and HatEval obtaining good results",Yes,57,01.01.2021,21.09.2022,Yes,Yes,"According to Kate Manne in ""The Logic of Misogyny"", misogyny refers to social environments in which ""women will tend to face hostility of various kinds because they are women in a man's world who are held to be failing to live up to men's standards"".

If the tweet contained certain traits of misogynous behaviour, such as objectification, dominance, derailing, sexual harassment, or discredit, the tweet was labelled as misogyny.",No,,Yes,Website,,https://pln.inf.um.es/corpora/misogyny/misocorpus-spanish-2020.rar,Yes,"MisoCorpus-2020 (new)

Subsets: Violence Against Relevant Women (VARW), European Spanish vs that of Latin America (SELA), Discredit, Dominance, Sexual harassment, and Stereotype (DDSS)

Other existing datasets: AMI dataset, task A of HatEval 2019",6,4,Twitter,Misogyny,Spanish,N/A,N/A,"A subset SELA compiles tweets from Latin American and European Spanish people separately (parameters: latitude, longitude, radius)",N/A,"The authors followed the same approaches employed in IberEval and Evallta and used Twitter as a data provider. They compiled around 32,969 tweets from the criteria previously described for each subset of the corpus:

• Violence Against Relevant Women (VARW). This corpus contains tweets directed towards relevant specific women (e.g., Greta Thunberg). The authors also selected the accounts of women politicians from the left-wing and right-wing Spanish parties, in addition of those of journalists and other relevant women. They attained tweets from news site accounts related to these women in order to have objective tweets which reported cases but that could not be considered as misogynous.

• European Spanish vs that of Latin America (SELA). The authors compile tweets from Latin American and European Spanish people separately. The tweets from Latin America were obtained from latitude: -0.1596997, longitude: -78.452125313, radius: 1, 500 km whereas the Spanish tweets were collected from latitude: (40.416705, longitude: -3.703583, radius: 520 km.

• Discredit, Dominance, Sexual harassment, and Stereotype (DDSS). The authors extracted tweets from keywords related to misogynistic traits, such as discredit, dominance, sexual harassment or stereotype. They mainly selected those tweets that discussed topics related to gender violence. In addition, and as a consequence of the Spanish electoral debate, there was a discussion in which certain political parties dismissed the concept of gender violence in favour of the term violencia intrafamiliar o violencia doméstica (domestic violence) with the aims of dismissing the visibility of gender violence.",Not discussed,,"automated, manual",Yes,"This work manually classified the tweets into misogyny, not-misogyny, or out-of-domain. However, all the tweets captured from news sites were automatically labelled using a distant supervision approach.

The manual classification of the tweets were performed by seven people. The out-of-domain tweets were removed. A tweet labelled as misogyny by one annotator and labelled as non-misogyny by another was discarded. Furthermore, the following guidelines were applied in order to select representative non-misogynous tweets: (1) Tweets compiled from news sites were automatically annotated as not misogyny, and (2) tweets labelled as not misogyny and out-of-domain were considered to be not misogyny tweets.

Each tweet was rated individually by at least two annotators. The authors calculated the inter-coder reliability by applying the Krippendorff’s Alpha.",Not discussed,,7682,10-fold cross-validation,10-fold cross-validation,"Seven people, five of whom were women and two of whom were men. They were the authors of the manuscript, two colleagues from the University of Murcia, in addition to a Master's degree student.",Yes,"The following guidelines were considered:

• If the tweet contained images or links, and they were necessary to understand the meaning of the tweet, the tweet was labelled as out-of-domain.

• If the tweet contained words in the Catalan or Basque languages, the tweet was labelled as out-of-domain.

• If the tweet contained figurative language that made the motivation of the text unclear, the tweet was labelled as out-of-domain.

• If the tweet contained certain traits of misogynous behaviour, such as objectification, dominance, derailing, sexual harassment, or discredit, the tweet was labelled as misogyny. 

• If the tweet contained a clear predisposition towards equality between men and women, even if slag or offensive language were employed, the tweet was labelled as not-misogyny.

• If the Twitter account belonged to an online news site, the tweet was labelled as not-misogyny.",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,"Sex, Gender",gender,Yes,Women,Yes,"Greta Thunberg, women politicians from the left-wing, right-wing Spanish parties, journalists, other",To a person,No,Human
233,No,"(Ahmed, Rahman, Nur, Islam, & Das, 2021)",Deployment of Machine Learning and Deep Learning Algorithms in Detecting Cyberbullying in Bangla and Romanized Bangla text: A Comparative Study,"2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",Bangladesh,"In this study, they developed a model to detect cyberbullying from Bangla and Romanized Bangla texts using Machine Learning and Deep Learning algorithms. They also presented a comparative analysis among the algorithms in terms of accuracy, precision, recall, f1-score and roc area. They prepared three datasets for Bangla, Romanized Bangla and a combination of both from social media. The three datasets contained 5000 Bangla, 7000 Romanized Bangla and a combination of 12000 Bangla and Romanized Bangla texts. ",Yes,12,06.04.2021,04.01.2023,Yes,Yes,"Cyberbullying. ""Cyberbullying can be defined as the willing and frequent harm imposed through electronic media (Hinduja & Patchin, 2008).""",No,,No,,,,,"Dataset 1, 2, 3",3,3,YouTube,Cyberbullying,Bengali,N/A,N/A,N/A,Not discussed,"To prepare the datasets, we manually selected YouTube videos of some well known Bangladeshi social media personals and used YouTube API version 3.0 to collect comments. The collected comments contained Bangla and Romanized Bangla texts. For the rest of the paper, the dataset containing the Bangla texts will be denoted as Dataset 1, the dataset containing the Romanized Bangla texts will be denoted as Dataset 2 and the combined dataset will be denoted as Dataset 3. They split the texts into two datasets. Dataset 1 contained 5000 Bangla texts and Dataset 2 contained 7000 Romanized Bangla texts. After that a third dataset was created by combining the first two datasets which contained a total of 12000 texts. ",Not discussed,,manual,Yes,"They manually labelled all three datasets in two classes, bullying and not bullying. ",Not discussed,,Dataset 1: 5000; Dataset 2: 7000; Dataset 3: 12000,Dataset 1: 5000; Dataset 2: 5600; Dataset 3: 9600,Dataset 1: 1000; Dataset 2: 1400; Dataset 3: 2400,Not discussed,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
234,No,"(Ottoni et al., 2018)","Analyzing Right-wing YouTube Channels: Hate, Violence and Discrimination",Proceedings of the 10th ACM conference on web science ,"Brazil, USA","In this paper, they observe issues related to hate, violence and discriminatory bias in a dataset containing more than 7,000 videos and 17 million comments. They investigate similarities and differences between users’ comments and video content in a selection of right-wing channels and compare it to a baseline set using a three-layered approach, in which they analyze (a) lexicon, (b) topics and (c) implicit biases present in the texts. Among other results, their analyses show that right-wing channels tend to (a) contain a higher degree of words from “negative” semantic fields, (b) raise more topics related to war and terrorism, and (c) demonstrate more discriminatory bias against Muslims (in videos) and towards LGBT people (in comments). The findings shed light not only into the collective conduct of the YouTube community promoting and consuming right-wing content, but also into the general behavior of YouTube users.",Yes,71,30.05.2018,04.01.2023,Yes,Yes,"Hate speech and discriminatory bias. ""…the presence of hateful vocabulary, violent content and discriminatory biases…""",No,,No,,,,,"Right-wing dataset, Baseline dataset",4,2,YouTube,Hate Speech,English,28.09.2017 - 12.10.2017,N/A,N/A,N/A,"Alex Jones’ channel and these other 12 channels supported by him were then collected using the YouTube Data API from September 28 to October 12 2017. From all videos posted in these channels (limited to around 500 videos per channel due to API limits), they collected (a) the video captions (written versions of the speech in the videos, manually created by the video hosts or automatically generated by YouTube’s speech-to-text engine), representing the content of the videos themselves; and (b) the comments (including replies to comments) posted to the videos. The total number of videos collected from these channels is 3,731 and the total number of comments collected from them is 5,071,728.
They collected the same information (captions and comments) from videos posted in the ten most popular channels (in terms of the number of subscribers in November 7 2017) of the category ""news and politics"" according to the analytics tracking site Social Blade. The total number of videos collected from the baseline channels is 3,942 and the total number of comments collected from them is 12,519,590. ",No,,automated,Not discussed,,Not discussed,,"Right-wing dataset: 5,071,728; Baseline dataset: 12,519,590",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,political,Non-targeted,N/A,No,No,,,Yes,Right-wing people,No,,,No,Human
235,No,"(Lozano, Cedeño, Castillo, Layedra, Lasso, & Vaca, 2017)",Requiem for Online Harassers: Identifying Racism from Political Tweets,2017 Fourth International Conference on eDemocracy & eGovernment (ICEDEG) ,Ecuador,"In this work, they collect tweets produced by the ego networks of the two former 2016 US Presidential Candidates: Hillary Clinton and Donald Trump, grouped in four datasets. After deleting spammers, they get 84,371 unique users labelled by using two different metrics: Sentiment Word Count and Racist Score. Both of them let them not only identify users as racists but also detect the level of negativism by analyzing their most recent 200 tweets, increasing the effectiveness of the method. Using it, they find the most negative and racist users and the most positive and non-racist users from all datasets. Taking advantage of the topological properties of the ego networks they analyzed, they also verify that our results satisfy the sociologist theory of homophily; where the followers of each candidate represent their homophilous. ",Yes,12,03.07.2017,04.01.2023,Yes,Yes,"Racist content. ""Those users who engage in polarized discussions sometimes might write racist content, in such way it expresses their attitude against a specific topic.""",No,,No,,,,,Dataset,5,4,Twitter,Racism,English,15.06.2016 - 27.06.2016,15.06.2016-27.06.2016,N/A,2016 US presidential election ,"Twitter API REST gives programmatic access to writing and reading data on Twitter. Using this API, they collect 31,888 unique Twitter users as follows. First, they consider as our seed users, the Twitter accounts of the 2016 US presidential election candidates, @HillaryClinton and @realDonaldTrump. Second, they collect their ego-networks, i.e. their “followers” and “friends” and finally, they proceed to remove “spam users”.
Twitter also has an API Streaming that, unlike Twitter API REST, lets them collect tweets in JSON format in real-time. Thus, they ran a tweet crawler from June 15th to June 27th, 2016 when the US presidential election was already running. They collect four datasets by using this crawler, filtering content written in English only and geolocated inside a United States of America bounding box.
",Not discussed,,automated,Yes,"After defining unique Twitter users, they analyze the last 200 tweets of every account timeline in order to quantify the homophily between users and presidential candidates. They infer the political leaning of every user by analyzing the use of political hashtags that represent a specific candidate.
They also calculate through sentiment analysis, how positive or negative is the content of each user timeline. In addition, they quantify how racist a user is, based on the racist score they propose in Section III. Using these scores or features, they classify users into five clusters.",Yes,The last 200 tweets,16874200,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,race,Targeted,race,No,Yes,Race,race,No,,No,,,No,Human
236,No,"(Song, Ryu, Lee, & Park, 2021)",A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit,Proceedings of the 25th Conference on Computational Natural Language Learning,South Korea,"In this paper, they propose a Comprehensive Abusiveness Detection Dataset (CADD), collected from English Reddit posts, with multifaceted labels and contexts. The dataset is annotated hierarchically for efficient annotation through crowdsourcing on a large scale. They also empirically explore the characteristics of the dataset and provide a detailed analysis for novel insights. The results of the experiments with strong pre-trained natural language understanding models on the dataset show that their dataset gives rise to meaningful performance, assuring its practicality for abusive language detection.
",Yes,0,11.11.2021,05.01.2023,Yes,Yes,"Abusiveness. ""In the present study, we assume that abusive language can fall into four types, following Nobata et al. (2016): Hate Speech (HS) A language that attacks people with a particular identity with respect to properties such as race, religion, gender, and age; Derogatory (D) A language that attacks a group or an individual, but is not considered hate speech; Profanity (P) A language that contains any sexual remarks or slur; or Non-abusive (N) A language that is not in any categories of abusive language.""",Yes,Hate Speech; Derogatory; Profanity; Non-abusive,Yes,Github,,https://github.com/nlpcl-lab/CADD_dataset,Yes,Comprehensive Abusiveness Detection Dataset (CADD),1,1,Reddit,Abusiveness,English,N/A,N/A,N/A,N/A,"They sampled posts from Reddit, which is one of the largest online communities. Each post consists of a title and a body, together with a comment. They choose a comment as a target of abusive language detection, and a title and a body as contextual information. Even though the discussion across multiple comments also provides important contextual information, they just included a single top-level comment for each post to keep the uniformity of the length. In order to prevent the data from being skewed toward non-abusive comments, they sought to balance the numbers of abusive and non-abusive comments.",Yes,"Since all the possible privacy concerns of the data should be respected (Šuster et al., 2017; Benton et al., 2017), the dataset is fully anonymized and will be made available to researchers who are informed of, and agree to ethical guidelines.",manual,Yes,"They first asked annotators to determine whether each comment is abusive or not and extracted only those comments that were considered abusive. In the second step, they also had the comments annotated with labels through AMT with 6,000 participants, with the same restrictions as in Step 1.",Not discussed,,"30,000","17,154","4,902","They recruited 2,000 participants through AMT with Master status, whose IP addresses were restricted to those of English-speaking countries. ",Yes,"There are three levels of hierarchy: i) abusiveness, ii) target, and iii) demographic characteristics, where (i) through (iii) indicate the levels of the hierarchy (i ; ii ; iii).",Not discussed,,Specific,"gender, sexuality, race, religion, disability, age",Targeted,"race, religion, gender, age, sexuality",Yes,Yes,"Gender, Sexual orientation, Race, Religion, Disability, Age","gender, sexuality, race, religion, disability, age",No,,No,,,No,Human
238,No,"(Mubarak, Rashed, Darwish, Samih, & Abdelali, 2020)",Arabic Offensive Language on Twitter: Analysis and Experiments,arXiv preprint,"Qatar, Turkey","In this paper, they focus on building a large Arabic offensive tweet dataset. They introduce a method for building a dataset that is not biased by topic, dialect, or target. They produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. They thoroughly analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers use offensive language. Lastly, they conduct many experiments to produce strong results (F1 = 83.2) on the dataset using SOTA techniques.",Yes,96,09.03.2021,05.01.2023,Yes,Yes,"Offensive language. ""Offensiveness is often associated with undesirable behaviors such as trolling, cyberbullying, online extremism, political polarization, and propaganda.""",Yes,"Vulgar, hate speech",Yes,Website,,https://alt.qcri.org/resources/ OSACT2020-sharedTask-CodaLab-Train-Dev-Testzip,Yes,Dataset,1,1,Twitter,Offensiveness,Arabic,15.04.2019-06.05.2019,N/A,N/A,N/A,"Using Twitter APIs, they collected 660k Arabic tweets having this pattern between April 15 – May 6, 2019. To increase diversity, they sorted the word sequences between the vocative particles and took the most frequent 10,000 unique sequences. For each word sequence, they took a random tweet containing that sequence. ",Not discussed,,manual,Yes,"They annotated those tweets, ending up with 1,915 offensive tweets which represent roughly 19% of all tweets. Each tweet was labeled as: offensive, which could additionally be labeled as vulgar and/or hate speech, or Clean.",Not discussed,,"1,915",N/A,N/A,"An experienced annotator, who is a native Arabic speaker with good knowledge of various Arabic dialects.",Yes,"Tweets were given one or more of the following four labels: offensive, vulgar, hate speech, or clean. Since the offensive label covers both vulgar and hate speech and vulgarity and hate speech are not mutually exclusive, a tweet can be just offensive or offensive and vulgar and/or hate speech.",Not discussed,,General,N/A,Targeted,political,Yes,No,,,No,,No,,,No,Human
239,No,"(Sprugnoli, Menini, Tonelli, Oncini, & Piras, 2018)",Creating a WhatsApp Dataset to Study Pre-teen Cyberbullying,Proceedings of the 2nd Workshop on Abusive Language Online (ALW2) ,Italy,"They describe in this paper the activities that led to the creation of a WhatsApp dataset to study cyberbullying among Italian students aged 12- 13. They present not only the collected chats with annotations about user roles and types of offence but also the living lab created in a collaboration between researchers and schools to monitor and analyse cyberbullying. Finally, they discuss some open issues, dealing with ethical, operational and epistemic aspects.",Yes,51,31.10.2018,03.02.2022,Yes,Yes,"Cyberbullying. ""The notion of cyberbullying indicates each episode of online activity aimed at offending, menacing, harassing or stalking another person.""",Yes,"Defense, General Insult, Curse or Exclusion, Threat or Blackmail, Encouragement to the Harassment, Body Shame, Discrimination-Sexism, Attacking relatives, Other, Defamation",Yes,Github,,https://github.com/dhfbk/WhatsApp-Dataset,Yes,WhatsApp dataset,1,1,WhatsApp,Cyberbullying,Italian,N/A,N/A,Italy,N/A,"After receiving the necessary authorisation from the school director, the school board, and the teenagers’ parents, the researchers presented the experimentation to the participants, conceived as a role-play. In each class, two WhatsApp groups with around 10 teens were created. Teachers were part of the groups and could assist with the conversation, but they never actively participated in the chat. In each group, one researcher played for the whole time the role of the victim. Students were instead given the following roles: cyberbully (2 students), cyberbully assistants (3-4 students), and victim assistants (3-4 students). Teachers divided the classes and assigned the roles to pupils, so to take into account previous class dynamics and children's personalities. Each role-play lasted for 3 days, after which pupils changed roles within the same chat; students were allowed to participate in the chat only after the school hours, and could be excluded for a short time in case of misbehaviour. Teachers and researchers used dedicated mobile phones provided by the project, not their private ones. ",Yes,"WhatsApp groups are closed and not accessible from the outside, a preliminary agreement was signed involving stu- dents’ parents, teachers and headmasters to allow the activity. The threads were then saved in anony- mous form and manually annotated by two ex- pert linguists. The original names were not com- pletely removed, but they were replaced by ficti- tious names, so that it was still possible to track all the messages exchanged by the same person.",manual,Yes,"Following these guidelines, the annotator should identify all the harmful expressions in a conversation and, for each of them, he/she should annotate: (i) the cyberbullying role of the message’s author; (ii) the cyberbullying type of the expression; (iii) the presence of sarcasm in the expression; (iv) whether the expression containing insults is not really offensive but a joke. The guidelines identifies four cyberbullying roles: Harasser (person who initiates the harassment), Victim (person who is harassed), Bystander-defender (person who helps the victim and discourages the harasser), Bystander-assistant (person who takes part in the actions of the harasser). As for the type of cyberbullying expressions, we distinguish between different classes of insults, discrimination, sexual talk and aggres- sive statements: Threat or blackmail, General Insult, Body Shame, Sexism, Racism, Curse or Exclusion, Insult Attacking Relatives, Harmless Sexual Talk, Defamation, Sexual Harassment, Defense, Encouragement to the Harassment, and Other.",Not discussed,,"14,600",N/A,N/A,Two expert linguists,Yes,"Their guidelines are an adaptation to Italian of the “Guidelines for the Fine-Grained Analysis of Cyberbullying” developed for English by the Language and Translation Technology Team of Ghent University (Van Hee et al., 2015c). They added a new type of insult called Body Shame to cover expressions that criticize someone based on the shape, size, or appearance of his/her body. They have also changed the original type Encouragement to the Harasser into Encouragement to the Harassment, so to include all the incitements between the bully and his/her assistants.",Not discussed,,Specific,other,Non-targeted,N/A,No,No,,,No,,Yes,,To a person,No,Human
240,No,"(Shang et al., 2019)",VulnerCheck: A Content-Agnostic Detector for Online Hatred-Vulnerable Videos,2019 IEEE International Conference on Big Data (Big Data),USA,"This paper focuses on the problem of identifying online hatred-vulnerable videos where the videos themselves do not contain any hateful content but unexpectedly trigger hateful comments from the audience. In this paper, the authors develop VulnerCheck, an end-to-end supervised learning approach to effectively classify hatred-vulnerable videos from hateful and hatred-free ones by exploring the structure and semantics features of audience's comment networks. VulnerCheck is content-agnostic in the sense that it does not analyze the content of the video and is therefore robust against sophisticated content creators who craft hateful videos to bypass the current content censorship. The authors evaluate VulnerCheck on a real-world dataset collected from YouTube. Results demonstrate that this scheme is both effective and efficient in identifying hatred-vulnerable videos and significantly outperforms the state-of-the-art baselines.",Yes,7,09.12.2019,20.12.2022,Yes,Yes,"Online hatred-vulnerable videos where the content of the video is hate-free, but they unexpectedly trigger or attract hateful comments from users with a certain ideology, religion, or cultural background.",Yes,"Labels: Hatred-vulnerable, Hateful, Hatred-free",No,,,,,the real-world dataset,1,1,"YouTube, Gab",Hatred-vulnerable Videos,English,N/A,XX.07.2006 - XX.06.2019,N/A,N/A,"The authors start to collect video instances shared on a hate speech favorable platform, Gab. As an emerging social media platform that claims to promote free speech, Gab has been leveraged by users who were banned from mainstream social media to post and spread extreme and hateful content. They also observe that video posts on Gab rely heavily on YouTube as a video library and many of the videos have the URLs from YouTube. To evaluate the performance of VulnerCheck, they crawl a set of Gab posts containing YouTube video links and record each video by its unique video ID. In particular, they collect a set of 1082 videos for our experiments. The publishing dates of the collected videos range from July 2006 to June 2019. For each video, they collect the title, description, thumbnail, and comments information through YouTube API. For the baselines that analyze the video content, they also collect the video and its corresponding transcript using the command-line video downloader youtube-dl.",Not discussed,,manual,Yes,"The authors hire three independent human annotators to label the ground truth of each video by asking them to carefully watch the entire video clip and review the corresponding comments of the video. The annotators manually check the existence of both hateful content in the video clip and hateful comments in the comment section of the video before generating their labels. The ground truth labels are based on the majority vote of these human annotators. In addition, the authors translate all non-English comments to English using the Google Translation API.",Not discussed,,1082 videos,70% (~757),30% (~325),N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"political, religion, other",Yes,No,,,No,,No,,,Yes,Human
241,No,"(Rye et al., 2020)",Reading In-Between the Lines: An Analysis of Dissenter,Proceedings of the ACM Internet Measurement Conference,USA,"In this work, the authors obtain a history of Dissenter comments (Dissenter, a browser and web application that provides a conversational overlay for any web page), users, and the websites being discussed, from the initial release of Dissenter in Feb. 2019 through Apr. 2020 (14 months). The corpus consists of approximately 1.68M comments made by 101k users commenting on 588k distinct URLs. They first analyze macro characteristics of the network, including the user-base, comment distribution, and growth. They then use toxicity dictionaries, Perspective API, and a Natural Language Processing model to understand the nature of the comments and measure the propensity of particular websites and content to elicit hateful and offensive Dissenter comments. Using curated rankings of media bias, they examine the conditional probability of hateful comments given left and right-leaning content. Finally, they study Dissenter as a social network, and identify a core group of users with high comment toxicity.",Yes,10,27.10.2020,20.12.2022,Yes,Yes,"In a nutshell, toxicity is loosely defined as anti-social behavior that causes harm to a community at the social level. Things like harassment, hate speech, personal attacks, and trolling can all be considered toxic, as they reduce the inherent utility of the platform they occur on, as well as harm its underlying community of users.",Yes,"Perspective categories: LIKELY_TO_REJECT, OBSCENE, SEVERE_TOXICITY",No,,,,,Dissenter dataset; NY Times; Daily Mail; Reddit,4,2,"Dissenter, Gab, YouTube, NY Times, Daily Mail, Reddit",Toxicity,English,N/A,XX.02.2019 - XX.04.2020,N/A,N/A,"Registering for Dissenter requires an active Gab account. Therefore, Dissenter users are necessarily Gab users, and the authors leverage this fact in order to enumerate Dissenter users before beginning to crawl other Dissenter content. Each Dissenter user account is associated with a unique identifier. In a similar vein, Gab accounts also have a unique user identifier. Unlike Dissenter's author-ids, however, Gab user IDs do not encode creation time, but are instead a counter beginning at 1, the user ID associated with ""@e"", belonging to former Gab Chief Technology Officer (CTO) Ekrem Büyükkaya. Having created a test account for which the Gab ID is known, the authors query the Gab API endpoint https://gab.com/api/v1/accounts/&lt;GabIDfor IDs between 1 and their account's ID to retrieve JSON-encoded information pertaining to that user. Gab's API helpfully returns an error when an ID is not associated with a user account, and in this manner, they are able to exhaustively enumerate Gab's user base.

They then crawled Dissenter for URLs that users comment on, the comments they made about those pages, as well as replies to other users' comments. The crawler first visits the home page of each Dissenter user to capture their metadata, including username, display name, author-id, and biography. Then the crawler gathers the set of URLs the user has commented on. They then iterate over the set of commented-upon URLs. For each URL, the authors visit its comment page in Dissenter and collect the commenturl-id, the number of comments and number of up- and down-votes the URL has received, as well as the title and brief description. Each comment is available from the URL https://dissenter.com/ comment/&lt;CID;/, where &lt;CIDis the comment-id. They call this page the comment-page, as its purpose is to display a single comment as well as any replies. For reasons that are unclear, comment pages contain a JavaScript element with an unused (commentedout) JavaScript variable called ""commentAuthor"". commentAuthor defines an array with user data. While much of this embedded user data is identical to what is available to us via the user's home page, it also contains otherwise undiscoverable meta-data including the user's language setting, permissions, and view-filter preferences. We save these additional hidden meta-data as part of our per-user characterization.

To obtain the NSFW and ""offensive"" content, they re-spider Dissenter using the HTTP cookies of an authenticated account they created with NSFW and ""offensive"" content enabled separately, so that they are able to discern between content labeled NSFW by the submitter and comments marked as ""offensive"". These comments have no specific flag or other identifier present in the document body to indicate their presencetherefore, they infer NSFW and ""offensive"" comments as those found when authenticated with these flags enabled that were not previously discovered. In order to ensure they do not erroneously mislabel content as NSFW or ""offensive"" because of crawler errors, they monitor request timeouts and re-request missed pages, and ensure that subsequent runs only consider comments made during the initial spider's time frame. Additionally, they manually confirm 100 comments classified as NSFW or ""offensive"" by their subsequent crawls by attempting to view these comments both while authenticated with the NSFW and ""offensive"" view preferences enabled and while not authenticated.

In total, they obtain 1.68M comments and replies on 588k distinct URLs made by ;101k users.

They rely on the title and description provided by the Dissenter application in the comment page in order to gain valuable context about URLs being commented upon. However, Dissenter's own methodology for URL content appears unable to handle the most popular source of commented-upon URLs: YouTube videos. These pages generally appear with the title ""/watch"" and a null description, although the video itself is embedded in the page. Therefore, because YouTube content in particular represents a sizable percentage of the data (128k URLs) and because they seek to understand the content that generates comments, they also gather the content of the underlying web page for YouTube Dissenter comments. Because YouTube pages require JavaScript to render properly, they use Selenium to automate content retrieval. The data they seek (e.g., video title, uploader name) resides in large blocks of JavaScript, which may explain its absence from Dissenter. For each URL, they classify the content as one of three distinct types - ""video"", pages that contain a single YouTube video, ""user"", a home page for a particular YouTube user, and ""channel"", which is a collection of videos under a single banner.

Finally, they return to Gab in order to gain context about the social network that Dissenter users inhabit. Social relationships on Gab are directionalmuch like in Twitter, a user may become a follower of another user, and may accumulate followers themselves. While Dissenter users are able to ""follow"" other Dissenter users, none of the Dissenter browser, plugin, user home page, web application, or hidden meta-data reveal followers, or allow even an authenticated user to view his or her followers and following users. Presumably this is because the social network aspect of Dissenter is a subset of Gab, and is an as-yet unimplemented part of the Dissenter experience. Therefore, they use Gab followers as a proxy, and because Gab users are a strict superset of Dissenter users, any two Dissenter users can follow each other on Gab. Using the Gab API, they gather the followers and followed users of each Dissenter user. They note that Gab exposes its rate-limiting in the HTTP response headers by including the number of remaining requests, as well as the time at which the request limit will be refreshed. To minimize impact on the service, they issue at most one request per second, and monitor the number of remaining requests. If necessary, we wait until the number of available requests has been refreshed before continuing to issue new requests for Gab friends. Note that results from querying the Gab API for the social network are paginated, thus we can ensure that we gather the complete network graph. Finally, by removing non-Dissenter users from the followers and those followed obtained by querying Gab, they construct a Dissenterspecific social network graph.

Baseline Datasets: In addition to Dissenter comments, we construct three additional datasets: 1) NY Times, 2) Daily Mail, and 3) Reddit (summarized in Table 3). The NY Times and Daily Mail datasets are comments crawled from their respective sites, acquired from [43]\*. The Reddit dataset includes comments by accounts on Reddit that the authors believe are likely to controlled by a corresponding Dissenter user. They construct this dataset by querying Reddit for users matching their known Dissenter usernames. This revealed a large number of Reddit users: more than 56k Dissenter usernames (567) correspond to a registered Reddit account. While it is a near certainty that there are false positives in this construction, especially for particularly short usernames or usernames based on common words, previous work established a lower bound precision of 0.6 for this type of matching and found it sufficient to describe behavioral trends when studying user migration from Reddit. With these caveats in mind, for each of the 56k identified Reddit accounts, they query Pushshift for all of the comments they made on Reddit.

\*[43] Savvas Zannettou, Mai ElSherief, Elizabeth Belding, Shirin Nilizadeh, and Gianluca Stringhini. 2020. Measuring and Characterizing Hate Speech on News Websites. In WebSci.",Not discussed,,automated,Yes,"(1) Dictionary: The authors utilize the modified Hatebase dictionary of toxic terms. This dictionary contains 1,027 hate words. They tokenize each Dissenter comment and reply, perform stemming, and then count the number of tokens that match a term in the dictionary. Their per-comment hate dictionary score is then the ratio of hate words over the number of tokens in the comment.
(2) Perspective: Next, they leverage the Google Perspective API. The Perspective API provides several models that provide scores for different aspects of toxicity.
(3) NLP: Finally, they employ Natural Language Processing models to build a three-class (hate, offensive, or neither) comment classifier. To train the classifier, they use labeled data from [19] which contains 1,194 hate, 16,025 offensive, and 20,499 neither labels of Twitter tweets gathered via crowd-sourcing. Because of the imbalanced complexion of data, they use ADASYN to oversample. (\*[19] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated Hate Speech Detection and the Problem of Offensive Language. https: /aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15665)
To better understand the differences of each classifier as applied to Dissenter data, they evaluate all comments and replies with the three approaches and compare the resulting scores. They see that the classifiers largely agree - the distribution of Perspective toxicity for comments that score low with Hatebase or the NLP classifier is significantly skewed toward less toxicity, while those with high scores are skewed toward higher Perspective toxicity. Because of the general agreement between classifiers and their use of the classification as a relative metric, they use Perspective for the remainder of the evaluation.",Not discussed,,1.68M comments,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
242,No,"(Jiang et al., 2022)",SWSR: A Chinese dataset and lexicon for online sexism detection,Online Social Networks and Media,UK,"The authors propose the first Chinese sexism dataset – Sina Weibo Sexism Review (SWSR) dataset, as well as a large Chinese lexicon SexHateLex made of abusive and gender-related terms. They introduce their data collection and annotation process, and provide an exploratory analysis of the dataset characteristics to validate its quality and to show how sexism is manifested in Chinese. The SWSR dataset provides labels at different levels of granularity including (i) sexism or non-sexism, (ii) sexism category and (iii) target type, which can be exploited, among others, for building computational methods to identify and investigate finer-grained gender-related abusive language. The authors conduct experiments for the three sexism classification tasks making use of state-of-the-art machine learning models. The results show competitive performance, providing a benchmark for sexism detection in the Chinese language, as well as an error analysis highlighting open challenges needing more research in Chinese NLP.",Yes,4,01.01.2022,20.12.2022,Yes,Yes,"Misogyny is not always equivalent to sexism, and frequently implies the expression of hostility and hatred against women. As for sexism, Glick and Fiske define the concept of sexism referring to two forms of sexism: hostile sexism and benevolent sexism. Hostile sexism is characterised by an explicitly negative attitude towards women, while benevolent sexism is more subtle with seemingly positive characteristics (see examples in Figure I). Sexism includes a wide range of behaviours (such as stereotyping, ideological issues, sexual violence, etc.), and may be expressed in different ways: direct, indirect, descriptive or reported. Thus, misogyny is only one case of sexism.",Yes,Sexism Category: • Stereotype based on Appearance (SA) • Stereotype based on Cultural Background (SCB) • Microaggression (MA) • Sexual Offense (SO) Target Category: • Individual (I) • Generic (G),Yes,Other,Zenodo,http://doi.org/10.5281/zenodo.4773875,Yes,Sina Weibo Sexism Review (SWSR) dataset,1,1,Weibo,Sexism,Chinese,N/A,XX.06.2015 - XX.06.2020,N/A,N/A,"To construct the dataset, the authors use keyword-driven search to collect gender-related weibos from Sina Weibo platform (weibo.cn). In terms of relevance to the topic and through manual exploration, they firstly determine to use seven different keywords related to some hot topics and events of sexism for weibo data collection, namely 婊子(bitch), 女同性恋(lesbian), 女权(feminism), 厌女(misogyny), metoo运动(metoo movement), 性别歧视(gender discrimination) and 性骚扰(sexual harassment). Then they search and extract weibos containing these keywords. In addition, they retrieve user profiles, which include self-reported values such as gender and location, and other variables such as number of followers. To protect user privacy in the dataset, usernames are anonymised by replacing them with a special token &lt;username;. Then they combine these features into the weibo. The number of weibos collected for each keyword is listed in Table 2, which amounted to a total of 9,087 weibos collected for all keywords. Data collection was limited to posts made between June 2015 to June 2020.",Yes,"The dataset does not present any personally identifiable information, as the authors have anonymised all user names in the dataset, including any user names mentioned in the posts (replaced by the special token &lt;username;). The dataset does not include any private messages between users, and there was no interaction between Weibo users and researchers.",manual,Yes,"During the annotation process of our SWSR dataset, they perform three annotation tasks as follows:

1\. Sexism Identification: whether a text is sexist, as a binary annotation task determining if a comment is sexist (1) or non-sexist (0).

Where a comment is deemed sexist, they also perform two additional annotations:

2\. Sexism Category: stereotype based on appearance (SA), stereotype based on cultural background (SCB), microaggression (MA) and sexual offense (SO).

3\. Target Type: individual (I) or generic (G).

All three annotations were performed independently by three annotators, all of them PhD students, including two females and one male. The authors use the open 19 source text annotation tool doccand to facilitate the annotation work and to enable independent annotation effectively by three annotators. Guidelines were iteratively developed through collective annotation of a small sample of 100 comments by a broader set of five annotators. These annotators met and discussed disagreements between them, which led to revised guidelines. They report inter-annotator agreement rates for the three annotators by using Cohen's kappa as a metric.",Not discussed,,"8,969 comments (associated with 1,527 weibos)",90% (~8072),10% (~897),"Three annotators, all of them PhD students, including two females and one male",Yes,"Given the difficulty of identifying sexist behaviours, the authors carefully crafted guidelines for the three annotations tasks based on the insights from the annotation testing: sexism identification, sexism category and target category, along with examples of annotations by sexism category and target category.

Annotation I: Sexism Identification

A comment is considered sexist if it belongs to at least one of the following categories:

• explicitly attacks or insults gender groups or individuals using sexist language.

• incites gender-based violence or promote sexist hatred but not directly use a sexual abusive language.

• abuses those who attack or have negative attitudes towards a gender group.

• shows support of problematic incidents or intentions of sexual assault, sexual orientation and sexual harassment.

• negatively stereotypes gender groups by describing physical appeal, oversimplifying image or expressing superiority of men over women.

• expresses underlying gender bias in a sarcastic or tacit way.

The rest of the texts are considered non-sexist. This includes neutral descriptions or testimonies of sex-related events or phenomena.

Annotation II: Sexism Category

Each of the comments marked as sexist in the first task needs to be classified into one of the following, determining the sexism category of the comment:

• Stereotype based on Appearance (SA): describes physical appeal, oversimplifies image, or makes comparison with narrow /vulgar standards towards a gender group.

• Stereotype based on Cultural Background (SCB): expresses opinions indicating the superiority of men over women and emphasises gender inequality under the concept of a patriarchal society.

• Microaggression (MA): intentionally or unintentionally expresses hostile, derogatory or negative attitudes or remarks against gender groups or individuals.

• Sexual Offense (SO): incites sexual-related behaviour or attitude against women, such as sexual harassment, sexual assault, rape and violence.

4.2.3. Annotation III: Target Category

Each of the comments marked as sexist in the first task needs to have the type of target identified, which can be one of the following two:

• Individual (I): a post with sexist content addressing a specific person.

• Generic (G): a post with sexist content addressing a broader group (such as a gender-based group of people.",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Gender/Sex (Sexism),gender,No,,Yes,a post with sexist content addressing a specific person,To a person,No,Human
243,No,"(Kumar & Sachdeva, 2022)",Multi-input integrative learning using deep neural networks and transfer learning for cyberbullying detection in real-time code-mix data,Multimedia Systems,India,"This research focuses on cyberbullying detection in the code-mix data, specifically the Hinglish, which refers to the juxtaposition of words from the Hindi and English languages. The authors explore the problem of cyberbullying prediction and propose MIIL-DNN, a multi-input integrative learning model based on deep neural networks. MIIL-DNN combines information from three sub-networks to detect and classify bully content in real-time code-mix data. It takes three inputs, namely English language features, Hindi language features (transliterated Hindi converted to the Hindi language) and typographic features, which are learned separately using sub-networks (capsule network for English, bi-LSTM for Hindi and MLP for typographic). These are then combined into one unified representation to be used as the input for a final regression output with linear activation. The advantage of using this model-level multi-lingual fusion is that it operates with the unique distribution of each input type without increasing the dimensionality of the input space. The robustness of the technique is validated on two datasets created by scraping data from the popular social networking sites, namely Twitter and Facebook. Experimental evaluation reveals that MIIL-DNN achieves superlative performance in terms of AUC-ROC curve on both the datasets.",Yes,27,13.07.2022,20.12.2022,Yes,Yes,"Cyberbullying refers to bullying a person or a group by sending inappropriate messages by means of electronic communication. The target of cyberbullying is to harass, humiliate or to harm the reputation of cyber victim and to create infinite social dilemmas. Cyberbullying is not confined to any particular country or religion and has spread its roots all over the world.",No,,No,,,,,Dataset 1 (DS-1) [Face book];  Dataset 2 (DSII) [Twitter]; (existing dataset) Toxic Comment Classification Challenge dataset from a Kaggle competition,3,2,"Facebook, Twitter, Wikipedia",Cyberbullying,Hindi-English,N/A,N/A,N/A,N/A,"Two datasets were created by scraping data from the popular social networking sites namely, Twitter and Facebook. Data was based on the selection of certain hashtags and keywords from the domain of politics, public figures, entertainment, etc. and was restricted to code-mixed 'Hinglish (Hindi + English)' language. The posts fetched from Facebook were ""profile-based"". The most popularly searched profiles of Sh. Narendra Modi ji (Prime Minister of India), Mr. Shahrukh Khan (actor), public profiles of NDTV (news channel) and Jawaharlal Nehru University (university in India) were observed for the data analysis. GraphAPI was used for the extraction of Facebook comments. For Twitter, ""topic-based"" tweets were scraped that belonged to the most trending topics such as ""#Ind VS Pak, #Beaf Ban, #movies"". 'Tweepy tool was used for the extraction of tweets from Twitter. Also, the posts that were solely written in English or Hindi or code-mix of English and Hindi were removed using manual filtering. Finally, two datasets with 6500 (English-Hindi) code-mixed posts each, for both Facebook, Dataset 1 (DS-I) and Twitter, Dataset 2 (DS-2) were created.",Not discussed,,manual,Not discussed,,Not discussed,,6500 (DS-I); 6500 (DS-II);,10-fold cross-validation,10-fold cross-validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
244,No,"(Haddad et al., 2019)",T-HSAB: A Tunisian Hate Speech and Abusive Dataset,Arabic Language Processing: From Theory to Practice,"Tunisia, Turkey","Being an underrepresented dialect, no previous Abusive or Hate speech datasets were provided for the Tunisian dialect. In this paper, the authors introduce the first publicly-available Tunisian Hate and Abusive speech (T-HSAB) dataset with the objective to be a benchmark dataset for automatic detection of online Tunisian toxic contents. They provide a detailed review of the data collection steps and how they design the annotation guidelines such that a reliable dataset annotation is guaranteed. This was later emphasized through the comprehensive evaluation of the annotations as the annotation agreement metrics of Cohen’s Kappa (k) and Krippendorff’s alpha (α) indicated the consistency of the annotations.",Yes,29,02.10.2019,20.12.2022,Yes,Yes,"In the literature, there has been no clear distinction between Abusive speech (AS) and Hate speech (HS). … defined HS as ""any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic"". Although HS can be conducted as a subtask of the abusive language detection, it remains challenging since it requires to consider the correlation between the abusive language and the potential groups that are usually targeted by HS.",Yes,"3 classes: Normal, Abusive, Hate",Yes,Github,,https://github.com/Hala-Mulki/T-HSAB-A-Tunisian-Hate-Speech-and-Abusive-Dataset,Yes,T-HSAB (A Tunisian Hate Speech and Abusive Dataset),1,1,different social media platforms,Abusiveness,Tunisian,N/A,XX.10.2018 - XX.03.2019,N/A,N/A,"The proposed dataset was constructed out of Tunisian comments harvested from different social media platforms. The authors collected the dataset comments based on multiple queries, each of which represents a potential entity that is usually attacked by abusive/hate speech. Among the used queries, they can mention: (Jews) (Africans) (gender equality in inheritance). To harvest the query-related comments, the collection process focused on the comments posted within the time period: October 2018-March 2019. Initially, they retrieved 12,990 comments; after filtering out the non-Arabic, non-textual, AD-containing and duplicated instances, they ended up with 6,075 comments, written in the Tunisian dialect. In order to prepare the collected comments for annotation, they were normalized through eliminating platform-inherited symbols such as Rt, @ and #, Emoji icons, digits, in addition to non-Arabic characters found in URLs and user mentions.",Not discussed,,manual,Yes,"The annotation task was assigned to three annotators, two males and one female. All of them are Tunisian native speakers and at a higher education level (Master/PhD). Besides the previous annotation guidelines, and based on the domain and context of the proposed dataset, they provided the annotators with the nicknames usually used to refer to certain minorities and ethnic groups. For instance, within an insulting context, Sub-Saharan African ethnic groups are usually referred to using these nicknames:  (slaves), (black),(nig\*\*a) and  (of a dark skin). Having all the annotation rules setup, they asked the three annotators to label the 6,075 comments as Normal, Abusive or Hate. For the whole dataset, they received a total of 18,225 judgments. When exploring these annotations, they faced three cases:

1\. Unanimous agreement: the three annotators annotated a comment with the same label. This was encountered in 4,941 comments.

2\. Majority agreement: two out of three annotators agreed on a label of a comment. This was encountered in 1,098 comments.

3\. Conflicts: each rater annotated a comment differently. They were found in 36 comments.

After excluding the comments having 3 different judgments, the final released version of T-HSAB is composed of 6,039 comments.",Not discussed,,"6,039 comments","4,831 (80%)","1,208 (82%)","Three annotators, two males and one female. All of them are Tunisian native speakers and at a higher education level (Master/PhD).",Yes,"The authors designed the annotation guidelines such that all the annotators would have the same perspective about HS. The annotation instructions defined the 3 label categories as:

• Normal comments are those instances which have no offensive, aggressive, insulting and profanity content.

• Abusive comments are those instances which combine offensive, aggressive, insulting or profanity content.

• Hate comments are those instances that: (a) contain an abusive language, (b) dedicate the offensive, insulting, aggressive speech towards a person or a specific group of people and (c) demean or dehumanize that person or that group of people based on their descriptive identity (race, gender, religion, disability, skin color, belief).",Not discussed,,General,N/A,Targeted,"race, gender, sexuality, nationality, religion, other",Yes,No,,,No,,No,,,No,Human
245,No,"(Grimminger & Klinger, 2021)",Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",Germany,"The 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. The authors investigate if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. They publish a corpus of 3000 tweets that are annotated both for stance and hate speech detection. Based on a manual analysis of these annotations, the results suggest that Tweets that express a stance against Biden contain more hate speech than those against Trump. The baseline classification experiments show that the detection of the stance that somebody is in-favor of a candidate performs better than that somebody is against a candidate. Further, the detection of hate/offensive speech on this corpus remains challenging.",Yes,27,19.04.2021,20.12.2022,Yes,Yes,"Defined as ""any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic"" (Nockelby, 2000), hate speech is considered ""a particular form of offensive language"" (Warner and Hirschberg, 2012). However, some authors also conflate hateful and offensive speech and define hate speech as explicitly or implicitly degrading a person or group.",Yes,"stance towards the targets Trump, Biden, and West: Favor, Against, Neither, Mixed, Neutral",Yes,Website,,https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/stance-hof/,Yes,Twitter-corpus of hateful/offensive speech detection and stance detection,1,1,Twitter,Hate Speech,English,N/A,"6 weeks leading to the presidential election, on the election day and for 1 week after the (US President 2020) election",N/A,2020 US Elections,"The authors used the Twitter API v 1.1. to fetch tweets for 6 weeks leading to the presidential election, on the election day and for 1 week after the election. As search terms, we use the mention of the presidential and vice presidential candidates and the outsider Westthe mention of hashtags that show a voter's alignment such as the campaign slogans of the candidate websites, and further nicknames of the candidates. The list of search terms is: #Trump2020, #TrumpPence 2020, #Biden2020, #BidenHarris2020, #Kanye2020, #MAGA2020, #BattleForTheSoulfTheNation, #2020Vision, #VoteRed2020, # VoteBlue2020, Trump, Pence, Biden, Harris, Kanye, President, Sleepy Joe, Slow Joe, Phony Kamala, Monster Kamala.

After removing duplicate tweets, the final corpus consists of 382.210 tweets. From these, there are 220.941 that contain Trump related hashtags and mentions, 230.629 tweets that carry hashtags and mentions associated with Biden and 1.412 tweets with hashtags and mentions related to Kanye West.",Not discussed,,manual,Yes,"Given the text of a tweet, annotators rated the stance towards the targets Trump, Biden, and West in the text. The detected stance can be from one of the following labels:

• Favor: Text argues in favor of the target

• Against: Text argues against the target

• Neither: Target is not mentionedneither implicitly nor explicitly

• Mixed: Text mentions positive as well as negative aspects about the target

• Neutral: Text states facts or recites quotesunclear, whether text holds any position towards the target.

The default value shown in the annotation environment is Neither.

The text was further annotated as being hateful and non-hateful. They did not separate if a group or a single person was targeted by hateful language. Further, they adapted the guidelines on hate speech annotation to be able to react to name-calling and down talking of the political opponent. Thus, they rated expressions such as ""Dementia Joe"" and ""DonTheCon"" as hateful/offensive (HOF).

To evaluate the annotation guidelines (which we make available together with the data) we perform multiple annotation iterations with three annotators. Annotator 1 and 2 annotated 300 tweets in three iterations with 100 tweets per iteration. After each iteration, the annotators discussed the tweets they rated differently and complemented the existing guidelines. Finally, Annotator 2 and 3 annotated 100 tweets with the improved guidelines to check whether the rules are clear and understandable, especially if read for the first time. The final annotation of the overall data set has been performed by Annotator 2.",Yes,"From the 382.210 tweets, they sampled 3000 tweets for annotation.",3000 tweets,80% (~2400),20% (~600),"Three annotators. Annotator 1 is a 22 year old male undergraduate student of computational linguistics who speaks German, English, Catalan, and Spanish. Annotator 2 is a 26 year old female undergraduate student of computational linguistics who speaks German and English. Annotator 3 is a 29 year old female graduate student of computational linguistics who speaks German and English.",Yes,"The annotators were provided with the guidelines established during the iterations between Annotator 1 and 2. The comprehensive guideline is available together with the data. It contains the description of the tasks, the definitions of each category, and examples.",Not discussed,,Specific,political,Targeted,"race, gender, sexuality, nationality, religion, other",Yes,No,,,No,,Yes,"presidential election: Trump, Biden, Kanye West","About a person,To a person",No,Human
246,No,"(Nguyen et al., 2021)",Constructive and Toxic Speech Detection for Open-Domain Social Media Comments in Vietnamese,Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices,Vietnam,"In this paper, the authors create a dataset for constructive and toxic speech detection, named UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset) with 10,000 human-annotated comments. For these tasks, they propose a system for constructive and toxic speech detection with the state-of-the-art transfer learning model in Vietnamese NLP as PhoBERT. With this system, they obtain F1-scores of 78.59% and 59.40% for classifying constructive and toxic comments, respectively. Besides, they implement various baseline models as traditional Machine Learning and Deep Neural Network-Based models to evaluate the dataset. With the results, they can solve several tasks on the online discussions and develop the framework for identifying constructiveness and toxicity of Vietnamese social media comments automatically.",Yes,15,19.07.2021,20.12.2022,Yes,Yes," Toxic: Comments whose contents are sarcasm and criticismhaving a mockery attitudedisagreeing with the opinion but with a lack of delicacy and impolite.

 Quite toxic: Comments whose contents might be harmful to people (but not everyone) in specific contexts express disappointment.

 Non-toxic: Comments which have non-constructive content only express pure emotions and not make much sense.

 Constructive: Comments of users reinforce their point of view for the article. Frequently, those comments provide lots of information and particularized opinions and contribute to promoting the topic.

 Non-constructive: Comments of users which have little information. Their contents are only about expressing simple emotions and do not have much meaning.",Yes,constructive: Constructive/Non-constructive toxic speech: Very toxic / Toxic / Quite toxic / Non-toxic,Yes,Website,,https://sites.google.com/uit.edu.vn/  Please contact us via email: 17520721@gm.uit.edu.vn (Mr. Luan Nguyen) to sign the corpus user agreement and then receive the corpus.,Yes,UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset),1,1,VnExpress.net,"Toxicity, Constructiveness",Vietnamese,N/A,N/A,N/A,N/A,"Before collecting data, the authors do several surveys about our social media topic, which have many comments relevant to the interests. With the obtained results, they decide to crawl data from comments of users in VnExpress.net because there are huge comments that are useful for our research. Then, they build a tool for crawling data using Python and BeautifulSoup4 library useful functions for crawling data. The dataset consists of 10,000 comments and is divided equally into ten domains, including entertainment, education, science, business, cars, law, health, world, sports, and news (1,000 comments each domain).",Not discussed,,manual,Yes,"Three annotators label each comment in the dataset.

The main task in this paper is constructive speech detection of Vietnamese comments. To understand the study, they explain constructive (as label 1) or nonconstructive (as label 0) labels and their examples. Following the guidelines, annotators annotate comments with one of the two labels.

 Constructive: Comments of users reinforce their point of view for the article. Frequently, those comments provide lots of information and particularized opinions and contribute to promoting the topic.

 Non-constructive: Comments of users which have little information. Their contents are only about expressing simple emotions and do not have much meaning.

Furthermore, they also concentrate on another task of toxic speech detection. With this characteristic, they annotate comments with one of four different labels consisting of very toxic, toxic, quite toxic, or non-toxic as follows:

 Very toxic: Comments which have offensive contentsdirectly attacking individuals and organizations, showing disrespect to others. Especially using offensive words often causes shutting down conversation rapidly.

 Toxic: Comments whose contents are sarcasm and criticismhaving a mockery attitudedisagreeing with the opinion but with a lack of delicacy and impolite.

 Quite toxic: Comments whose contents might be harmful to people (but not everyone) in specific contexts express disappointment.

 Non-toxic: Comments which have non-constructive content only express pure emotions and not make much sense.",Not discussed,,10000 comments,70% (~7000),10% (~1000),N/A,Yes,"Before labeling data, the authors built an annotation scheme with detailed and necessary information, which helps annotators label data quickly and precisely. We describe each task with a detailed definition. Several examples, as well as easily confused comments, are also listed in the annotation scheme.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
247,No,"(Park & Kim, 2021)","Tweeting about abusive comments and misogyny in South Korea following the suicide of Sulli, a female K-pop star: Social and semantic network analyses",Profesional de la información,South Korea,"This study examined the development of the public discussion on Twitter about the abusive comments specific to misogynistic discourse after the suicide of Sulli, a female celebrity in South Korea. Both the pattern of social networking between the users and the semantic representations of user responses were analyzed from a social network perspective using a large-scale Twitter dataset. A total of 37,101 tweets generated by 25,258 users were collected and analyzed. The findings of the network analysis suggest that hubs and authorities on Twitter were closely connected to each other and contributed to promoting the public discussion about abusive comments in response to her death. The results of the semantic network analysis suggested that her death, presumably due in part to continuous hateful comments from trolls, evoked an open discussion about the deeply rooted abusive comments and misogyny that are prevalent in South Korea. Users perceived that sensational news coverage about celebrities and unethical journalistic practices led to abusive comments and her death. The users shared their observations that gendered hate speech contributed to Sulli’s bullying. Dominant words that referred to Sulli’s sexual harassment show the ways in which haters had bullied her, as well as the criticism of online harassment. The results imply that the issue of online misogyny was closely associated with abusive comments in the public consciousness. This study verified the role of celebrities in increasing awareness about social issues and word-of-mouth dissemination even after a death. This study also offers methodological insights by demonstrating how social network analysis can be used to analyze public discussion using big data.",Yes,3,09.09.2021,20.12.2022,Yes,Yes,"Misogyny is generally defined as hatred or contempt for women (MoloneyLove, 2018) and, according to Ging and Siapera (2018), in recent decades has morphed into abusive comments posted by commenters.

…“celebrity bashing”…online aggression targeting celebrities that is conducted by journalists and their audience.",No,,No,,,,,Twitter dataset,1,1,Twitter,"Misogyny, Celebrity Bashing",Korean,14.10.2019 - 01.11.2019,14.10.2019 - 01.11.2019,N/A,Sulli's suicide,"To explore both social networking patterns and topical trends in a discussion about abusive comments in response to Korean idol Sulli's death on Twitter, all tweets written in Korean, including the term ""설리"" (""Sulli""), were collected from October 14, 2019, when Sulli died, to November 1, 2019. Considering that individuals mostly react to celebrities' death on Twitter within two weeks after the death of celebrities (Park; Hoffner, 2020), this data collection period is appropriate to capture public responses to her death. The tweets were extracted using an application programming interface (API)-based social media analytics tool NodeXL (Smith et al., 2010). A total of 37,101 tweets generated by 25,258 users were collected. The unique number of tweets was 3,890, including 2,069 tweets (5.57%), 1,272 mentions (3.42%), and 468 replies to others (1.26%). The majority of the tweets were retweets of others' tweets (33,292; 89.73%).",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,gender,Targeted,gender,No,No,,,No,,Yes,"Sulli, a female K-pop star",About a person,No,Human
248,No,"(Akhtar et al., 2019)",A New Measure of Polarization in the Annotation of Hate Speech,AI\*IA 2019 – Advances in Artificial Intelligence,Italy,"Current work on the automatic detection of various forms of hate speech (HS) typically employs supervised learning, requiring manually annotated data. The highly polarizing nature of the topics involved raises concerns about the quality of annotations these systems rely on, because not all the annotators are equally sensitive to different kinds of hate speech. The authors propose an approach to leverage the fine-grained knowledge expressed by individual annotators, before their subjectivity is averaged out by the gold standard creation process. This helps researchers to refine the quality of training sets for hate speech detection. They introduce a measure of polarization at the level of single instances in the data to manipulate the training set and reduce the impact of most polarizing text on the learning process.",Yes,20,12.11.2019,20.12.2022,Yes,Yes,Hate speech (HS) is a form of abusive language directed at specific targets and inciting hatred and violent actions.,Yes,"binary classification for racism, sexism, and homophobic, separately",No,,,,,"Sexism, Racism, Homophobia",3,3,Twitter,"Hate Speech, Sexism, Racism, Homophobia, Polarization","English, Italian",N/A,N/A,N/A,N/A,"The authors gathered a dataset of hate speech in social media. The corpus is borrowed from previous studies by a previous study on HS detection in the English language. The original dataset was composed of 6,909 tweets annotated with racism and sexism. The dataset is available on a Github repository', where only the Twitter IDs and the labels are provided. Querying Twitter to retrieve the messages by using the IDs resulted in the collection of a smaller dataset consisting of 6,361 tweets, due to the perishability of the data on the online platform. They also employ an additional set of tweets in Italian, to test the application of our method in a multilingual perspective. The Italian dataset comprises 1,859 tweets on topics related to the LGBT community. The homophobia dataset was annotated by volunteers.",Not discussed,,manual,Yes,"Sexism

The dataset from [30] contains tweets annotated according to four categories: sexism, racism, both, and neither, in a multi-label fashion. The authors isolated the sexism and racism classes to focus on them individually with two binary classification tasks. In other words, they converted the labels sexism and both to sexist, and the labels racism and neither to non-sexist.

Racism

They extracted a binary labeled Racism dataset from the data of [30] following the same procedure they applied to derive the Sexism dataset. The annotation scheme remains the same as with the original dataset. The only difference is the mapping of the original labels: racism and both are mapped to racist, while sexism and neither are mapped to non-racist. In the resulting Racism dataset, 100 tweets out of 6,361 (1.57%) are marked racist.

Homophobia

They exploited a dataset from the ACCEPT project on the monitoring of homophobic hate online. The data consist of tweets selected with a number of LGBT +related keywords and annotated by five volunteers contacted by the largest Italian LGBT+ non-for-profit organization (Arcigay) selected along different demographic dimensions such as age, education and personal view on LGBT stances. The original dataset is labeled in a multi-class fashion according to four categories: homophobic, not homophobic, doubtful or neutral. They map nothomophobic, doubtful and neutral to not homophobic and leave the label homophobic unchanged, to restrict the problem definition to a binary classification task.",Not discussed,,"6,361 (Sexism dataset); 6,361 (Sexism dataset); 1,859 (Homophobia dataset)",80%,80%,"Homophobia dataset: five volunteers contacted by the largest Italian LGBT+ non-for-profit organization (Arcigay) selected along different demographic dimensions such as age, education and personal view on LGBT stances.",Not discussed,,Not discussed,,Specific,"race, gender, sexuality",Non-targeted,N/A,Yes,Yes,"Race, Gender/Sex, Sexuality","race, gender, sexuality",No,,No,,,No,Human
249,No,"(Nizzoli et al., 2019)",Extremist Propaganda Tweet Classification with Deep Learning in Realistic Scenarios,Proceedings of the 10th ACM Conference on Web Science,Italy,"In this work, the authors tackled the problem of the automatic classification of the extremist propaganda on Twitter, focusing on the Islamic State of Iraq and al-Sham (ISIS). They built and published several datasets, obtained by mixing 15,684 ISIS propaganda tweets with a variable number of neutral tweets, related to ISIS, and random ones, accounting for imbalances up to 1%. They considered three state-of-the-art, deep learning techniques, representative of the main current approaches to text classification, and two strong linear machine learning baselines. They compared their performance when varying the composition of the training and test sets, in order to explore different training strategies, and to evaluate the results when approaching realistic conditions. They demonstrated that a Recurrent-Convolutional Neural Network, based on pre-trained word embeddings, can reach an excellent F1 score of 0.9 on the most challenging test condition (1%-imbalance).",Yes,18,26.06.2019,20.12.2022,Yes,Yes,"Extremist Propaganda. ""In this work, we focused on the Islamic State of Iraq
and al-Sham (ISIS). We addressed the needs of a potential stakeholder who leverages the Twitter Streaming API [4] to obtain a
mixture of ISIS propaganda tweets (pro-ISIS), tweets reporting ISIS
related content but not supporting the organization (about-ISIS),
and completely random ones, with unknown relative proportion.""",Yes,"pro-ISIS, about-ISIS, random",Yes,Website,,"https://ci.iit.cnr.it/ept  For the login credentials, please e-mail leonardo.nizzoli@iit.cnr.it.",Yes,pro-ISIS English tweets; about-ISIS tweets; random tweets,3,3,Twitter,Extremist Propaganda,English,N/A,N/A,N/A,N/A,"The authors built several datasets by combining pro-ISIS, about-ISIS and random tweets, with different proportions. The 15,684 pro-ISIS Engish tweets were sampled from a large, reliable, publicly available dataset published by Fifth Tribe, a digital agency providing services to US government. They sampled the about-ISIS tweets from a dataset including tweets containing at least one ISIS related keyword. They collected random tweets via the Twitter Streaming API, by means of the GET statuses/sample method, which returns a small random sample of all public statuses. The largest and most imbalanced (1%) dataset included, for each pro-ISIS tweet, 6 about-ISIS and 93 random tweets. Less imbalanced datasets (from 2% to 10%, and balanced) were derived from more imbalanced ones, by removing negative instances. Each dataset underwent stratified splitting to create training (64%), validation (16%) and test sets (20%).",Not discussed,,N/A,Not discussed,,Not discussed,,"15,684 pro-ISIS tweets; ?  about-ISIS tweets; ? random tweets",64% (~10038),20% (~3137),N/A,Not discussed,,Not discussed,,Specific,political,Targeted,political,Yes,No,,,Yes,pro-ISIS,No,,,No,Human
251,No,"(Elisabeth et al., 2020)",Hate Code Detection in Indonesian Tweets using Machine Learning Approach: A Dataset and Preliminary Study,2020 8th International Conference on Information and Communication Technology (ICoICT),Indonesia,"This paper presents an implementation of hate code detection for Indonesian tweets using machine learning and a classification explainer. First, the authors developed a dataset for hate codes ground truth. They generated hate codes from two scenarios i.e., hate code from hate speech classification and hate code from hate code classification. They used Logistic Regression (LR), Naive Bayes (NB), and Random Forest Decision Tree (RFDT) as the classifier. They also used TF-IDF and word bigrams as the features. The codes consist of word and phrase form. The best f-measure score is 94.90% from hate code classification using Logistic Regression with abusive codes elimination. This number means the model can detect all tweets that have no hate codes. For tweets that annotated have hate code, the f-measure is 28.23% for recognized all the hate codes, and the recall is 56.91%.",Yes,5,24.06.2020,14.10.2022,Yes,Yes,"Hate speech: …hate speech is an act that intentionally and without the right to disseminate information intended to incite hatred and hostility of individuals or specific groups of people based on ethnicity, religion, race, and group.

Hate code: Hate speech targets can be hidden by giving a nickname or code to an individual or group. The use of nicknames or codes is intended to avoid harmful content detection. The nickname or code we will later refer to as hate codes. Magu and Lao define hate speech that uses hate codes as euphemistic hate speech.",Yes,hate / not-hateful;  with hate code / without hate code,Yes,Github,,https://github.com/delisabeths/Dataset-for-Hate-Code-Detection-in-Indonesian-Tweets,No,Dataset-for-Hate-Code-Detection-in-Indonesian-Tweets,1,1,Twitter,"Hate code, Hate speech",Indonesian,N/A,N/A,N/A,N/A,"(The initial dataset were collected in previous research by other authors. )

The authors in the previous work collected Twitter dataset from other datasets and  crawled Twitter dataset using words/phrases which frequently used by Indonesian netizens when spreading abusive language and hate speech in their social media. Their crawling process was done using Twitter APP that implemented using Tweepy Library.",Not discussed,,manual,Yes,"The authors used hate speech label from the initial dataset.

In addition, they did hate code annotation for the dataset. They built annotation guidelines first to help the annotators for understanding their task. In order to get a valid annotation guideline, they did a discussion and consultation with sociolinguistics expert. Each tweets was annotated by three annotators, and the final label was decided using words majority voting strategy.",Not discussed,,13169,10-fold cross-validation,10-fold cross-validation,N/A,Yes,"The authors built annotation guidelines to help the annotators for understanding their task (for hate code annotation).

The guidelines are available at: https://github.com/delisabeths/Dataset-for-Hate-Code-Detection-in-Indonesian-Tweets/blob/master/%5BBahasa%5DGuideline_Hate%20Code%20Detection%20in%20Indonesian%20Tweets%20Annotation.pdf",Not discussed,,General,N/A,Targeted,"race, religion",Yes,No,,,No,,No,,,No,Human
252,No,"(Liao et al., 2016)",A hybrid epidemic model for deindividuation and antinormative behavior in online social networks,Social Network Analysis and Mining,USA,"With the increasing popularity of user-contributed sites, the phenomenon of “social pollution”, the presence of abusive posts has become increasingly prevalent. In this paper, the authors describe a novel approach to investigate negative behavior dynamics in online social networks as epidemic phenomena. They show that using hybrid automata, it is possible to explain the contagion of antinormative behavior in certain online commentaries. They present two variations of a finite-state machine model for time-varying epidemic dynamics, namely triggered state transition and iterative local regression, which differ with respect to accuracy and complexity. They validate the model with experiments over a dataset of 400,000 comments on 800 YouTube videos, classified by genre, and indicate how different epidemic patterns of behavior can be tied to specific interaction patterns among users.",Yes,9,19.03.2016,14.10.2022,Yes,Yes,"In the psychology literature, the phenomenon we describe is referred to as deindividuation, a state of decreased self-evaluation and decreased evaluation apprehension causing antinormative and disinhibited behavior",Yes,"Antinormative: positive, neutral, or negative;  Video type: Music, Comedy, Entertainment, Unlabeled, News/politics, Others",No,,,,,the YouTube dataset,1,1,YouTube,"Deindividuation, Antinormative Behavior, Abusiveness",English,N/A,N/A,N/A,N/A,"The dataset in this work includes 800 randomly selected YouTube videos from a database of 67,000 videos, each with approximately 500 comments for a total of about 400,000 comments (Siersdorfer et al. 2010). Each video is accompanied by detailed information including ID, title, data, tags, rating, and number of comments. For each comment, information about its content, sender id and rating is included. The authors crawled the corresponding standardized classification scheme provided by YouTube to identify each video's genre, i.e., politics, science, technology, sports, entertainment. The comment rating score is an integer value calculated as +1 for each upvote a comment receives and -1 for each downvote.",Not discussed,,"automated, manual",Yes,"The authors bin each comment into one of three classes, i.e., positive, neutral, or negative.

A. Comment rating score (automated). Users can vote with ""thumbs up"" (+1) or ""thumbs down"" (-1) on individual comments on YouTube. The total rating of a comment is considered as an indicator of the general sentiment of other users toward the comment and likewise an indicator of the comment's representativeness of group opinion. In general, a comment with positive rating score is considered positive while a negative rating score comment is labeled negative.

To tackle unrated comments and comments with neutral comment rating score, they also include language and sentiment analysis.

B. Jargon and language (automated). For each comment not definitively up-rated or down-rated by users, they extract individual words from the text and compare them against a dictionary of known abusive words (List of swear 2016) to determine the ratio of abusive words which appear. If the result is above a selected threshold (e.g., 20 % for their tests), they label the comment's language to be negative; otherwise, it is considered neutral.

C. Label validation (manual). They validate this labeling approach by asking two independent labelers to hand-label 10 consecutive comments on 100 randomly selected videos from the dataset containing negative, offensive, or abusive content of any kind. Comments were binned into ""negative"" and ""non-negative"" classes, with final label assignment being considered ""negative"" if both labelers described it as such and ""non- negative"" otherwise.",Yes,"For manual label validation, the authors chose 10 consecutive comments on 100 randomly selected videos from the dataset containing negative, offensive, or abusive content of any kind.","~400,000 comments",30% (~120000),N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
254,No,"(Warner & Hirschberg, 2012)",Detecting hate speech on the world wide web,Proceedings of the Second Workshop on Language in Social Media,USA,"The authors present an approach to detecting hate speech in online text, where hate speech is defined as abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation. While hate speech against any group may exhibit some common characteristics, they have observed that hatred against each different group is typically characterized by the use of a small set of high frequency stereotypical words; however, such words may be used in either a positive or a negative sense, making the task similar to that of words sense disambiguation. In this paper the authors describe their definition of hate speech, the collection and annotation of their hate speech corpus, and a mechanism for detecting some commonly used methods of evading common ""dirty word"" filters. They describe pilot classification experiments in which they classify anti-semitic speech reaching an accuracy 94%, precision of 68% and recall at 60%, for an F1 measure of. 6375.",Yes,658,07.06.2012,14.10.2022,Yes,Yes,"Hate speech is a particular form of offensive language that makes use of stereotypes to express an ideology of hate.

Merely mentioning, or even praising, an organization associated with hate crimes does not by itself constitute hate speech.

An author's excessive pride in his own race or group doesn't constitute hate speech. A disparagement of others is required to satisfy the definition.

Unnecessary labeling of an individual as belonging to a group often should be categorized as hate speech.

If the identity of the speaker cannot be ascertained, and if no orthographic or other contextual cues are present, such terms are categorized as hateful. (Sometimes such “hateful"" words are used by a speaker who belongs to the targeted group, which should not be classified as hate speech. )",Yes,"anti-semitic, anti-black, anti-asian, anti-woman, anti-muslim, anti-immigrant, or other-hate",No,,,,,"Majority corpus, Gold corpus",2,2,"Yahoo!, a list of websites (Attenberg’s URLs) provided by American Jewish Congress (AJC)",Hate Speech,English,N/A,N/A,N/A,N/A,"Through the partnership with the American Jewish Congress, the authors received a list of 452 URLs previously obtained from Josh Attenberg which were originally collected to classify websites that advertisers might find unsuitable.

They downloaded the webdata, and captured paragraphs that matched a general regular expression of words relating to Judaism and Israel (jewish|jewIzionist|holocaust|denier|rabbi|israelsemitic|semite). This resulted in about 9,000 paragraphs. Of those, they rejected those that did not contain a complete sentence, contained more than two unicode characters in a row, were only one word long or longer than 64 words.",Not discussed,,manual,Yes,"The authors identified seven categories to which labelers would assign each paragraph. Annotators could label a paragraph as anti-semitic, anti-black, anti-asian, anti-woman, anti-muslim, anti-immigrant or other-hate. These categories were designed for annotation along the anti-semitic/not anti-semitic axis, with the identification of other stereotypes capturing mutual information between anti-semitism and other hate speech.

The authors created a simple interface to allow labelers to assign one or more of the seven labels to each paragraph. They instructed the labelers to lump together South Asia, Southeast Asia, China and the rest of Asia into the category of anti-asian. The anti-immigrant category was used to label xenophobic speech in Europe and the United States. Other-hate was most often used for anti-gay and anti-white speech, whose frequency did not warrant categories of their own.

The authors examined interlabeler agreement only for the anti-semitic vs. other distinction.

They created two corpora from this same set of 1000 paragraphs. First, the majority corpus was generated from the three labeled sets by selecting the label with on which the majority agreed. One of the authors checked and corrected some apparent “errors” in annotator labeling to create a gold corpus. As a way of gauging the performance of human annotators, the authors compared two of the annotators' labels to the gold corpus by treating the annotators' labeled paragraphs as input to a two fold cross validation of the classifier constructed from the gold corpus. This sets an upper bound on the performance the authors expect from a classifier.",Not discussed,,1000 paragraphs,10-fold cross-validation,10-fold cross-validation,N/A,Yes,"The authors instructed the labelers to lump together South Asia, Southeast Asia, China and the rest of Asia into the category of anti-asian. The anti-immigrant category was used to label xenophobic speech in Europe and the United States. Other-hate was most often used for anti-gay and anti-white speech, whose frequency did not warrant categories of their own.",Not discussed,,Specific,"race, gender, religion, nationality",Non-targeted,N/A,No,Yes,"Ethnicity, Race, Gender, Religion, Nationality","race, gender, religion, nationality",Yes,"anti-semitic, anti-black, anti-asian, anti-woman, anti-muslim, anti-immigrant",No,,,No,Human
255,No,"(Zampieri et al., 2019)",Predicting the Type and Target of Offensive Posts in Social Media,"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","UK, USA, India","In this work, the authors target several different kinds of offensive content for the task of identifying potentially offensive messages. In particular, they model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, they complied and published the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme. They discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.",Yes,489,02.06.2019,14.10.2022,Yes,Yes,"Offensive (OFF): Posts containing any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct. This includes insults, threats, and posts containing profane language or swear words.",Yes,"Categorization of Offensive Language: Targeted Insult (TIN), Untargeted (UNT);  Offensive Language Target Identification: Individual (IND), Group (GRP), Other (OTH)",Yes,Other,CodaLab,https://scholar.harvard.edu/malmasi/olid,Yes,Offensive Language Identification Dataset - OLID,1,1,Twitter,Offensiveness,English,N/A,N/A,N/A,N/A,"Keywords: medical marijuana, they are, to:New Yorker, you are, she is, to:BreitBartNews, he is, gun control, -filter:safe, conservatives, antifa, MAGA, liberals

The authors retrieved the examples in OLID from Twitter using its API and searching for keywords and constructions that are often included in offensive messages.

They first carried out a round of trial annotation of 300 instances with six experts using nine keywords (medical marijuana, they are, to:New Yorker, you are, she is, to:BreitBartNews, he is, gun control, -filter:safe). The trial keywords that they ultimately decided to exclude due to low percentage of offensive tweets are medical marijuana, they are, to:New Yorker. They computed Fleiss' kappa on the trial dataset for the five annotators on 21 of the tweets.

The authors then sampled our full dataset, so that 50% of the tweets come from political keywords, and the other 50% come from non-political keywords. Within these two groups, tweets were evenly sampled for the keywords. In addition to 'gun control', and 'to:BreitbartNews' used during the trial annotation, four new political keywords were used to collect tweets for the full dataset: 'MAGA', 'antifa', ""conservatives, and 'liberals.

The keywords for the full dataset: you are, she is, to:BreitBartNews, he is, gun control, -filter:safe, conservatives, antifa, MAGA, liberals.",Yes,"As to normalization and anonymization, the authors did not store any user meta- data or Twitter IDs, and they substituted the URLs and the Twitter mentions by placeholders.",manual,Yes,"A. Trial annotation

They first carried out a round of trial annotation of 300 instances with six experts using nine keywords. They computed Fleiss’ kappa on the trial dataset for the five annotators on 21 of the tweets.

B. Full annotation

The data were annotated using crowdsourcing. The authors used Figure Eight and ensured data quality by (i) only hiring annotators who were experienced in the platform, and (ii) using test questions to discard annotations by individuals who did not reach a certain threshold (with the gold standard from the trial annotation).

They first acquired two annotations for each instance. In the case of disagreement, they requested a third annotation, and they then took a majority vote. The annotators were asked to label each tweet at all three levels of the annotation scheme, and they considered there to be agreement only when the annotators agreed on the labels for all levels. Each instance in the dataset was annotated by multiple annotators and inter-annotator agreement was calculated at the end.",Yes,"The authors modified the keyword list for data collection using trial annotation, to ensure a certain percentage of offensive tweets are present in the collected dataset.",14100,13240,860,Annotators are hired only if they were experienced in the platform. Their annotations were used only if they reach a certain threshold in the test questions.,Yes,"Level A: Offensive language Detection

Level A discriminates between the following types of tweets:

• Not Offensive (NOT): Posts that do not contain offense or profanity;

• Offensive (OFF): Posts containing any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct. This includes insults, threats, and posts containing profane language or swear words.

2.2 Level B: Categorization of Offensive Language

Level B categorizes the type of offense:

• Targeted Insult (TIN): Posts containing insult/threat to an individual, a group, or others;

• Untargeted (UNT): Posts containing non-targeted profanity and swearing. Posts with general profanity are not targeted, but they contain non-acceptable language.

2.3 Level C: Offensive Language Target Identification

Level C categorizes the targets of insults/threats:

• Individual (IND): Posts targeting an individual. This can be a famous person, a named individual or an unnamed participant in the conversation. Insults and threats targeted at individuals are often defined as cyberbulling.

• Group (GRP): Posts targeting a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or other common characteristic. Many of the insults and threats targeted at a group correspond to what is commonly understood as hate speech.

• Other (OTH) The target of these offensive posts does not belong to any of the previous two categories (e.g., an organization, a situation, an event, or an issue).",Not discussed,,Specific,"race, gender, sexuality, political, religion, other",Non-targeted,N/A,Yes,Yes,"Ethnicity, Gender, Sexual orientation, Political affiliation, Religious belief, Other","race, gender, sexuality, political, religion, other",No,,No,,To a person,No,Human
256,No,"(Nagar et al., 2021)",Empirical assessment and characterization of homophily in classes of hate speeches,CEUR Workshop Proceedings,India,"In this paper, the authors investigate homophily in hate speech generation on social media platforms. They note that similarity among the users can be defined along with multiple aspects like: profile meta-data, the content generated and style of writing. These derived features are capable of capturing similarity along multiple dimensions, primarily semantic, lexical, syntactical, stylometric and topical. They leverage the important features for authorship attribution, word embeddings, latent and empath topics to compute lexical, syntactical, stylometric, semantic and topical features. They empirically demonstrate the presence of homophily on a dataset from Twitter along with the different aspects of similarity. Further, they investigate how homophily varies with different hateful types such as hate manifesting in topics of gender, race, ethnicity, politics and nationalism. The results indicate higher homophily in users associating with topics of racism and nationalism.",Yes,2,09.02.2021,14.10.2022,Yes,Yes,"Hate Speech. ""…We further investigate the strength of homophilic phenomenon for different types of hate such as,
hate against gender, race, politics and ethnicity.""",No,,No,,,,,The dataset (adapted from the dataset provided by Ribeiro et al.),1,1,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"The authors use the dataset provided by (Ribeiro et al. 2017). This dataset contains 200 most recent tweets of 100, 386 users, totaling to 19M tweets. It also contains a retweet induced graph of the users. It has 2, 286, 592 directed edges. The dataset does not have labels for the tweet content. Therefore the authors of this paper manually annotate the tweets as hateful or not.",Not discussed,,manual,Not discussed,,Yes,"The authors pick only a sub-set of the users whose tweets they manually annotate. They run modularity optimization-based community detection using networkx to pick a sub-set of users on the retweet network. The two communities picked have an equal number of edges around 1, 60, 000 while the users are 7, 679 and 3, 277 respectively. These two communities have a sufficient number of users (from the perspective of the number of tweets to label) and edge density varies significantly between the two.",N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"gender, race, political, nationality, sexuality",Yes,No,,,No,,No,,,No,Human
258,No,"(Halevy et al., 2021)",Mitigating Racial Biases in Toxic Language Detection with an Equity-Based Ensemble Framework,"Equity and Access in Algorithms, Mechanisms, and Optimization",USA,"Recent research has demonstrated how racial biases against users who write African American English exists in popular toxic language datasets. While previous work has focused on a single fairness criteria, this work proposes to use additional descriptive fairness metrics to better understand the source of these biases. The authors demonstrate that different benchmark classifiers, as well as two in-process bias-remediation techniques, propagate racial biases even in a larger corpus. They then propose a novel ensemble-framework that uses a specialized classifier that is fine-tuned to the African American English dialect. They show that the proposed framework substantially reduces the racial biases that the model learns from these datasets. They demonstrate how the ensemble framework improves fairness metrics across all sample datasets with minimal impact on the classification performance, and provide empirical evidence for its ability to unlearn the annotation biases towards authors who use African American English.",Yes,8,05.10.2021,14.10.2022,Yes,Yes,Racial biases in toxic language detection,No,,Yes,Github,,"https://github.com/matanhalevy/DebiasingHateDetectionAAE  The code is available, including the code to aggregate the datasets that was used in this work.",Yes,"Toxic Aggregate Dataset (new); Hate Aggregate Dataset (new);

Existing datasets: DWMW17; FDCL18; Golbeck; WH16",6,2,Twitter,"Racial Biases in Toxic Language Detection, Hate Speech",English,N/A,N/A,N/A,N/A,"The aggregated Toxic dataset included DWIMW17, FDCL18, Golbeck et. al's dataset and the positive samples from WH16.

The Hate dataset is aggregated on the hate labels from DWMW17 and FDCL18 and only the subset of positive instances of WH16 that were labeled as racist or sexist.",Not discussed,,N/A,Not discussed,,Not discussed,,Toxic Aggregate Dataset: 134457;  Hate Aggregate Dataset: 123886.,80% (~107566; ~99109),10% (~13446; ~12389),N/A,Not discussed,,Not discussed,,Specific,race,Targeted,race,Yes,Yes,Race,race,Yes,African American,No,,,No,Human
260,No,"(Caselli et al., 2021)",DALC: the Dutch Abusive Language Corpus,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),Netherlands,"This work introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for abusive language. The resource addresses a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.",Yes,4,06.08.2021,14.10.2022,Yes,Yes,"Abusive language is defined as: impolite, harsh, or hurtful language (that may contain profanities or vulgar language) that result in a debasement, harassment, threat, or aggression of an individual or a (social) group, but not necessarily of an entity, an institution, an organization, or a concept.",Yes,"Explicitness layer: Explicit (EXP), Implicit (IMP), Not abusive (NOT) Target layer: Individual (IND), Group (GRP), Other (OTH).",Yes,Github,,https://github.com/tommasoc80/DALC,Yes,DALC - Dutch Abusive Language Corpus,1,1,"Twitter, Reddit",Abusiveness,Dutch,N/A,(for a subset)  1 12.11.2015 - 22.11.2015 (November 2015 Paris attacks) 2 07.03.2017 - 17.03.2017 (2017 Dutch general election) 3 12.11.2018 - 22.11.2018 (Intocht Sinterklaas 2018) 4 2020-08 (protests in solidarity with the Black Lives Matter movement) 5 2015-04 6 2018-06 7 2019-05 8 2019-09 (for a subset) 12.11.2018 - 22.11.2018,the Netherlands (for a subset),(for a subset) November 2015 Paris attacks; 2017 Dutch general election; Intocht Sinterklaas 2018; protests in solidarity with the Black Lives Matter movement.,"The corpus is composed by tweets in Dutch extracted using different strategies and covering different time windows.

- Keywords: the first method is based on van Rosendaal et al. (2020), where keyword collection is refined via cross-fertilization between two social media platforms, namely Reddit and Twitter. Controversial posts from the subreddit r/thenetherlands, the biggest Reddit community in Dutch, at specific time periods are scraped, and a list of unigram keywords is extracted using TF-IDF. The top 50 unigrams are used as search terms in the corresponding time period in Twitter. The authors identified 8 different time periods between 2015 and 2020. They include both periods of time that may contain ""historically significant events"" (e.g., the Paris Attack in November 2015the Dutch General Election in March 2017the Sinterklaas intocht in December 20218the Black Lives Matter protests after the killing of George Floyd in August 2020) and random time periods where no major events occurred (e.g., April 2015June 2018May and September 2019). This results in a total of 12,884,560 retrieved tweets. From each time period, they have generated samples of 10k messages composed as follows: 5k messages are randomly sampled, while the remaining 5k (non-overlapping) messages are extracted using two Dutch lexicon of potentially offensive/hateful terms, namely HADES and HurtLex v1.2.
    
- Geolocation: the second method is inspired by previous work showing that in the Western areas of the (north hemisphere of the) world hatred messages tend to be more frequent in geographical areas that are economically depressed and where disenfranchised communities live. They use data from the Dutch Central Bureau voor de Statistiek (CBS) about unemployment to proxy such communities in the Netherlands, identifying two provinces: Zuid-Holland and Groningen. They develop a set of heuristics, including the use of city names in these two provinces, to randomly collect messages from these areas. This is needed since the geolocation of the users is optional and does not have a fixed format. They managed to successfully extract 356,401 messages that can be reliably assigned to one of the two provinces. A sample of 5k messages is extracted using the lexicons and an additional 5k randomly.
    
- Authors: The last method uses seed users. The authors manually compile an ad-hoc list of 67 profanities, swearwords, and slurs by extending the lexicons. They then search for messages containing any of these elements in a ten-day window in December 2018 (namely 2018-11-12-2018-11-22. This results in a total of 3,105,833 messages. They rank each users according to the number of messages containing at least one of the target words. They select the top 50 users as seed users. They then extract for each of the selected user a maximum of 100 messages in a different time period, namely between May and June 2020, for a total of 5k tweets.
;",Yes,"The authors have made publicly available only the tweet IDs. This will protect the users' rights to delete their messages or accounts.  They also make available another version of the corpus, DALC Full Text. This version of the corpus allows users to access to the full text message of all 8,156 tweets. The DALC Full Text dataset is released with a BY-NC 4.0 licence. In this case, they make available only the text, removing any information related to the time periods or seed users. They have also anonymized all users' mentions and external URLs. The CC licence is extended with further restrictions explicitly preventing users to actively search for the text of the messages in any form.",manual,Yes,"During Phase 1 (March–May 2020), the authors validate the annotation guidelines by means of a pairwise inter-annotator agreement (IAA) study on two independent subsets of 100 messages each. The first sample is obtained using the keyword method and the second using the geolocation. Cases of disagreement have been discussed between the an- notators and resolved. The data used for the IAA has been integrated in DALC v1.0. No IAA has been computed for the messages collected using seed authors. In phase 2 they further expanded the initial data annotation. In phase 2 (November–December 2020) they further expanded the initial data annotation. The final corpus has been manually curated by one of the authors of this paper.",Yes,"Keywords: they randomly extracted batches of 500 messages for each time period.
Geolocation: Four batches of 500 instances each have been manually annotated.
Authors: They directly created batches of 500 messages each for the manual annotation.
All messages sampled for the manual annotation do not contain retweets.","8,156 tweets",5706,1901,"Phase 1: five annotators, all bachelor students in Information Science. The students conducted the annotation of the data as part of their bachelor thesis project.

Phase 2: conducted by one master student in Information Science with previous experience in this task.

• Annotator #1: Age: 21Gender: femaleRace/ethnicity: caucasianNative language: DutchSocioeconomic status:n/a Training in linguistics/other relevant discipline: BA in Information science

• Annotator #2: Age: 21Gender: maleRace/ethnicity: caucasianNative language: DutchSocioeconomic status:n/a Training in linguistics/other relevant discipline: BA in Information science

• Annotator #3: Age: 21Gender: maleRace/ethnicity: caucasianNative language: DutchSocioeconomic status:n/a Training in linguistics/other relevant discipline: BA in Information science

• Annotator #4: Age: 21Gender: maleRace/ethnicity: caucasianNative language: DutchSocioeconomic status:n/a Training in linguistics/other relevant discipline: BA in Information science

• Annotator #5: Age: 23Gender: maleRace/ethnicity: caucasianNative language: DutchSocioeconomic status:n/a Training in linguistics/other relevant discipline: BA in Information science

• Annotator #6: Age: 24Gender: maleRace/ethnicity: caucasianNative language: DutchSocioeconomic status:n/a Training in linguistics/other relevant discipline: MA in Information science",Yes,DALC v1.0 has been manually annotated using internally developed guidelines. The guidelines provides the annotators with a definition of abusive language that refines proposals in previous work.,Not discussed,,Specific,"race, gender, political, religion, disability, other",Non-targeted,N/A,Yes,Yes,"Ethnicity, Gender, Political affiliation, Religion, Disabilities, Other","race, gender, political, religion, disabilities, other",No,,No,,To a person,No,Human
261,No,"(Elbasani & Kim, 2022)",AMR-CNN: Abstract Meaning Representation with Convolution Neural Network for Toxic Content Detection,Journal of Web Engineering,South Korea,"Compared to commonly developed toxic content detection systems that use lexicon and keyword-based detection, this work tries to embrace a different approach by the meaning of the sentence. Meaning representation is a way to grasp the meaning of linguistic input. This work proposed a data-driven approach utilizing Abstract meaning Representation to extract the meaning of the online text content into a convolutional neural network to detect level profanity. This work implements the proposed model in two kinds of datasets from the Offensive Language Identification Dataset and other datasets from the Offensive Hate dataset merged with the Twitter Sentiment Analysis dataset. The results indicate that the proposed model performs effectively, and can achieve a satisfactory accuracy in recognizing the level of online text content toxicity.",Yes,0,22.02.2022,14.10.2022,Yes,Yes,"Since the start this work already defined a profane sentence as a sentence that includes offensive word or hate speech.

The ‘Offensive’ and ‘Hate’ data were considered as ‘Toxic’ and the tweets that neither contain hate or offensive words was labelled as ‘Non-Toxic’.",No,,No,,,,,"DTSD (merged dataset of Offensive tweet from Davidson et al. and data from Twitter Sentiment Analysis)

existing dataset: OLID (Offensive Language Identification Dataset)",2,1,Twitter,Toxicity,English,N/A,N/A,N/A,N/A,"Dataset 2 (DTSD) was a merged dataset of Offensive tweet from Davidson et al. and data from Twitter Sentiment Analysis. Both source datasets have an imbalance data distribution. With merged both data, the distribution of positive/negative classes is roughly balanced. (25595 tweets vs 31150 tweets)",Not discussed,,N/A,Not discussed,,,,DTSD: 56745 OLID: 13240.,5-fold cross-validation,5-fold cross-validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
263,No,"(Lees et al., 2021)",Capturing Covertly Toxic Speech via Crowdsourcing,Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing,USA,"The authors study the task of labeling covert or veiled toxicity in online conversations. Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions. Their investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing. They introduce an initial dataset, COVERTTOXICITY, which aims to identify and categorize such comments from a refined rater template. Finally, they fine-tune a comment-domain BERT model to classify covertly offensive comments and compare against existing baselines.",Yes,8,20.04.2021,14.10.2022,Yes,Yes,Toxic speech—comments likely to make someone leave a conversation. Covert toxicity is an umbrella term which includes types of toxicity that may not be immediately obvious.,Yes,"covert toxicity types: Microaggression, Obfuscation, Emoticons/Emojis, Sarcasm/Humor, Masked Harm",Yes,Other,TensorFlow,https://www.tensorflow.org/datasets/catalog/civil_comments,Yes,"COVERTTOXICITY dataset (new);

SBIC Microaggressions Dataset;",2,1,Civil Comments,Covertly Toxic Speech,English,N/A,N/A,N/A,N/A,"(The CivilComments dataset is a publicly available corpus of ~1.8 million crowd rated comments labeled for toxicity. The CivilCommentsIdentities dataset is a subset of ∼400K comments, additionally rated for specific identity terms, such as gender, religion, or sexual orientation. The CivilCommentsIdentities toxicity label is the fraction of raters who voted for the label. )

For the COVERTTOXICITY dataset, the authors applied the methodology of (Han and Tsvetkov, 2020) to the CivilCommentsIdentities set: comments with at least one identity attack annotation(s) and low Perspective API toxicity scores (0 &ltP(toxicity) &lt0.4) were marked as candidates for covert toxicity.",Not discussed,,manual,Yes,"The authors iterated on versions of an instruction templates to assist raters in identifying covertly toxic language with high precision.

• Ask crowdworkers to reflect and put themselves in the shoes of participant

• Encourage crowdworkers to critically think about the task prior to engagement.

Two priming questions were added in the final template, which significantly improved rater performance.

• Overtly Offensive Rule: Overt offenses occur when text has some words that are clearly toxic and requires no hidden meaning interpretation. | Can you think of a word/phrase that is clearly spelled or mis-spelled using toxic vocabulary, or is threatening?

• Covertly Offensive Rule: Covert offenses occur when text has some words that might have hidden meanings or relevance to a community. | Can you think of a community or members of a community that might be targeted by use of words/phrases or might find text derogatory without explicitly calling out on them?",Yes,Comments with at least one identity attack annotation(s) and low Perspective API toxicity scores (0 &lt; P(toxicity) &lt; 0.4) were marked as candidates for covert toxicity.,~50000,~48000,~2000,N/A,Yes,"Instructions for identifying implicitly (covertly) toxic comments versus explicit toxicity are available to the annotators and the screenshot of the instructions can be seen in Fig 1A.

The instructions include the priming questions and the classification rules for each label with Examples and Reasons.",Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
264,No,"(Bhowmick et al., 2021)",A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person,ACM Transactions on Asian and Low-Resource Language Information Processing,India,"By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-tuned state-of-the-art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual.",Yes,5,02.11.2021,14.10.2022,Yes,Yes,"A derogatory notion about a recognized individual (or person) of the country…far-ranging social, political, financial influence of intentionally publishing material that has malicious intent against an individual.",Yes,"four sub-categories: Hate Speech, Targeted insult, Profane Language, and Sentiment analysis.",No,,,,,"synthesized Facebook-Meme dataset;

existing datasets: Twitter sentence dataset (Go et al.), Twitter sentiment analysis dataset (Howard and Ruder), HOSOC FIRE 2019",4,1,Facebook,Derogatory Content,Hindi-English,N/A,N/A,N/A,N/A,"A new synthesized Facebook-Meme dataset has been manually created using the Facebook-ID of the corresponding post (URL of the post).

The in-house Facebook-Meme dataset consists of 650 Facebook meme posts. The authors consider Memes that contain both the meme images and meme text. As they are headed to classify the derogatory social media posts, the majority of the considered Memes refer to famous personalities from different fields: cricketers (Virat Kohli, MS Dhoni, etc.), politicians (Mahatma Gandhi, Narendra Modi, Rahul Gandhi, etc.), and poets and writers (Rabindranath Tagore, etc.). These posts are manually labeled by their names. Apart from that the other considered Memes where the face cannot be identified manually, the authors labeled them as unknown. After selecting the Facebook posts, the main text and comments related to the posts are taken into account. The main texts and the comments are available in English, Hindi, and mixed languages. The generated dataset takes into account more than 2,500 comments.",Not discussed,,manual,Yes,"For the classification task, the comments, main texts, and meme texts are analyzed and manually labeled into four sub-categories: Hate Speech, Targeted insult, Profane Language, and Sentiment analysis. All four aforementioned categories are sub-classified into two categories: Yes and No.",Not discussed,,~650 meme posts + 2500 comments,5-fold cross-validation,5-fold cross-validation,N/A,Not discussed,,Not discussed,,Specific,"political, other",Non-targeted,N/A,Yes,No,,,No,,Yes,"famous personalities from different fields: cricketers (Virat Kohli, MS Dhoni, etc.), politicians (Mahatma Gandhi, Narendra Modi, Rahul Gandhi, etc.), and poets and writers (Rabindranath Tagore, etc.)",About a person,No,Human
265,No,"(Rosenthal et al., 2021)",SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"USA, Denmark, UK, Qatar","The previous dataset, OLID, for offensive language identification is limited in size and it might be biased towards offensive language as it was collected using keywords. In this work, the authors present SOLID, an expanded dataset, where the tweets were collected in a more principled manner. SOLID contains over nine million English tweets labeled in a semi-supervised fashion. They demonstrate that using SOLID along with OLID yields sizable performance gains on the OLID test set for two different models, especially for the lower levels of the taxonomy.",Yes,19,01.08.2021,14.10.2022,Yes,Yes,"OFF(ENSIVE):  Inappropriate language, insults, or threats.",Yes,"Categorization of Offensive Language: Targeted Insult (TIN), Untargeted (UNT) Offensive Language Target Identification: Individual (IND), Group (GRP), Other (OTH)",Yes,Website,,https://sites.google.com/site/offensevalsharedtask/solid,Yes,"Semi-Supervised Offensive Language Identification Datatset (SOLID);

existing dataset: Offensive Language Identification Dataset (OLID);",2,1,Twitter,Offensiveness,English,XX.XX.2019-XX.XX.2019,XX.XX.2019-XX.XX.2019,N/A,N/A,"The authors collected the data from Twitter using the Twitter streaming API via Twython in 2019. They search the API using the 20 most common English stopwords (e.g. the, of, and, to) to ensure truly random tweets and avoid rate limits. Using stopwords ensures that they are more likely to obtain English tweets as well as a diverse set of random tweets. They kept the stream running the entire time and continuously choose a stopword at random based on its frequency in Project Gutenberg, a sizeable monolingual corpus. They collected 1,000 tweets for each stopword. Thus, tweets, including more frequent stopwords, are collected more frequently.

They used the langdetect tool to select English tweets and discarded tweets with less than 18 characters or less than two words. In total, we collected over 12 million tweets.",Yes,They substituted all user mentions with @USER for anonymization purposes.,"automated, manual",Yes,"A. Semi-Supervised Training Dataset

In this work, the authors employ democratic co-training to create semi-supervised labels for all three levels of SOLID using OLID as the seed dataset. Distant supervision is conducted by the ensemble of models with different inductive biases. They first trained each model on the OLID dataset using 10% of the training dataset for validation. They compute the aggregated single prediction based on the average and the standard deviation of the confidences predicted by each of the models. In particular, they compute the scores based on the confidences for the positive class at Levels A and B and the confidences for the IND, GRP, and OTH classes at Level C. They performed the above aggregation step instead of just using the scores of each model to avoid over-fitting to any particular model in the ensemble.

The dataset is labeled using the semi-supervised manner by assigning a Level A label to all the tweets. Then, they select the subset of tweets that are likely to be offensive for all models (BERT and LSTM ≥ 5, PMI and FT=OFF) as instances that should be assigned a label for Level B. They chose the tweets likely to be TIN at Level B with a standard deviation lower than 0.25 for Level C. Thus, only the instances that are most likely to be offensive are considered at Levels B and C, and only those that are most likely to be offensive and targeted are considered at Level C.

B. SOLID Test Dataset

The authors also annotated a portion of the held-out 3 million tweets to create a new SOLID test set. First, all co-authors of this paper (five annotators) annotated 48 tweets that were predicted to be OFF in order to measure inter-annotator agreement. (A tweet may address targets of different types, e.g., both an individual and a group, but only one label can be chosen.)

After having observed high IAA, they annotated additional offensive tweets with a single annotation per instance. They divided our Level A data into four portions (easy OFF, hard OFF, easy NOT, hard NOT) based on model confidence. (They selected the rest of the thresholds after a manual examination of the confidence scores for each model. They chose the threshold where the model is confident and mostly correct.)

They annotated 3,493 tweets for Level A. Furthermore, to create a complete test dataset for Level A (where we only labeled offensive tweets), they also took a random set of 2,500 Easy NOT tweets. The resulting test sizes are shown in Table 2. Of the 3,493 annotated tweets, 491 were determined to be NOT. In total, there are 5,993 tweets in the test set. In all cases, all three levels were annotated, but the decision of whether a tweet in Level B/C is Easy or Hard is still based on its Level A confidence.",Yes,"They select the subset of tweets that are likely to be offensive for all models (BERT and LSTM ≥ 5, PMI and FT=OFF) as instances that should be assigned a label for Level B. They chose the tweets likely to be TIN at Level B with a standard deviation lower than 0.25 for Level C. Thus, only the instances that are most likely to be offensive are considered at Levels B and C, and only those that are most likely to be offensive and targeted are considered at Level C.",(SOLID) ~9 million Semi-Supervised Training Dataset; 5993 tweets in Test Dataset,(SOLID) ~9 million,(SOLID) ~3 million; 5993 tweets in Test Dataset,The authors,Not discussed,,Not discussed,,Specific,"race, gender, sexuality, religion",Non-targeted,N/A,Yes,Yes,,"race, gender, sexuality, religion",No,,No,,To a person,No,Human
266,No,"(Rani et al., 2020)",A Comparative Study of Different State-of-the-Art Hate Speech Detection Methods in Hindi-English Code-Mixed Data,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",Ireland,"In an environment where multilingual speakers switch among multiple languages, hate speech detection becomes a challenging task using methods that are designed for monolingual corpora. In this work, the authors attempt to analyze, detect and provide a comparative study of hate speech in a code-mixed social media text. They also provide a Hindi-English code-mixed data set consisting of Facebook and Twitter posts and comments. The experiments show that deep learning models trained on this code-mixed corpus perform better.",Yes,30,11.05.2020,14.10.2022,Yes,Yes,"Hate speech is a direct or indirect statement targeted towards a person or group of people intended to demean and brutalize another or use derogatory language on the basis of ethnicity, religion, disability, gender or sexual orientation (Schmidt and Wiegand, 2017).",No,,Yes,Github,,https://github.com/deepanshu1995/HateSpeech-Hindi-English-Code-Mixed-Social-Media-Text,Yes,"Dataset-1 (Bohra et al., 2018);

Dataset-2 (HASOC FIre 2019);

Dataset-3 (annotated by the authors)",3,1,"Twitter, Facebook",Hate Speech,Hindi-English,N/A,N/A,N/A,N/A,"(1) The first data set was collected from Github! Data set-1 consist of 4575 Hindi-English code-mixed annotated tweets in the Roman script only. Tweets were extracted from twitter using the Twitter API. In order to remove the noise from the data set, rigorous preprocessing was carried out, which resulted in the removal of URLs and punctuation, replacing user names and emoticons.
(2) Data set-2 was taken from a Shared Task called HASOC, which was organised at FIRE 2019. It consists of 4665 annotated posts partially collected from Twitter and Facebook. The collection was done with the help of the Twitter API using specific hashtags and keywords which helped in crawling an unbiased data set.
(3) In addition to Data set-1 and Data set-2 the authors created a third data set (Data set-3) which has also been used for an aggression detection task (Kumar et al., 2018). This unannotated data set contains 3367 posts and tweets which were annotated by the authors of this paper. The data for the current corpus was crawled from Facebook and Twitter. The data was collected using some of the popular hashtags around such contentious themes as a beef ban, India vs Pakistan cricket matches, election results, opinions on movies, etc., i.e., topics that are typically discussed among Indians and may give rise to hate speech.",Not discussed,,manual,Yes,"The annotation was completed by six annotators: three male and three female in three different phases. In order to make the annotation process more accessible and user-friendly, 33 Google forms were made which contained the necessary annotator information, annotation scheme and 100 posts in each Google form. In the very first phase, 500 posts were annotated by all the six annotators. An inter-annotator agreement was calculated before the completion of the first annotation phase, after which changes in the annotation guidelines were made since the inter-annotator agreement score was below par for hate speech detection. The second phase of the annotation was conducted with another set of 500 posts/tweets.

While calculating the inter-annotator agreement after the second round of annotation, the authors found that one of the annotators had difficulty understanding social media language while another annotator was unable to finish the annotation taskconsequently, the inter-annotator agreement was very poor. Therefore both annotators were eliminated, which resulted in a much higher agreement score compared to the previous score.

After completion of the second round of annotation, a preliminary experiment was done to train the system, followed by a third phase of annotation, conducted on the rest of the tweets.",Not discussed,,4575 (Dataset-1); 4665 (Dataset-2); 3367 (Dataset-3),70% (~3203; ~3266; ~2357),20% (~915; ~933; ~673),"Six annotators, three male and three female in three different phases. Two of these annotators were excluded in the third phase.",Yes,"The guidelines include the rules and examples for the binary classification: Hate and Not Hate.

The guidelines are available at: https://www.dropbox.com/s/lydv9tt7kh4k01b/Hate%20speech%20annotation%20guide%20line.pdf?dl=0",Not discussed,,General,N/A,Targeted,"race, religion, disability, gender, sexuality",Yes,No,,,No,,No,,,No,Human
267,No,"(Dinkov et al., 2019)",Detecting Toxicity in News Articles: Application to Bulgarian,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Bulgaria, Qatar","The authors propose and develop a news toxicity detector that can recognize various types of toxic content. While previous research primarily focused on English, here they target Bulgarian. They created a new dataset by crawling a website that for five years has been collecting Bulgarian news articles that were manually categorized into eight toxicity groups. Then they trained a multi-class classifier with nine categories: eight toxic and one non-toxic. They experimented with different representations based on ElMo, BERT, and XLM, as well as with a variety of domain-specific features. Due to the small size of our dataset, they created a separate model for each feature type, and they ultimately combined these models into a meta-classifier. The evaluation results show an accuracy of 59.0% and a macro-F1 score of 39.7%, which represent sizable improvements over the majority-class baseline (Acc=30.3%, macro-F1=5.2%).",Yes,9,02.09.2019,14.10.2022,Yes,Yes,"Toxicity. ""…our aim here is to try to detect articles that can harm, give false impressions, or deceive the readers. Such articles can use some of the following techniques:
• Sensationalism: overexposing insignificant
or ordinary events by manipulating the main
point of an article;
• Fake news: news that sound right, but totally
misinterpret facts such as statistical data, locations, or dates, with the conscious aim of
proving something wrong.
• Conspiracy theories: information that usually gives a lot of detail, but does not offer
officially confirmed evidence or scientific research to back the claims that are being made.
This is typically centered around political, or
strange scientific phenomena.
• Hate speech: specifically targeting a person
or a social group to brutalize them or to bully
over the rate of normal conversation, to directly hurt or to manipulate them.""",Yes,"toxicity labels: fake news, sensations, hate speech, conspiracies, anti-democratic, pro-authoritarian, defamation, delusion.",Yes,Github,,https://github.com/deepanshu1995/HateSpeech-Hindi-English-Code-Mixed-Social-Media-Text,Yes,mediascan_bg_articles,1,1,Media Scan,Toxicity,"Bulgarian, English",N/A,N/A,N/A,N/A,"The authors used Media Scan' as a source of toxicity labels for Bulgarian media Web sites. The site contains information about 700 media, and 150 of them are associated with at least one toxic article. Many of these toxic articles are removed after the respective media have been contacted and informed about the problems with these articles. Naturally, the author of Media Scan wanted to preserve the original form of the evaluated article, and thus had a link to a PDF copy in case the original HTML page was not accessible. They only crawled the HTML for articles that have not been changed or removed at the time they created the dataset.

For each Web page with a toxic label, they ran a mechanical crawler to obtain its contents. This was not very reliable as each individual medium site has its own structure, while the crawler expected more or less semantic and valid HTML to be able to process it. Thus, they manually verified the data, fixed any issues they could find and added any missing information. They ended up with a little over 200 articles with some kind of toxicity. In addition to this dataset of only toxic articles, they added some ""non-toxic"" articles, fetched from media without toxicity examples in Media Scan: they added a total of 96 articles from 25 media.

For each article, they extracted its title and its body. They further extracted some meta information about the corresponding news medium. As some NLP resources are only available or are better for English, they translated the articles to English, by using Google Translate API, so that we can extract features from them.",Not discussed,,manual ,Not discussed,,Not discussed,,317,5-fold cross-validation,5-fold cross-validation,N/A,Not discussed,,Not discussed,,Specific,political,Targeted,political,Yes,No,,,Yes,"anti-democratic, pro-authoritarian…",No,,,No,Human
268,No,"(Chandra et al., 2020)","AbuseAnalyzer: Abuse Detection, Severity and Target Prediction for Gab Posts",Proceedings of the 28th International Conference on Computational Linguistics,India,"Previous works have focused on classifying user posts into various forms of abusive behavior. But there has hardly been any focus on estimating the severity of abuse and the target. In this paper, the authors present a first of the kind dataset with 7,601 posts from Gab which looks at online abuse from the perspective of presence of abuse, severity and target of abusive behavior. They also propose a system to address these tasks, obtaining an accuracy of ∼80% for abuse presence, ∼82% for abuse target prediction, and ∼65% for abuse severity prediction.",Yes,6,08.12.2020,14.10.2022,Yes,Yes,"Abuse in social media is spread across a wide spectrum from mild expressions of attitudes and beliefs to strong violent threats. Inspired by hate theories from Anti-Defamation League (ADL), we broadly classify forms of abuse as 'Biased Attitude, 'Act of Bias and Discrimination' and 'Violence and Genocide'. Moreover, abusive content could be targeted at specific individuals (e.g., a politician, a celebrity, etc.) or particular groups (a country, LGBTQ+, a religion, gender, an organization, etc.).",Yes,"Target of Hate labels: Individual (Second Person), Individual (Third Person), Group  Class of Hate labels: Biased Attitude, Act of Bias and Discrimination, Violence and Genocide",Yes,Github,,https://github.com/mohit3011/AbuseAnalyzer,Yes,AbuseAnalyzer_Dataset,1,1,Gab,Abusiveness,English,XX.07.2018-XX.10.2018,XX.07.2018-XX.10.2018,N/A,N/A,"The authors obtained a collection of 8.4 million Gab posts from http://files.pushshift.io/gab/ for a period of 4 months from Jul to Oct 2018. They used a high precision lexicon which consists of racial, sexist, xenophobic, extremist and other derogatory terminologies aggregated from multiple source. They used this to filter 7,601 posts written in English for the annotation process. While they made efforts to strike a balance between abusive versus non-abusive posts, they made no efforts to maintain balance within abuse severity or abuse target classes.",Yes,The usermentions in the dataset have been changed to `@usermention` for privacy and ethical reasons.,manual,Yes,"Four annotators with fluent English skills were provided clear guidelines (refined iteratively) for annotating the posts across all the three abuse prediction tasks. In case a post could belong to more than one severity classes, annotators were asked to mark the higher severity class (based on life-threatening consequences), to avoid multi-labels. Each example was annotated by exactly 3 annotators and all the disagreements were resolved after involving all the annotators. As a measure of inter-annotator agreement, the authors observed Cohen's Kappa Score as (1) 0.719 for presence/absence of abuse, (2) 0.720 for presence+target, and (3) 0.683 for presence+severity classification.",Not discussed,,7601 posts,5-fold cross-validation,5-fold cross-validation,Four annotators with fluent English skills.,Yes,The annotators were provided clear guidelines (refined iteratively) for annotating the posts across all the three abuse prediction tasks.,Not discussed,,Specific,"political, race, gender, religion, organization/institution, other",Targeted,"political, nationality, sexuality, religion, gender, organization/institution",Yes,Yes,"Ideology, Race, Gender, Religion, Work industry, Other","political, race, gender, religion, organization/institution, other",No,,No,,To a person,No,Human
270,No,"(Çöltekin, 2020)",A Corpus of Turkish Offensive Language on Social Media,Proceedings of the Twelfth Language Resources and Evaluation Conference,Germany,"This paper introduces a corpus of Turkish offensive language. The corpus consists of randomly sampled micro-blog posts from Twitter. The annotation guidelines are based on a careful review of the annotation practices of recent efforts for other languages. The corpus contains 36 232 tweets sampled randomly from the Twitter stream during a period of 18 months between Apr 2018 to Sept 2019. The authors found approximately 19 % of the tweets in the data contain some type of offensive language, which is further subcategorized based on the target of the offense. The authors describe the annotation process, discuss some interesting aspects of the data, and present results of automatically classifying the corpus using state-of-the-art text classification methods. The classifiers achieve 77.3 % F1 score on identifying offensive tweets, 77.9 % F1 score on determining whether a given offensive document is targeted or not, and 53.0 % F1 score on classifying the targeted offensive documents into three subcategories.",Yes,93,11.05.2020,14.10.2022,Yes,Yes,"A common trend is to use a set of classes based on the target of the offensive language. Particularly, if the target is an individual (or a number of loosely related individuals), or a group of people based on their race, gender, political/ideological affiliation, religion or a similar property. The former target category often includes acts of cyberbullying, while the latter is likely to be an instance of hate speech.",Yes,"offensive vs non-offensive (non)  offensive: not-targeted (prof), targeted targeted: group (grp), individual (ind), other (oth)",Yes,Website,,https://coltekin.github.io/offensive-turkish/,Yes,corpus of Turkish offensive language,1,1,Twitter,Offensiveness,Turkish,XX.03.2018-XX.09.2019 (with a 20-day gap during 11.2018),XX.03.2018-XX.09.2019 (with a 20-day gap during 11.2018),N/A,N/A,"The data was collected from Twitter using Twitter streaming API. As it is a common practice, the stream was filtered based on a list of frequent words in Turkish tweets and by Twitter's language identification mechanism. The data collection covers a wide time span from March 2018 to September 2019, with a gap of two weeks during November 2018. The authors obtained approximately 2 billion tweets.",Not discussed,,manual,Yes,"The annotation scheme follows the hierarchical structure:

1.offensive vs non-offensive (non)

2\. offensive: not-targeted (prof), targeted;

3\. targeted: group (grp), individual (ind), other (oth)

The annotators were asked to label each document instance with one or more of these labels. In case the text contained multiple offensive statements toward different types of targets, they allow multiple labels. However, annotators were discouraged to use multiple labels.

The annotators were asked to read the guidelines and annotate a small number of selected tweets before annotating the documents assigned to them. Each document was assigned to two annotators. However, not all annotators completed the annotation of the assigned documents.

In total, 36232 documents were annotated. The authors discarded the results from the annotators who annotated less than 100 documents, as well as 948 documents that were marked by at least one of the annotators for exclusion. The authors instructed annotators to exclude only those documents that cannot be understood by a native Turkish speaker, rather than excluding documents based on well-formedness or grammaticality. Most of the excluded documents are spam messages which are typically composed of sequences of unrelated frequent words or phrases, as well as documents that were mistakenly recognized as Turkish (often tweets in Azeri) by the Twitter's language detection mechanism.",Yes,"The authors’ pilot study showed that over 10% of the random tweets contained some form of offensive language. Hence, they simply annotate randomly sampled tweets with minimal filtering which makes the resulting corpus less biased and more representative of the offensive language use in this platform.

They selected 40 000 tweets to be annotated randomly from the large collection of tweets introduced above. We filter tweets by rejecting sampled tweets based on following criteria: re-tweets,  duplicates, tweets that belong to a verified user, tweets that contain less than five alphabetic tokens, tweets that contain URLs.

Some of the users that are represented more than once in the data set are robots and spammers, which end up being excluded from the final data set.","35,284",10-fold cross-validation,10-fold cross-validation,"The annotators were volunteers recruited from the author's personal contacts. All of the annotators are native speakers of Turkish, and all are highly educated.",Yes,"The annotation guidelines and their English translations are available at the corpus web page. The guidelines included explanations for the label set, the outline of annotation process, and a list of examples.",Not discussed,,General,N/A,Targeted,"race, gender, political, religion",Yes,Yes,"Gender, Ethnicity, Political affiliation, Religious belief, Other","gender, race, political, religion, other",No,,No,,To a person,No,Human
271,No,"(Vishwamitra et al., 2020)",On Analyzing COVID-19-related Hate Speech Using BERT Attention,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),USA,"In this work, the authors aim to discover the hate-related keywords linked to COVID-19 in hateful tweets posted on Twitter so that users posting such keywords can be asked to reconsider posting them. They first collect a new dataset of tweets targeting older people supplementing with a dataset targeting the Asian community. Then, they develop an approach to analyze the datasets with BERT (a transformer-based model) attention mechanism and discover 186 novel keywords targeting the Asian community and 100 keywords targeting older people. They then propose a control mechanism wherein a user can be asked to reconsider using certain sensitive words identified by our approach. They further perform an exploratory analysis of BERT attention mechanism and find that the most high-impact, long distance attentions are learned in the earlier or later layers of the model depending on the underlying data distribution. This study indicates that the BERT model in some cases uses a hate keyword and an associated group or individual to make predictions, a finding that is inline with existing hate-speech research, which suggests that hate-speech is often aimed at certain groups or individuals.",Yes,15,14.12.2020,14.10.2022,Yes,Yes,"Existing studies of hate speech from the social science literature have shown that hate speech is directed at an individual or group based on ""an arbitrary or normatively irrelevant feature, and that it casts the target as an ""undesirable presence and a legitimate object of hostility."" We used a similar definition for our annotation task: (a) has one or more COVID-19-related keywords, (b) is directed towards an individual or a group of older people (Boomers), and (c) is abusive or derogatory.",No,,No,,,,,Boomer-Hate Dataset (new); Asian-Hate Dataset,2,1,Twitter,Hate Speech,English,N/A,XX.12.2019 - XX.06.2020,N/A,COVID-19,"The authors adopted a keyword-based approach to collect COVID-19 tweets against old people using an online Twitter data collection tool (https://github.com/Jefferson-Henrique/GetOldTweets-python). They used the keywords ""boomer"" with COVID-19 related keywords such as ""Coronavirus"" and ""Covid-19 to search for such tweets. They restricted the tweet collection to English language only. Using these keywords, they collected 28,827 tweets between December 2019 and June 2020 from 1401 Twitter users.",Not discussed,,manual,Not discussed,,Yes,They first cleaned the tweets based on sentiment polarity and removed the tweets that are neutral sentiment using Python NLTK library. The two experts then labeled all the tweets in the dataset.,"1746 (Boomer-Hate), 2319 (Asian-Hate)",90% (~1571; ~2087),10% (~175; ~232),Two experts in the research team.,Not discussed,,Not discussed,,Specific,"race, nationality, age",Targeted,age,Yes,Yes,"Race, Ethnicity, Nationality, Age","race, nationality, age",Yes,"Asian, Older people",No,,,No,Human
272,No,"(Garcia-Recuero et al., 2018)",Trollslayer: Crowdsourcing and Characterization of Abusive Birds in Twitter,"2018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)","Spain, France, UK","As of today abuse is a pressing issue to participants and administrators of Online Social Networks (OSN). Given the difficulty in finding and accessing a large enough sample of abuse ground truth from the Twitter platform, the authors built and deployed a custom crawler that they use to judiciously collect a new dataset from the Twitter platform with the aim of characterizing the nature of abusive users, a.k.a abusive “birds”, in the wild. They provide a comprehensive set of features based on users' attributes, as well as social-graph metadata. The former includes metadata about the account itself, while the latter is computed from the social graph among the sender and the receiver of each message. Attribute-based features are useful to characterize user's accounts in OSN, while graph-based features can reveal the dynamics of information dissemination across the network. In particular, they derive the Jaccard index as a key feature to reveal the benign or malicious nature of directed messages in Twitter.",Yes,6,15.10.2018,14.10.2022,Yes,Yes,"Abuse in Twitter can spawn from arguments generated for influencing outcomes of a political election, the use of bots to automatically spread misinformation, and generally speaking, activities that deny, disrupt, degrade or deceive other participants and, or the network.",Yes,"abusive, acceptable, undecided.",Yes,Github,,https://github.com/algarecu/trollslayer,Yes,Trollslayer,1,1,Twitter,Abusiveness,English,N/A,N/A,N/A,N/A,"In order to collect data from Twitter the authors adapt the usual BFS for crawling social media studies and start the crawl of Twitter from a set of seed accounts that represent likely victims. Half of these accounts are chosen independently of any sign or trace of abuse in their public Twitter timeline in order to account for randomness in the measurements. The second half is selected based in their public timeline containing traces or likelihood of abuse, namely potential victims of abuse. Therefore, the authors define the seed set as made up of potential victims and likely victims. They then bootstrap their crawler, following the recursive procedure, which collects messages directed towards each of the seeds. If a message is directed towards or mentioning two or more victims, they consider it several times for the same message sender but with different destinations. They also collect the subscription and subscriber accounts of sender and receiver in the Twitter social graph, namely follower and followee relationships.

The configuration of the crawler controls from where the crawl starts and puts some restrictions on where it should stop. The first one of such restrictions during the graph traversal is collecting incoming edges a.k.a followers in Twitter when the number does not exceed an upper bound, depending on the chosen maxfollowers as node popularity. Secondly, the followers must be within a maximum depth we call maxdepth in order to collect the related metadata in the graph belonging to them. For each node meeting the above constraints, the authors also collected user account metadata as well as their respective public timeline of messages metadata in Twitter is collectedthen it starts crawling the followers of nodes at depth 1, and next depth 2 (followers of followers). In the dataset, the authors never go any further than second degree followers to collect relationships among users in the social graph crawled.",Not discussed,,manual,Yes,"In these two platforms (Trollslayer, developed by the authorsCrowdflower) the authors display the same tweets and the same guidelines to crowd workers that annotate messages. Therefore, the authors are able to compute the global scores from both platforms on the same tweets to end up at least 3 annotations per tweet as mentioned.

They aggregate the votes received for each tweet into a consensus score: if the aggregated score is between -1 and 1 the message is considered undecided. The sum of scores will render a tweet as abusive in the ground truth only when the score ;1 or &lt;-1 for acceptable.

To ensure agreement among crowd workers is valid, the authors calculate the inter-assessor agreement score of Randolph's multi-rater kappa among the crowd workers with common tweets annotated.",Not discussed,,14193,N/A,N/A,"156 crowd workers in Crowdflower and 7 trusted crowd workers in Trollslayer, accounting to 163 crowd workers overall.",Yes,The guidelines are displayed to the crowd workers.,Yes,"On Crowdflower, the authors spent around $30 in credit using a student data for everyone pack. The incentives for each annotator are not discussed.",General,N/A,Targeted,political,Yes,No,,,No,,No,,,No,Human
274,No,"(Díaz-Torres et al., 2020)",Automatic Detection of Offensive Language in Social Media: Defining Linguistic Criteria to build a Mexican Spanish Dataset,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Mexico, France","Phenomena such as bullying, homophobia, sexism and racism have transcended to social networks, motivating the development of tools for their automatic detection. The challenge becomes greater for languages rich in popular sayings, colloquial expressions and idioms which may contain vulgar, profane or rude words, but not always have the intention of offending, as is the case of Mexican Spanish. Under these circumstances, the identification of the offense goes beyond the lexical and syntactic elements of the message. This first work aims to define the main linguistic features of aggressive, offensive and vulgar language in social networks in order to establish linguistic-based criteria to facilitate the identification of abusive language. For this purpose, a Mexican Spanish Twitter corpus was compiled and analyzed. The dataset included words that, despite being rude, need to be considered in context to determine they are part of an offense. Based on the analysis of this corpus, linguistic criteria were defined to determine whether a message is offensive. To simplify the application of these criteria, an easy-to-follow diagram was designed. The paper presents an example of the use of the diagram, as well as the basic statistics of the corpus.",Yes,15,11.05.2020,14.10.2022,Yes,Yes,"We defined the concepts of offensive, aggressive and vulgar language, based on Austin's Speech Acts theory (Austin, 1962).
Beyond these lexical and syntactic elements, the pragmatic aspect of the messages is crucial to qualify them as aggressive, offensive or vulgar. According to the Speech Acts theory (Austin, 1962), the production of a statement performs three types of actions or acts at the same time: the locutionary act, the linguistic expression itself, its syntactic structure and the literal meaning semanticthe illocutionary act, the force or intention of the expression provided by the speakerand the perlocutionary act, the consequence or effect of the statement on the interlocutor. The second act is the one that interests the detection of abusive language, since the illocutionary force of a message is its underlying purpose, which could go from asking a question, an invitation, a reminder, to a warning, a promise, or a threat, among many others.",Yes,"Offensive language, Aggressive language, Vulgar language",Yes,Website,,https://sites.google.com/view/mex-a3t/,No,Mexican Spanish Dataset,1,1,Twitter,Offensiveness,Spanish,XX.08.2017 - XX.11.2017,XX.08.2017 - XX.11.2017,"Mexico City as the center, with a radius of 500km",N/A,"The authors considered Twitter as the source media. To build the corpus, they collected tweets from August to November of 2017. They used some rude words and controversial hashtags to narrow the search. They collected a set of 143 terms that served as seeds for extracting the tweets, which included words classified as vulgar and non-colloquial in the Diccionario de Mexicanismos de la Academia Mexicana de la Lengua, as well as words and hashtags identified by the Instituto Nacional de las Mujeres as related to violence and sexual harassment against women on Twitter.

To ensure their origin, the tweets were collected considering their geolocation. We considered Mexico City as the center and extracted all tweets that were within a radius of 500km. Finally, nearly 10,500 tweets in Mexican Spanish were collected and analyzed.",Not discussed,,manual,Yes,"The creation of the annotation scheme and the annotation task itself were part of an incremental and complementary process. Two linguists from the research team studied the abusive language phenomenon through the literature and analyzed the collected tweets, to arrive to a typology that identified the defining characteristics of vulgar, aggressive and offensive language. Then, the linguists wrote the annotation diagram and used it to classify the corpus.

The labeling process begins with the selection of a tweet, and the first question that asks if the tweet uses coarse language or with a sexual connotation. If the answer is yes, this indicates the message is vulgar, otherwise it is not. Following, the annotator is asked whether the tweet refers to an individual or to a group of people, or not. This question serves to make an early discard of aggressiveness and offensiveness, since these classes, unlike vulgar language, require of a target to qualify as such. If the message does not have a specific referent, the labeling process ends there. On the contrary, if the answer is positive, then the next question concerns aggressiveness, and asks if the tweet incites violence or tries to force the will of its referent. Finally, to determine if the message is offensive, the diagram directs the annotator to observe if the tweet uses pejorative, derogatory or negative intensifiers of a term to refer to its targetif the tweet seeks to humiliate or insult its referent. Be any of these questions answered affirmatively, the tweet shall be labeled as offensive.

It should be noted that each of these classifications, vulgar, aggressive, and offensive, are non-exclusive qualities of the tweet.",Not discussed,,10475 tweets,10-fold cross-validation,10-fold cross-validation,N/A,Yes,"The annotation scheme was designed as a flowchart, for the purpose of supporting abusive language categorization into aggressive, offensive and vulgar in a clear, visual way. It was devised with the goal to be easy to read and useful for annotators without strong linguistics knowledge, to account for the diversity of backgrounds in the field of natural language processing. The typology portrays each concept as a non-exclusive quality of the message or tweet. This way, the tool allows for a better characterization of the texts when considering the possibility of a tweet belonging to one, two or even all classes, which represents more accurately the nature of these messages in social networks. The flowchart presents questions regarding the form and function of the message, about the presence of insults, derogatory, or sexually-charged vocabulary, but most prominently it is concerned on the illocutionary force of the message; that is, the intention and target of the tweet.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
275,No,"(Del Vigna et al., 2017)","Hate me, hate me not: Hate speech detection on Facebook",CEUR Workshop Proceedings,Italy,"In this work, the authors aim at containing and preventing the alarming diffusion of hate campaigns. Using Facebook as a benchmark, they consider the textual content of comments appeared on a set of public Italian pages. They first propose a variety of hate categories to distinguish the kind of hate. Crawled comments are then annotated by up to five distinct human annotators, according to the defined taxonomy. Leveraging morpho-syntactical features, sentiment polarity and word embedding lexicons, the authors design and implement two classifiers for the Italian language, based on different learning algorithms: the first based on Support Vector Machines (SVM) and the second on a particular Recurrent Neural Network named Long Short Term Memory (LSTM). They test these two learning algorithms in order to verify their classification performances on the task of hate speech recognition. The results show the effectiveness of the two classification approaches tested over the first manually annotated Italian Hate Speech Corpus of social media text.",Yes,339,17.01.2017,14.10.2022,Yes,Yes,"The target of the trolls are often selected victims but, in some circumstances, the hate can be directed towards wide groups of individuals, discriminated for some features, like race or gender.",Yes,"levels of hate: No hate, Weak hate, and Strong hate distinct categories: Religion, Physical and/or mental handicap, Socio-economical status, Politics, Race, Sex and Gender issues, and Other.",No,,,,,hate speech Italian corpus,1,1,Facebook,Hate Speech,Italian,N/A,N/A,N/A,N/A,"Aiming at monitoring the ""hate level"" across Facebook, the authors have built a corpus of comments retrieved from the Facebook public pages of Italian newspapers, politicians, artists, and groups. These pages typically host discussions spanning across a variety of topics.

They have developed a versatile Facebook crawler, which exploits the Graph API to retrieve the content of the comments to Facebook posts. The crawler leverages the flexibility of the Laravel framework to deploy a wide set of features, like flexibility, code reuse, different storage strategies and parallel processing. Implemented as a Web service, it can be controlled through a Web interface or using a cURL command. The tool requires a set of registered application keys and some target pages to crawl. It is capable of storing data in the filesystem either as JSON files, or in Kafka queues or in Elasticsearch indexes. According to the number of application keys provided to the application, it is able to crawl multiple pages in parallel. Starting from the most recent post, the crawler collects all the information related to the posts, up to comments to comments. For the sake of simplicity, in this work we have however limited our analysis to direct comments to the posts. Overall, they collected 17,567 Facebook comments from 99 posts crawled from the selected pages.",Not discussed,,manual,Yes,"The authors asked 5 bachelor students to annotate comments, and the majority of comments received more than one annotation. Students annotated 5742, 3870, 4587, 2104 and 2006 comments respectively. In particular, among the annotated comments, 3,685 received at least 3 annotations.

The annotators were asked to assign one class to each post, where classes span over the followings levels of hate: No hate, Weak hate, and Strong hate.

The authors then divided hate messages into distinct categories: Religion, Physical and or mental handicap, Socio-economical status, Politics, Race, Sex and Gender issues, and Other.

The authors have also computed the Fleiss’ kappa κ inter-annotator agreement metric.",Not discussed,,"17,567 (For the experiments, only documents that were annotated at least by three different annotators and where the most annotated class exists were used. --; Two subsets: 3356 documents; 3575 documents)",10-fold cross-validation,10-fold cross-validation,Five bachelor students,Not discussed,,Not discussed,,Specific,"religion, class, political, race, gender, other",Targeted,"race, gender",Yes,Yes,"Religion, Physical, mental handicap, Class, Politics, Race, Sex, Gender, Other","religion, class, political, race, gender, other",No,,No,,,No,Human
276,No,"(Bohra et al., 2018)",A Dataset of Hindi-English Code-Mixed Social Media Text for Hate Speech Detection,"Proceedings of the Second Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media",India,"This work is the first attempt in detecting hate speech in Hindi-English code-mixed social media text. In this paper, the authors analyze the problem of hate speech detection in code-mixed texts and present a Hindi-English code-mixed dataset consisting of tweets posted online on Twitter. The tweets are annotated with the language at word level and the class they belong to (Hate Speech or Normal Speech). They also propose a supervised classification system for detecting hate speech in the text using various character level, word level, and lexicon based features.",Yes,154,06.06.2018,14.10.2022,Yes,Yes,"Hate speech. ""Hate speech detection in social media texts is an important Natural language Processing task, which has several crucial applications like sentiment analysis, investigating cyber bullying and examining socio-political controversies. """,No,,Yes,Github,,https://github.com/deepanshu1995/HateSpeech-Hindi-English-Code-Mixed-Social-Media-Text,Yes,Hindi-English Code-Mixed Social Media Text,1,1,Twitter,Hate Speech,Hindi-English,N/A,"""in the last five years""",N/A,N/A,"The authors constructed the Hindi-English code-mixed corpus using the tweets posted online in last five years. Tweets were scrapped from Twitter using the Twitter Python API which uses the advanced search option of twitter. They have mined the tweets by selecting certain hashtags and keywords from politics. public protests, riots, etc., which have a good propensity for the presence of hate speech. They retrieved 1,12,718 tweets from Twitter in json format, which consists of information such as timestamp, URL, text, user, re-tweets, replies, full name, id and likes. An extensive processing was carried out to remove all the noisy tweets. Furthermore, all those tweets which were written either in pure English or pure Hindi language were removed. As a result of manual filtering, a dataset of 4575 code-mixed tweets was created.",Yes,Due to privacy policy of twitter the authors are releasing only tweet ids not the tweet text. Tweet text can be requested by mailing at deepanshuvijay01995@gmail.com,manual,Yes,"Annotation of the corpus was carried out as follows:

Language at Word Level: For each word, a tag was assigned to its source language. Three kinds of tags namely, 'eng', 'hin' and 'other' were assigned to the words by bilingual speakers. 'eng' tag was assigned to words which are present in English vocabulary, such as ""School"", ""Death"" etc. 'hin' tag was assigned to words which are present in the Hindi vocabulary such as ""nafrat"" (Hatred), ""marna"" (dying). The tag other' was given to symbols, emoticons, punctuations, named entities, acronyms, and URLs.

Hate Speech or Normal Speech: Each tweet is enclosed within &lt;tweet;&lt;/tweet;tags. First line in every annotation consists of tweet id. Language tags are added before every token of the tweet, enclosed within &lt;word;&lt;/wordtags. Each tweet is annotated with one of the two tags (Hate Speech or Normal Speech).

A sample annotation set consisting of 50 tweets (25 hate speech and 25 non hate speech) selected randomly from all across the corpus was provided to both the annotators in order to have a reference baseline so as to differentiate between hate speech and non hate speech text. In order to validate the quality of annotation, the authors calculated the inter-annotator agreement (IAA) for hate speech annotation between the two annotation sets of 4575 code-mixed tweets using Cohen's Kappa coefficient.",Not discussed,,4575,10-fold cross-validation,10-fold cross-validation,Two human annotators having linguistic background and proficiency in both Hindi and English.,Not discussed,,Not discussed,,General,N/A,Targeted,political,Yes,No,,,No,,No,,,No,Human
277,No,"(Espinoza & Weiss, 2020)",Detection of Harassment on Twitter with Deep Learning Techniques,Machine Learning and Knowledge Discovery in Databases,Chile,"In this paper, the authors present a new dataset of harassment detection on Twitter with four classes, presented for the SIMAH competition. Then they apply three different deep learning architectures (CNN, LSTM, and BiGRU) to classify these tweets showing that it is a hard problem to solve especially because of the lack of annotated data within some classes. The results only on the test set reach 46% in f1-score and using all data to train gives 55% using the same metric.",Yes,5,28.03.2020,14.10.2022,Yes,Yes,"Harassment. ""This hate language is generally used to attack other people about their sexuality, ethnicity, political affiliation, among others, and can cause great harm to them.""",Yes,"Harassment: “Indirect harassment”, “Physical harassment”, and “Sexual harassment”.",No,,,,,dataset with oversampling (w/ SMOTE); dataset without oversampling (w/o SMOTE),2,1,Twitter,Harassment,English,N/A,N/A,N/A,N/A,"The authors used the dataset published in a previous research.

Due to the imbalance between classes, they opted to use the data augmentation technique SMOTE. This oversampling strategy uses the local neighborhood of the real samples to create synthetic data based on the mean of near data in the space, allowing to have an equal number of samples per class. As embeddings are n-dimensional vectors we passed the collection of embeddings to SMOTE to generate the missing data in the unbalanced classes. In this new scenario, it should be easier for the model to learn the minority classes. Finally, we had a dataset of 20.616 examples as a result of applying over-sampling to both training and validation dataset.

To compare the results of applying SMOTE, we handled a version of the data with oversampling and a version without it.",Not discussed,,N/A,Not discussed,,Not discussed,,10623,6374,2123,N/A,Not discussed,,Not discussed,,Specific,"gender, sexuality",Targeted,"sexuality, race, political ",Yes,No,,,No,,No,,,No,Human
279,No,"(Mubarak et al., 2017)",Abusive Language Detection on Arabic Social Media,Proceedings of the First Workshop on Abusive Language Online,"Qatar, UK","In this paper, the authors present the work on detecting abusive language on Arabic social media. They extract a list of obscene words and hashtags using common patterns used in offensive and rude communications. They also classify Twitter users according to whether they use any of these words or not in their tweets. They expand the list of obscene words using this classification and report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean). They make this dataset freely available for research, in addition to the list of obscene words and hashtags. They are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site's rules and guidelines.",Yes,230,30.07.2017,14.10.2022,Yes,Yes,"Abusive language. ""Jay and Janschewitz (2008) identified three categories of offensive speech, namely: Vulgar, which include explicit and rude sexual references, Pornographic, and Hateful, which includes offensive remarks concerning peoples race, religion, country, etc. The goal of this work is to detect vulgar and pornographic obscene speech in Arabic social media without the need for manually curating word lists.""",Yes,"obscene, offensive (but not obscene), or clean.",Yes,Website,,http://alt.qcri.org/~hmubarak/offensive/ObsceneWords.txt  http://alt.qcri.org/~hmubarak/offensive/TweetClassification-Summary.xlsx  http://alt.qcri.org/~hmubarak/offensive/AJCommentsClassification-CF.xlsx,Yes,test set for the obscene and offensive language detection; Aljazeera Deleted Comments,2,2,"Twitter, Aljazeera.net","Abusiveness, Offensiveness, Obscenity",Arabic,XX.03.2014,XX.03.2014,N/A,N/A,"The authors created a set of obscene words to work as our seeding list. They extracted the list from a large set of tweets containing 175 million tweets that were obtained from Twitter during March 2014 using the Twitter streaming API with language filter set to Arabic ""lang:ar"". They searched the tweets for some patterns that are usually used in offensive communications, such as (You, son(s) of, daughters) of, .. etc.) along with variant spellings. The words appearing after these patterns were then collected and manually assessed for being obscene or not. The final list after manual assessment contained obscene 288 words and phrases. Additionally, they added the 127 hashtags that are used to screen pornographic pages in an online tweet aggregator TweetMogaz.

For extrinsic evaluation, they built a test set for the obscene and offensive language detection that contains 1,100 annotated tweets all together.

--

Aljazeera Deleted Comments: They also release a dataset of 32K deleted comments from Al jazeera.net (a popular Arabic news channel) which moderates all the comments that appear on their site. According to the site's ""Community Rules and Guidelines"" (http: //www.aljazeera.com/aboutus/2011/ 01/201111681520872288.htm1), a user comment is not accepted if it is a personal attack, racist, sexist, or otherwise offensive, inciting violence, non-relevant, advertising, etc. Initially they obtained a corpus of 400K comments on approximately 10K articles that cover many genres such as politics, economy, society, and science. From these comments, they selected 32K comments whose lengths are between 3 and 200 characters to ease subsequent annotation.",Not discussed,,manual,Yes,"For the 1100 tweets, the authors submitted each tweet along with its context (thread of replies) to CrowdFlower.com to be judged by 3 different annotators from Egypt. The annotators could mark the tweets as: obscene, offensive (but not obscene), or clean.

For the  Aljazeera Deleted Comments, the selected comments were annotated using CrowdFlower, where three annotators were asked to classify comments as obscene, offensive, or clean. The annotators were also given article titles as we did not have the entire thread of comments.",Yes,"The authors built a test set for the obscene and offensive language detection that contains 100 highly discussed tweets that each had at least 10 replies. Specifically, they collected the 100 tweets by identifying 10 controversial tweeps from the top tweeps in Egypt, according to SocialBakers.com. For each of the tweeps, they randomly selected 10 tweets that have 10 or more comments/replies. In all, they had 100 original tweets plus 1,000 comment/reply tweets - 1,100 tweets all together.

From the ~400K Aljazeera Deleted Comments, they selected 32K comments whose lengths are between 3 and 200 characters to ease subsequent annotation.",1100 (tweets); ~32K(Aljazeera Deleted Comments),N/A,1100,Recruited via CrowdFlower,Not discussed,,Not discussed,,General,N/A,Targeted,"race, religion, nationality",Yes,No,,,No,,No,,,No,Human
281,No,"(Rezvan et al., 2018)",A Quality Type-aware Annotated Corpus and Lexicon for Harassment Research,Proceedings of the 10th ACM Conference on Web Science,USA,"This paper provides both a quality annotated corpus and an offensive words lexicon capturing different types of harassment content: (i) sexual, (ii) racial, (iii) appearance-related, (iv) intellectual, and (v) political. The authors first crawled data from Twitter using this content-tailored offensive lexicon. As mere presence of an offensive word is not a reliable indicator of harassment, human judges annotated tweets for the presence of harassment. This corpus consists of 25,000 annotated tweets for the five types of harassment content and is available on the Git repository.",Yes,53,15.05.2018,14.10.2022,Yes,Yes,"Our focus here is on the sender, whose
messages are intended to harass. We study harassment[7] in five
content areas: (i) sexual, (ii) racial, (iii) appearance-related, (iv) intellectual, and (v) political. Below, we briefly describe each type.
• Sexual harassment concerns sexuality and often targets
females. The harasser might refer to a victim’s sex organs
with slang or describe sexual relations with slang. However,
slang itself is not sufficient to indicate sexual harassment5
6
.
• Racial harassment targets race and ethnicity characteristics of a victim such as color, country, culture, faith, and religion7
.
• Appearance-related harassment is related to body appearance apart from sexuality. All dimensions of appearance are candidates, for example, hair style or looks. Fat
shaming [1] and body shaming are critical sub-types.
• Intellectual harassment concerns intellectual power or
the merits of individual opinion. Sub-types include level of
formal education and grammar. Victims may in fact be intellectually gifted8
.Political harassment relates to political views9
, regarding
issues under governmental influence such as global warming, the opiod epidemic, immigration or gun control. Typical
targets are politicians and politically active individuals
10",Yes,"harassment in five content areas: (i) sexual, (ii) racial, (iii) appearance-related, (iv) intellectual, and (v) political.",Yes,Other,request via email,"Although the authors mentioned in the paper that it is available at: https://github.com/Mrezvan94/Harassment-Corpus, the github page says “To getting our annotated tweets in five context, please contact the authors via these emails: Mohammadreza Rezvan: mohammadrezarezvan94@gmail.com Saeedeh Shekarpour: sshekarpour1@udayton.edu""",Yes,Harassment-Corpus,1,1,Twitter,Harassment,English,18.12.2016 - 10.01.2017,18.12.2016 - 10.01.2017,N/A,N/A,"The identification of cyberbullying typically begins with a lexicon of potentially profane or offensive words. The authors created a lexicon containing offensive (i.e., profane) words covering five different types of harassment content. The resulting compiled lexicon includes six categories: (i) sexual, (i) racial, (iii) appearance-related, (iv) intellectual, (v) political, and (vi) a generic category that contains profane words not exclusively attributed to the five specific types of harassment. A native English speaker conducted this categorization.

The authors utilized the first five categories of our lexicon as seed terms for collecting tweets from Twitter between December 18th, 2016 to January 10th 2017. Requiring the presence of at least one lexicon item, they collected 10,000 tweets for each contextual type for a total of 50,000 tweets.",Not discussed,,manual,Yes,"Human judges annotated the corpus to discriminate harassing tweets from non-harassing tweets. Three native English speaking annotators determined whether or not a given tweet is harassing with respect to the type of harassment content and assigned one of three labels ""yes"", ""no"", and ""other"". The last label indicates that the given tweet either does not belong to the current context or cannot be decided. Finally, 75,000 annotation work had been done totally. The eventual corpus excluded all of the tweets that did not have a consensus label of ""ves"" and ""no"". The authors used Cohen’s kappa coefficient to measure the quality of annotation.",Not discussed,,24189,N/A,N/A,Three native English speaking annotators,Not discussed,,Not discussed,,Specific,"race, gender, body, other",Targeted,"sexuality, race, body, political, other",Yes,Yes,"Race, sex/gender, appearance, intelligence","race, gender, body, other",No,,No,,,No,Human
282,No,"(Purba et al., 2018)",A Study on the Methods to Identify and Classify Cyberbullying in Social Media,"2018 Fourth International Conference on Advances in Computing, Communication & Automation (ICACCA)",Malaysia,"In this research, a survey will be conducted to review current researches in cyberbullying classification. There are three steps to classify cyberbullying, i.e. collection of data set, training, and classification process. There are two approaches that can be used for the system namely, statistical and machine or deep learning approach. This study shows that the technique used to classify cyberbullying texts are shifting from statistical approach to machine learning such as SVM in 2015 and before, to deep learning such as CNN and LSTM in 2016 and later. Image analysis and social analysis of the victim or attacker can be added to help the cyberbullying classification. Deep learning is proven to be the most accurate method in most cases and data set. In this paper, we also contributed our Instagram dataset for public.",Yes,2,26.10.2018,14.10.2022,Yes,Yes,Cyberbullying is a general aggressive behavior done by misusing Internet-related technologies,Yes,"6 classes sentiment (abusive, sad, spam, neutral, happy, praising) based on common emoticons. (the class labels didn’t cover all the comments)",Yes,Github,,https://github.com/kristoradion/instagram/wiki,Yes,Instagram Comments Dataset,1,1,Instagram,Cyberbullying,English,XX.08.2018,XX.08.2018,N/A,N/A,"The authors collected the dataset scraping Instagram website. This dataset contains 2,360,584 comments from random media in Instagram.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
284,No,"(Tontodimamma et al., 2020)",Facebook Debate on Sea Watch 3 Case: Detecting Offensive Language Through Automatic Topic Mining Techniques,Data Science and Social Research II,Italy,"In this paper, the authors present a text analysis for detecting abusive language in Italian messages on Facebook, surrounding the debate over the migrant-rescue ship, Sea Watch 3, and its captain Carola Rackete. The study data consists of more than 130,000 posts retrieved from two pages relating to Matteo Salvini, the leader of the Italian Lega political party, and from the official Facebook pages of five Italian newspapers. To explore the presence of offensive and hatred expressions in the corpus and to establish to what extent social users’ language differs, depending on the type of Facebook pages analysed, the authors ran a topic model based on Latent Dirichlet Allocation. They have complemented this approach with tools from semantic network analysis.",Yes,2,26.11.2020,14.10.2022,Yes,Yes,"While there is no universally accepted definition of hate speech, it is possible to affirm that this term covers all forms of hostile, bias-motivated and malicious expression directed against particular ethnic, religious, racial or sexually oriented groups or persons in society (Almagor 2011).",No,,No,,,,,Carola Rackete corpus,1,1,Facebook,"Offensiveness, Hate Speech",Italian,N/A,N/A,N/A,Sea Watch 3 case,"The corpus was built by downloading the comments on Carola Rackete's posts. In particular, the authors decided to analyse the comments posted on the official page of Matteo Salvini and the closely linked page ""Lega Salvini Premier"".

They also gathered comments on posts retrieved from the official Facebook pages of five Italian newspapers. The comments were scraped using exportcomments.com and the analysed corpus consists of 131,403 posts (Salvini's pages: 34,305, Il Corriere della Sera: 22,306, Il Giornale: 10,843, La Republica: 28,998, Il Fatto quotidiano: 34,314, Libero: 637).

They developed a Python script to perform the screening and cleaning process of text documents in order to extract the relevant content and remove any unwanted stop words. Words with less than 75 occurrences have been pruned.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,other,Targeted,"race, religion, sexuality",No,No,,,No,,Yes,Carola Rackete,About a person,No,Human
285,No,"(Cortis & Handschuh, 2015)",Analysis of cyberbullying tweets in trending world events,Proceedings of the 15th International Conference on Knowledge Technologies and Data-driven Business,"Germany, Ireland","In this paper the authors tackle the problem of cyberbullying via a novel approach that analyses online posts in trending world events. These generally cause a lot of interest and controversy among online Web users. Twitter is the social network of choice, where a large dataset of tweets is collected. The two current world events selected are the Ebola virus outbreak in Africa and the shooting of Michael Brown in Ferguson, Missouri. Collected tweets are carefully analysed to identify the most popular hashtags and named entities used within cyberbullying tweets. This analysis provides a basis towards several useful applications, such as a cyberbullying online post detector for certain current trending world events.",Yes,43,21.10.2015,14.10.2022,Yes,Yes,"Chadwick defines cyberbullying as ""the use of technology to harass, threaten, embarrass, or target another person"", which definition is the most representative for the purposes of our study.",No,,Yes,Other,Google Drive,https://drive.google.com/drive/folders/0B8B9iB99uLWFTnNWdlM3ZTRqWkU?usp=sharing,Yes,The dataset (new); Formspring.me dataset (existing); Ask.fm dataset (existing),3,1,Twitter,Cyberbullying,English,N/A,mid-August - mid- November 2014,N/A,#ebola (Ebola virus outbreak); #ferguson (shooting in Ferguson),"The #ebola hashtag was chosen for the Ebola virus outbreak event, whereas the #ferguson hashtag was selected for the shooting in Ferguson.

Then the authors tried to identify the most popular terms used within cyberbullying online posts, where the choice was based on the work by Kontostathis et al.. Experiments from this study identified content words (such as insult and swear words, reaction words, personal pronouns) collected from www.noswearing.com that can be used to detect cyberbullying cases. The top twenty six weighted terms were determined from the collected Formspring.me (as of 2013 known as Spring.me) - a question-and-answer based social networking website - dataset. The top ten terms out of the twenty six identified by Kontostathis et al., were chosen to be used in the experiments. These terms were weighed by a machine learning method that the authors used to find the frequency of terms in the posts they collected. From the ten chosen terms, eight were classified as insult and swear words (whore, hoe, bitch, gay, fuck, ugly, fake, slut), one reaction word (thanks) and one personal pronoun (youre). Six (hoe, bitch, gay, fuck, ugly, slut) out of the ten words chosen for our study are in par with the negative words that obtained the highest frequency and eigenvector centrality within the Ask.fm dataset.

After the selection of hashtags and key terms that are generally used within cyberbullying online posts on social networks, the dataset of tweets required for the experiments was collected via an online post extractor. The query mechanism extracted tweets that contained a hashtag (or its equivalent as a keyword if no such hashtag was present) pertaining to the trending world event in question and one of the cyberbullying key terms identified above. The Twitter Search API was used for this dataset collection, specifically via the GET search/tweets call. This call is used for collecting relevant tweets that match a specified query. The following is an example of a query: https://api.twitter.com/1.1/search /tweets.json?q=\%23ebola\%20whore. Hence, tweets that have a hashtag for a particular event (#ebola in the query example) and one of the 10 key terms selected (""whore"" in the query example), are returned for every issued query. Some criteria were set to the queries, specifically to include both popular and real time results in their response and restrict the tweets to the English language. Additionally, the data was collected from tweets that were posted within a date range of three months from mid-August till mid-November.

The current implementation of the online post extractor employs the Scribe Auth Java library16 to retrieve data from Twitter. Since the response format of the Twitter API is JSON, the JSON-lib Java library is used for extracting the text value of the returned tweets. The dataset comprises of 2607 tweets. From this collection, 1480 tweets targeted the Ebola virus outbreak trending event, whereas the remaining 1127 tweets were about the shooting in Ferguson. Through the pre-processing techniques, The dataset was reduced to a total of 1544 tweets, with 908 targeting the Ebola event and 636 the Ferguson event.",Not discussed,,manual,Yes,"The first data curator was appointed to manually label the true cyberbullying tweets from the list of pre-processed ones. On the other hand, the second curator was tasked with verifying that all these labelled tweets were cases of cyberbullying, and that no true positives were missed from the pre-processed set. In addition, the data curators manually followed (hyperlink resolution) any URLs embedded within a particular tweet when in doubt of whether it was a cyberbullying case or not, in order to label the tweets into their correct category and avoid any false positives/negatives.

The final dataset was further reduced to a total of 843 tweets, with 468 targeting the Ebola event and 375 the Ferguson event.",Not discussed,,843,N/A,N/A,"Two data curators were both given ample time to familiarise themselves with profiles of cyber bullies and cyberbullying posts, specifically the datasets published by ChatCoder - a project that tackles cyberbullying and cyberprediction.",Yes,"Two data curators were both given ample time to familiarise themselves with profiles of cyber bullies and cyberbullying posts, specifically the datasets published by ChatCoder - a project that tackles cyberbullying and cyberprediction.",Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
286,No,"(Tulkens et al., 2016)",The Automated Detection of Racist Discourse in Dutch Social Media,Computational Linguistics in the Netherlands Journal,Belgium,"The authors present two experiments on the automated detection of racist discourse in Dutch social media. In both experiments, multiple classifiers are trained on the same training set. This training set consists of Dutch posts retrieved from two public Belgian social media pages which are likely to attract racist reactions. The posts were labeled as racist or non-racist by multiple annotators, who reached an acceptable agreement score. The different classification models all use the Support Vector Machine algorithm, but use different (sets of) linguistic features, which can be lexical, stylistic or dictionary-based. In the first experiment, the models are evaluated on a test set containing unseen comments retrieved from the same pages as the training set (and thus also skewed towards racism). In the second experiment, the same models from Experiment 1 are tested on an alternative test set, containing more neutral comments, retrieved from the social media page of a Belgian newspaper. In both experiments, the best performing model relies on a dictionary containing difierent word categories specifically related to racist discourse. It reaches an F-score of 0.47 (exp. 1) and 0.40 (exp. 2) for the racist class and ROC Area Under Curve scores of 0.64 (exp. 1) and 0.73 (exp. 2).",Yes,39,01.12.2016,14.10.2022,Yes,Yes,"We follow Bonilla-Silva, who contends that racism is not limited to physical or ethnic qualities, but can also include social and cultural aspects, as racism as an ideology has moved towards so-called color-blindness, which implies the use of ""cultural rather than biological explanation of minorities' inferior standing and performance in labor and educational markets"".
Racist slurs “have their pragmatic effect by forcing an addressed individual into a social group designation, then pejorating that group, by adding explicitly negative attributes or by invoking generally tabooed group labels”(Coupland 2010, p.251).",No,,Yes,Github,,https://github.com/clips/hades  It was witten that “the procedure for requesting the corpus are available at: https://github.com/clips/hades.” but the procedure cannot be found at the time of filling the information.  (14.10.2022),No,"Experiment1: training corpustest corpus;

Experiment2: test corpus (more neutral)",3,3,"a community hub of a prominent anti-Islamic organization, a page that was used to post articles by a well-known right-wing organization",Racism,Dutch,N/A,N/A,N/A,N/A,"Experiment 1 corpus:

The authors selected two pagesthe first one served as a community hub of a prominent anti-Islamic organization, while the second one was used to post articles by a well-known right-wing organization. To collect data, they used Pattern (De Smedt and Daelemans 2012) to scrape the 100 most recent posts from both social media pages, and then extracted all reactions to these posts. (the pages post articles from news websites which concern, for example, immigration. These news stories then attract comments by the people who subscribe to the page, and may contain racist remarks. The posts of the page itself therefore do not contain racist remarks, but instead serve as a catalyst for the posting of derogatory comments.)

Extracting the first 100 posts resulted in 5759 extracted comments: 4880 from the first page and 879 from the second one.

To obtain an independent test corpus they followed the same procedure, albeit at a different point in time. They mined the first 500 and first 116 comments from the first and second page, respectively, which makes the proportion of comments that were retrieved from the pages more or less identical to the proportions in the training corpus. Furthermore, this makes the size of the test set about 10% of the training set, which is a standard proportion in classification tasks.

Experiment 2 corpus:

To collect a more neutral test set, they turned towards the social media page of a well-known Belgian newspaper. From this page, we scraped 8 posts, all of which were links to news articles on the newspaper's website. The posts themselves only contained short utterances, usually paraphrases of the article to which the post linked. They retrieved all comments on these posts, just like they did for the first test set.

This resulted in a test set of 1138 comments - approximately 20% of the size of the training set.

(As the newspaper itself is neutral and does not exclusively post articles about subjects that could attract racist reactions, the authors argue that this set of comments will also be more nuanced and contain less instances of overt racism than the first test set. An important caveat is that, at the time of extraction, the arrest of a well-known terrorist figured prominently in the Belgian news. Because of this, many of the comments will still concern Muslim extremism, and possibly stereotypes regarding Muslims and extremism. Despite this, the authors argue that this set gives a more realistic view of racist reactions on general social media, thereby alleviating both of the problems above. )",Not discussed,,manual,Yes,"Experiment 1 (training set):

not be an explanation for the smaller amount of comments on the second page. The corpus was annotated by two annotators, A and B, who were both students of comparable age and background. When A and B did not agree, a third annotator, C, functioned as a tiebreaker in order to obtain gold-standard labels. The comments were annotated with three different labels: 'racist'. 'non-racist' and 'invalid'. The 'racist' label describes comments that contain negative utterances or insults about someone's ethnicity, nationality, religion or culture. This label also includes utterances which equate, for example, an ethnic group to an extremist group, as well as extreme generalizations. The label 'invalid' was used for comments that were written in languages other than Dutch, or that did not contain any textual information, i.e. comments that solely consisted of pictures or links. Before automatic classification, they excluded these from both the training and test set. The final label, 'non-racist', was the default label. If a comment was valid, but could not be considered racist according to our definition, this was the label they used. They calculated inter-annotator agreement using the Kappa score (κ) (Cohen 1968) and simple pairwise agreement.

Experiment 1 (test set):

The annotation scheme used for the test set was identical to the one for the training set. A difference in the annotation process was that C, who previously performed the tiebreak, also annotated a part of the posts. This was done to assess the degree to which the tiebreaker agreed with the annotators, something which was not possible in the setup used for the training data. To compute inter-annotator agreement, the first 25% of comments on each page, i.e. 125 comments for the first page and 30 comments for the second one, were annotated by all three annotators. The remaining comments were equally divided among the annotators.

Experiment 2 (new test set):

The annotation of the new test set was performed by the same three annotators from Experiment 1, following the same guidelines as before. The same three possible labels were kept: a post could either be invalid, racist or non-racist. 25% of the test comments were annotated by all three annotators. The remaining 75% of the comments was equally divided among the annotators.",Not discussed,,6031 (experiment-1); 1138 (experiment-2),"~90%, ~5400 (experiment-1)","~10%, ~600 (experiment-1); 1138 (experiment-2)","A and B, who were both students of comparable age and background.",Yes,The annotation guidelines include (may not limit to) the authors’ own definition of racist discourse.,Not discussed,,Specific,"race, nationality, religion, other",Targeted,race,Yes,Yes,"Race, Ethnicity, Nationality, Religion, Extremist","race, nationality, religion, other",No,,No,,,No,Human
287,No,"(Gaikwad et al., 2021)",Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi,Proceedings of the Conference Recent Advances in Natural Language Processing - Deep Learning for Natural Language Processing Methods and Applications,"USA, UK","The authors introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the ﬁrst dataset of its kind compiled for Marathi, thus opening a new domain for research in lowresource Indo-Aryan languages. They present results from several machine learning experiments on this dataset, including zero-short and other transfer learning experiments on state-ofthe-art cross-lingual transformers from existing data in Bengali, English, and Hindi.",Yes,28,01.09.2021,14.10.2022,Yes,Yes,offensive language,No,,Yes,Github,,https://github.com/tharindudr/MOLD,Yes,"MOLD (Marathi Offensive Language Dataset);

existing datasets: TRACOLIDSOLIDHASOC",5,1,Twitter,Offensiveness,Marathi,N/A,N/A,N/A,N/A,"The Marathi dataset contains data collected from Twitter using the Twitter API. They aimed to achieve a similar distribution of offensive vs. non-offensive content present in OLID, which contains around 33% offensive and 67% non-offensive tweets. To make sure that both classes were represented, they used both offensive and non-offensive keywords. For the offensive content they used 22 common curse words in Marathi and for the non-offensive content they used search phrases related to politics, entertainment, and sports along with the hashtag #Marathi.",Not discussed,,manual,Yes,"The authors followed OLID’s annotation scheme for English. A total 2,547 tweets were collected and annotated by 6 volunteer annotators. The annotation task is a binary classification, in which annotators assigned tweets as offensive (OFF) or not offensive (NOT). The annotators could flag a tweet as invalid if it contained four or more non-Marathi words. The final version of MOLD contains 2,499 annotated tweets. They provided a common set of 100 instances to each of the three pairs of annotators so that they can measure agreement between pairs of annotators using Cohen’s kappa.",Not discussed,,2499,"1,874",625,Six volunteer annotators who are native speakers of Marathi with age between 20 and 25 years old and a bachelors degree,Not discussed,,Not discussed,,Specific,other,Targeted,gender,Yes,Yes,"Individual, Group, Other",other,Yes,"individual, group, other",Yes,"individual, group, other",To a person,No,Human
288,No,"(Kumari et al., 2021)",Multi-modal aggression identification using Convolutional Neural Network and Binary Particle Swarm Optimization,Future Generation Computer Systems,"India, UK","This paper presents a model based on a Convolutional Neural Network (CNN) and Binary Particle Swarm Optimization (BPSO) to classify the social media posts containing images with associated textual comments into non-aggressive, medium-aggressive and high-aggressive classes. A dataset containing symbolic images and the corresponding textual comments was created to validate the proposed model. The framework employs a pre-trained VGG-16 to extract the image features and a three-layered CNN to extract the textual features in parallel. The hybrid feature set obtained by concatenating the image and the text features were optimized using the BPSO algorithm to extract the more relevant features. The proposed model with optimized features and Random Forest classifier achieves a weighted F1-Score of 0.74, an improvement of around 3% over unoptimized features.",Yes,22,01.05.2021,14.10.2022,Yes,Yes,"Cyber-aggression is characterized as hostile or violent behaviour with the aim of harming others by using electronic media. It comprises sending, posting or sharing threatening, negative or nasty information about an individual or a group causing character assassination, humiliation, emotional stress, depression, anxiety and suicidal thoughts to the victim or victims.",Yes,Class of post: Non-aggressive; Medium-aggressive; High-aggressive,No,,,,,multi-modal cyber-aggressive dataset,1,1,"Facebook, Twitter, Instagram, Google search",Aggressiveness,English,N/A,N/A,N/A,N/A,"Symbolic images are used by many people to annoy, threaten and humiliate other online users with the help of social media sites such as Twitter, Facebook and Instagram.

The authors collected some of symbolic images (used by many people to annoy, threaten and humiliate other online users) from Facebook, Twitter and Instagram. To find such images, they also used Google search with query terms like cyber-aggressive images, aggressive images and bullying images to increase the number of required images in the collection. Keeping in mind the level of aggression, the images were manually filtered and thus they finally got a total of 3,600 images.

Among these collected images, some were associated with comments and some were without associated comments. The comment was composed and added to each of those images that did not carry it by a group of undergraduate students.",Not discussed,,manual,Yes,"Considering both image and associated comment, each post was then manually labelled as either (i) Non-aggressive, (in) Medium-aggressive or (iii) High-aggressive by three independent expert annotators. To do this job the annotators were instructed to label the post indicating physical threat as a 'High-aggressive post', the post having indirect aggression or aggression other than a physical threat as a 'Medium-aggressive post', and the post with no aggression as a 'Non-aggressive post. After labelling independently, a majority voting scheme among the annotators was applied to assign the final label to each post.",Not discussed,,3600 posts,75% (~2700),25% (~900),Three independent expert annotators,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
289,No,"(Chiril et al., 2020)",An annotated corpus for sexism detection in French tweets,"LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings",France,"This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, the authors think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of the annotation scheme of this study. They also propose some preliminary results for sexism detection obtained with a deep learning approach. The experiments show encouraging results.",Yes,29,11.05.2020,14.10.2022,Yes,Yes,Sexism is prejudice or discrimination based on a person’s gender.,Yes,Sexist content: direct; descriptive; reporting,Yes,Github,,https://github.com/patriChiril/An-Annotated-Corpus-for-Sexism-Detection-in-French-Tweets,Yes,Annotated Corpus for Sexism Detection in French Tweets,1,1,Twitter,Sexism,French,XX.10.2017 - XX.05.2018,XX.10.2017 - XX.05.2018,N/A,N/A,"The corpus contains French tweets collected between October 2017 and May 2018. In order to collect sexist and non sexist tweets, the authors followed Anzovino et al. (2018) approach using:

• a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc.,

• the names of women/men potentially victims or guilty of sexism (mainly politicians): Ségolène Royal, Nadine Morano, Theresa May, Hillary Clinton, Dominique Strauss-Kahn, Nicolas Hulot, etc.,

• specific hashtags to collect stories of sexism experiences: #balancetonporc, #sexisme, #sex- iste, #SexismeOrdinaire, #Ensemble ContreLeSexisme, #payetashnek, #payetontaf, etc..

Thus, they collected around 115,000 tweets among which about 30,000 contain the specific hashtags.",Not discussed,,manual,Yes,"The authors used a set of 150 tweets to define the annotation guidelines. Given a tweet, annotation consists in assigning it one of the following three categories: (i) Sexist content (it can be either direct, descriptive, or reporting. The first two are real sexist messages but not the last one as reporting tweets must not be considered as sexist in a context of moderation) (ii) Non sexist, (iii) No decision.

300 tweets have been used for the training of 5 annotators and then removed from the corpus. Then, 1,000 tweets have been annotated by all annotators and the average Cohen's Kappa is 0.72 for sexist content/non sexist/no decision categories and 0.71 for di- rect/descriptive/reporting/non sexist/no decision categories which means a strong agreement. For these 1,000 tweets, the final labels have been assigned according to a majority vote.

Finally, a total of 11,834 tweets have been annotated according to the guidelines after removing the tweets annotated as ""no decision"".",Not discussed,,11834,6255,1532,Five annotators: they are master degree's students (3 female and 2 male) in Communication and Gender,Yes,The guidelines include definitions/explanations of each label and examples for them. Linguistic clues in the examples are underlined.,Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Gender (sexism),gender,Yes,Mainly women and girls,No,,,No,Human
290,No,"(Salawu et al., 2021)",A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),UK,"In this paper, the authors introduce a new English Twitter-based dataset for cyberbullying detection and online abuse. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, trolling, profanity, sarcasm, threat, porn and exclusion. The authors recruited a pool of 17 annotators to perform fine-grained annotation on the dataset with each tweet annotated by three annotators. All the annotators are high school educated and frequent users of social media. Inter-rater agreement for the dataset as measured by Krippendorff's Alpha is 0.67. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results.",Yes,1,06.08.2021,14.10.2022,Yes,Yes,"Cyberbullying has been defined as an aggressive and intentional act repeatedly carried out using electronic means against a victim that cannot easily defend him or herself (Smith et al., 2008). Online abuse by contrast can refer to a wide range of behaviours that may be considered offensive by the parties to which it is directed to (Sambaraju and McVittie, 2020).",Yes,"(multi-label) annotation of elements in abusive and offensive content: Bullying, Insult, Profanity, Sarcasm, Threat, Exclusion, Porn, Spam",Yes,Other,Bitbucket,https://bitbucket.org/ssalawu/cyberbullying-twitter,Yes,"cyberbullying and offensive language Twitter dataset (C);

existing datasets: Davidson (D) dataset, Kaggle (K) datasets",3,1,Twitter,"Cyberbullying, Abusiveness",English,N/A,late 2019,N/A,N/A,"Rather than indiscriminately mining Twitter feeds, the authors executed a series of searches formulated to return tweets with a high probability of containing the various types of offensive content of interest. For insulting and profane tweets, they queried Twitter using the 15 most frequently used profane terms on Twitter as identified by Wang et al. (2014). These are: fick, sh\*t, a\*s, bi\*ch, ni\*\*a, hell, wh°re, d\*ck, p\*ss, putty, sI\*t, p\*ta, t\*t, damn, f\*g, c\*nt, c\*m, c\*ck, bI\*wj\*b, retard. To retrieve tweets containing sarcasm, they used a strategy based on the work of Rajadesingan et al. (2015) which discovered that sarcastic tweets often include #sarcasm and #not hashtags to make it evident that sarcasm was the intention. They found #sarcasm more relevant and therefore queried Twitter using this hashtag. To discover prospective query terms for threatening tweets, they reviewed a random sample of 5000 tweets retrieved via Twitter's Streaming API and identified the following hashtags as potential query terms: #die#killyou#rape#chink, #muslim, #FightAfterTheFight and #cops. These hashtags were then used as the initial seed in a snowballing technique to discover other relevant hashtags. This was done by querying Twitter using the hashtags and inspecting the returned tweets for violence-related hashtags. The following additional hashtags were subsequently discovered through this process: #killallblacks#killallcrackers#blm#blacklivesmatter#alllivesmatter#bluelivesmatter#killchinese#bustyourhead#f\*ckyouup#killallwhites#maga#killallniggasand #nigger.

From the 5000 tweets seed sample, the authors classified six tweets as relating to social exclusion and from them identified the following hashtags for use as query terms: #alone, #idontlikeyou and #stayinyourlane. Due to the low number of tweets returned for these hashtags, they also extracted the replies associated with the returned tweets and discovered the following additional hashtags #notinvited, #dontcometomyparty, and #thereisareasonwhy which were all subsequently used as additional query terms. Rather than excluding retweets when querying as is common practice amongst researchers, the process initially extracted original tweets and retweets and then selected only one of a tweet and its retweets if they were all present in the results. This ensured relevant content was not discarded in situations where the original tweet was not included in the results returned, but retweets were. The final dataset contained 62,587 tweets published in late 2019.",Not discussed,,manual,Yes,"Using personal contacts, the authors recruited a pool of 17 annotators. All annotators were provided with preliminary information about cyberbullying including news articles and video reports, documentaries and YouTube videos as well as detailed information about the labelling task.

Since the presence of many profane words can be automatically detected, a program was written to label the tweets for profane terms based on the 15 profane words used as query terms and the Google swear words list. The profanity-labelled tweets were then provided to the annotators to alleviate this aspect of the labelling task. Each tweet was labelled by three different annotators from different ethnic/racial backgrounds, gender and countries of residence. This was done to control for annotators cultural and gender bias.",Not discussed,,"62,587",10-fold cross-validation,10-fold cross-validation,"17 annotators. The annotators are from different ethnic/racial backgrounds (i.e., Caucasian, African, Asian, Arabian) and reside in different countries (i.e., US, UK, Canada, Australia, Saudi Arabia, India, Pakistan, Nigeria and Ghana). Additionally, their self-reported online social networking habits met our definition of an active social media user. Due to the offensive nature of the tweets and the need to protect young people from such content while maintaining an annotator profile close to the typical age of the senders and recipients of the tweets, the annotators were aged 18- 35 years.",Yes,"All annotators were provided with preliminary information about cyberbullying including news articles and video reports, documentaries and YouTube videos as well as detailed information about the labelling task.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
291,No,"(Modha et al., 2020)",Detecting and visualizing hate speech in social media: A cyber Watchdog for surveillance,Expert Systems with Applications,"India, Germany","This paper presents an approach to detect and visualize online aggression, a special case of hate speech, over social media. Aggression is categorized into overtly aggressive (OAG), covertly aggressive (CAG), and non-aggressive labels (NAG). The authors designed a user interface based on a web browser plugin over Facebook and Twitter to visualize the aggressive comments posted on the Social media user’s timelines. They reported the results on a newly created dataset of user comments posted on Facebook and Twitter using the proposed plugins and the standard Trolling Aggression Cyberbullying 2018 (TRAC) dataset in English and code-mixed Hindi. Various classifiers like Support Vector Machine (SVM), Logistic regression, deep learning model based on Convolution Neural Network (CNN), Attention-based model, and the recently proposed BERT pre-trained language model by Google AI, have been used for aggression classification. The weighted F1-score of around 0.64 and 0.62 is achieved on TRAC Facebook English and Hindi datasets while on Twitter English and Hindi datasets, the weighted F1-score is 0.58 and 0.50, respectively.",Yes,33,15.12.2020,14.10.2022,Yes,Yes,"Aggression, a special case of hate speech, over social media.
Verbal aggression could be understood as any linguistic behavior which intends to damage the social identity of the target person and lower their status, and prestige (Ritesh et al., 2018bCulpeper, 2011). Any speech or text in which aggression is overtly expressed either through the use of specific kinds of lexical items or lexical features which are considered aggressive and or certain syntactic structures is overt aggression (Ritesh et al., 2018b). Covertly aggression contains an indirect attack against the victim and is framed as a polite or sarcastic expression and might not contain any abusive offensive controversial word. Non-aggressive posts do not contain any aggressive content.",Yes,"Aggression is categorized into overtly aggressive (OAG), covertly aggressive (CAG), and non-aggressive labels (NAG).",No,,,,,"Comments which are posted on political leaders’ Facebook pages and Twitter posts;

Exsiting dataset: Trolling Aggression and Cyberbullying (TRAC) dataset",2,1,"Facebook, Twitter","Aggressiveness, Hate Speech",Hindi-English,N/A,N/A,N/A,N/A,"The authors deployed their Facebook /Twitter plugin and extracted comments from the popular worldwide social media profiles, such as president Trumps' timeline.",Not discussed,,manual,Not discussed,,Not discussed,,"15,001 (TRAC), 652 (Facebook/Twitter plugin)",15001 (TRAC En or Hindi);,"970 (TRAC Facebook test, En)1194 (TRAC Twitter test, En);",N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
292,No,"(Mohapatra et al., 2021)",Automatic Hate Speech Detection in English-Odia Code Mixed Social Media Data Using Machine Learning Techniques,Applied Sciences,"India, Taiwan","This paper proposes a hate speech detection model by means of machine learning and text mining feature extraction techniques. In this study, the authors collected the hate speech of English-Odia code mixed data from a Facebook public page and manually organized them into three classes. In order to build binary and ternary datasets, the data are further converted into binary classes. The modeling of hate speech employs the combination of a machine learning algorithm and features extraction. Support vector machine (SVM), naïve Bayes (NB) and random forest (RF) models were trained using the whole dataset, with the extracted feature based on word unigram, bigram, trigram, combined n-grams, term frequency-inverse document frequency (TF-IDF), combined n-grams weighted by TF-IDF and word2vec for both the datasets. Using the two datasets, the authors developed two kinds of models with each feature—binary models and ternary models. The models based on SVM with word2vec achieved better performance than the NB and RF models for both the binary and ternary categories. The result reveals that the ternary models achieved less confusion between hate and non-hate speech than the binary models.",Yes,5,15.09.2021,14.10.2022,Yes,Yes,"Most of the research on social media defines HS (Hate Speech) as a language that attacks or diminishes, and incites violence or hate against groups, based on specific characteristics, such as race, ethnic origin, religious affiliation, political views, physical appearance, gender, etc. The definition points out that HS language incites violence or hatred against groups.",Yes,Offensive (OFS); Hate (HS) 1193; Neither (OK),No,,,,,Odia HS dataset,1,1,Facebook,Hate Speech,Odia,N/A,XX.04.2019 - XX.04.2020,N/A,N/A,"The Odia-English textual data are the posts and comments gathered from different categories of popular public pages on the Facebook platform, because Facebook's privacy policy does not allow access to the public contents of a private page. The sampling criteria and metrics used in this paper for selecting public pages on social media platforms are listed below:

• The number of followers and likes has to be greater than 50,000, because such criteria allows for more active public pages to be included in categories;

• Pages that post news or hot issues on political, ethnicity, religious, or gender issues at least once every two days;

• Pages that use the Odia language frequently for posts and comments;

• Pages that published more than 500 posts from April 2018 to April 2019.

This paper collects posts and comments from public pages that are in the listed categories (Categories Name: News media and broadcasting pagesBloggers' and journalists' pagesReligious media and religious group pagesPolitical party, politician and government official's pagesPublic figure like artists' and authors' pagesActivists', general or interest community pages) with larger number of followers and likes than the other pages in the category. All of the selected posts and comments were posted from April 2019 to April 2020, which covers the political and socio-economic changes experienced by the country in different aspects in a year, and, in that period, the usage of social media-especially Facebook-in the country has increased significantly. In addition, this paper collects the keywords for filtering the collected Odia text data and the annotation process of the posts and comments. These keywords are deemed as offensive words or the indicater of offensive or hate speech text, and the words used to identify a target group. This study focuses on political, ethnic, religious and gendered target groups.

The authors collected 27,422 posts. The total number of unique post and comment filtered were 27,162.",Not discussed,,manual,Yes,"The annotation process needs to label a post or comment in order to build a hate speech dataset. The paper uses a simple random sampling technique to select the posts and comments to be annotated. The technique allows all of the filtered posts and comments of each page to have an equal chance of being annotated. The annotation is conducted based on the instruction guidelines provided by the researcher.

There were four annotators in total and everyone labelled the posts and comments based on the same guidelines. Three of the annotators were given 500 similar instances and 1000 unique posts and comments. The same instances were used to evaluate the consistency of the annotations among the annotators. The fourth annotator was the researcher who oversaw the whole process of the annotation and annotated 1500 unique posts and comments. 500 of the same posts and comments were annotated by the three annotators and the researcher decided the final class using the majority vote's method.",Yes,"The authors utilized a simple random sampling technique to select the posts and comments to be annotated. This technique provided an equal chance for all of the filtered posts and comments to be annotated. Because of the time limitation of the research and the resources of the annotation process for filtered posts and comments, 5000 posts and comments were selected to be annotated.",5000,5-Fold cross-validation,5-Fold cross-validation,Three annotators + the fourth annotator who was the researcher and oversaw the whole process,Yes,The annotation is conducted based on the instruction guidelines provided by the researcher.,Not discussed,,General,N/A,Targeted,"race, religion, political, body, gender, other",Yes,No,,,No,,No,,,No,Human
293,No,"(Asif et al., 2020)",Sentiment analysis of extremism in social media from textual information,Telematics and Informatics,"Pakistan, Saudi Arabia","This research focuses on the sentimental analysis of social media multilingual textual data to discover the intensity of the sentiments of extremism. The study classifies the incorporated textual views into any of four categories, including high extreme, low extreme, moderate, and neutral, based on their level of extremism. Initially, a multilingual lexicon with the intensity weights is created. This lexicon is validated from domain experts and it attains 88% accuracy for validation. Subsequently, Multinomial Naïve Bayes and Linear Support Vector Classifier algorithms are employed for classification purposes. Overall, on the underlying multilingual dataset, Linear Support Vector Classifier out-performs with an accuracy of 82%.",Yes,62,01.05.2020,14.10.2022,Yes,Yes,"According to Davies, extremism is provoked ""when you do not allow for a different point of viewwhen you hold your own views as being quite exclusive and when you don't allow for the possibility of difference"" (Davies, 2020). The author further added to this definition that "" when you want to impose your view on others while using violence if necessary."" Extremism is also defined as "" activities (beliefs, attitudes, feelings, actions, strategies) of a character far removed from the ordinary"" (Coleman and Bartoli, 2003).

…These views also include foul, overtly sexiest and racist language and threats termed as extreme speech (Johnson, 2018).",Yes,"four classes: Moderate, Neutral, Highly Extreme, Low Extreme",No,,,,,The dataset,1,1,Facebook,Extremism,"Urdu, English",N/A,the past eight to nine years,N/A,N/A,"By using Data Miner Scraper, posts and comments are collected from Facebook news pages including ARY news, Ptv news, Dawn, The News, Samaa, Express, Dunya news, and Geo.

The fans comment on these news pages on different news and show their sentiments. Data miner scrapper has inbuilt recipes to scrape data from different websites. Moreover, it also provides the facility of creating recipes. For Facebook, data miner scraper creates a recipe that scrapes posts and its comments along with URL address, date of posts, and comments writer. Subsequently, with the help of field experts, some initial lexicons of every category are created. These lexicons are then used to search for posts and comments related to the interest of this study and for data collection purposes. Data is collected in the spreadsheets of the past eight to nine years. For all news pages, different spreadsheets are created for posts and comments.",Not discussed,,automated,Yes,"Sentiment lexicons are generated by undergoing the following steps:

• Took around 1800 positive/negative English lexicon lists from GitHub source for initialization.

• Assigned weights to the lexicons according to the extremism domain.

• Updated the list by adding domain-specific (extreme) lexicons manually along with their weights.

• Constructed the Urdu lexicons list related to extreme and moderate sentiment.

• Updated the lexicons with Roman Urdu, English, and Urdu lexicons.

Semantic orientation refers to a proportion of subjectivity and feeling in the content. It normally catches an evaluative factor (positive or negative) and intensity or quality (the degree to which the word, expression, sentence, or record being referred to is positive or negative).

In the proposed system, weight are assigned to the lexicons for depicting their strength. Scores are assigned manually according to the extreme sentiment of lexicons. The lexicons are weighted between - 5 to + 5. This lexicon based system classifies text among four classes of extremism, i.e., high extreme, low extreme, neutral, and moderate. The lexicons that show sentiment related to high extreme are given the highest weight, i.e., ""terrorists"" is assigned with - 5 score, ""attack"" is assigned with - 4 score. The lexicons depicting less extreme are assigned less score, for instance, -2 is assigned to ""injured"" lexicon and similarly positive scores are assigned to moderate and neutral class lexicons.",Not discussed,,29497,15597,3900,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"race, gender",Yes,No,,,No,,No,,,No,Human
294,No,"(Dinan et al., 2020)",Multi-Dimensional Gender Bias Classification,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),France,"In this work, the authors propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, they automatically annotate eight large scale datasets with gender information. In addition, they collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables researchers to train better and more fine-grained gender bias classifiers. The authors show their classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",Yes,46,16.11.2020,14.10.2022,Yes,Yes,"Gender bias. ""… statistical differences among gender labels (i.e., gender bias).""",Yes,"Dimensions of Gender Bias: bias when speaking ABOUT someone, bias when speaking TO someone, and bias from speaking AS someone.",Yes,Other,paperwithcode.com,https://paperswithcode.com/dataset/md-gender,Yes,"MD-GENDER (new evaluation dataset);

Existing Datasets that are annotated in this study: Wikipedia, Funpedia, Wizard of Wikipedia, ImageChat, Yelp, ConvAI2, OpenSubtitiles, LIGHT.",9,1,"Wikipedia, Funpedia, Yelp, OpenSubtitles, chi-chats",Gender Bias,English,N/A,N/A,N/A,N/A,"The authors collect a specialized corpus, MDGENDER, which acts as a gold-labeled dataset for the masculine and feminine classes.

First, they collect conversations between two speakers. Each speaker is provided with a persona description containing gender information, then tasked with adopting that persona and having a conversation. They are also provided with small sections of a biography from Wikipedia as the conversation topic. Using personas biographies to frame the conversation encourages crowdworkers to discuss ABOUT/TO/AS gender information. To ensure there is ABOUT/TO/AS gender information contained in each utterance, the authors perform a second pass over the dataset.

In this next phase, they ask a second set of annotators to rewrite each utterance to make it very clear that they are speaking ABOUT a man or a woman, speaking AS a man or a woman, and speaking TO a man or a woman. For example, given the utterance Hey, how are you today? I just got off work, a valid rewrite to make the utterance ABOUT a woman could be: Hey, what's up? I went for a coffee with my friend and her dog after work as the her indicates a woman. Annotators are additionally asked to label how confident they are that someone else could predict the given gender label, allowing for flexibility between explicit genderedness (like the use of he or she) and statistical genderedness.

This dataset was collected using crowdworkers from Amazon's Mechanical Turk.",Yes,For privacy reasons the authors do not associate the self-reported gender of the annotator with the labeled examples in the dataset and only report these statistics in aggregate.,automated,Yes,"The authors develop classifiers that decompose gender bias over sentences into semantic and/or pragmatic dimensions (about/to/as), including gender information that (i) falls outside the male-female binary, (ii) can be contextually determined, and (iii) is statistically as opposed to explicitly gendered.

Many of the annotated datasets contain cases where the ABOUT, AS, TO labels are not provided (i.e. unknown). For example, often they do not know the gender of the content creator for Wikipedia (i.e., the As dimension is unknown). To retain such examples for training, they either impute the gender label or provide a label at random. They apply the imputation strategy for data for which the ABOUT label is unknown using a classifier trained only on other Wikipedia data for which this label is provided. Data without a TO or AS label was assigned one at random, choosing between masculine and feminine with equal probability. From epoch to epoch, they switch these arbitrarily assigned labels so that the model learns to label unknown examples as masculine or feminine with roughly equal probability. This label flipping allows us to retain greater quantities of data by preserving un- known samples. During training, they balance the data across the masculine, feminine, and neutral classes by up-sampling classes with fewer examples.",Not discussed,,"~2345 (MDGENDER); 104K~12M (8 existing datasets, each)","104K~12M (8 existing datasets, each)",~2345 (MDGENDER);,"All workers are English-speaking and located in the United States. During the ""re-write phase"" crowdworkers were asked to provide their own gender identity if they were willing. Over two thirds of annotators identified as men.",Not discussed,,Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Gender,gender,No,,No,,,No,Human
295,No,"(ElSherief et al., 2021)",Latent Hatred: A Benchmark for Understanding Implicit Hate Speech,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,USA,"Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. The authors present systematic analyses of the dataset using contemporary baselines to detect and explain implicit hate speech, and they discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.",Yes,19,07.11.2021,14.10.2022,Yes,Yes,"Implicit hate speech is defined by coded or in- direct language that disparages a person or group on the basis of protected characteristics like race, gender, and cultural identity.",Yes,"high-level labels, explicit hate, implicit hate, or not hate.  Implicit hate category labels: Grievance, Incitement, Inferiority, Irony, Stereotypical, Threatening, Other",Yes,Github,,https://github.com/SALT-NLP/implicit-hate,Yes,Implicit Hate Speech,1,1,Twitter,Implicit Hate Speech,English,N/A,01.01.2015 - 31.12.2017,N/A,N/A,"The authors focus on the eight largest ideological clusters of U.S. hate groups as given by the SPLC (2019) report. These ideological classes are Black Separatist (27.1%), White Nationalist (16.4%), Neo-Nazi (6.2%), Anti-Muslim (8.9%), Racist Skinhead (5.1%), Ku Klux Klan (5.0%), Anti-LGBT (7.4%), and Anti-Immigrant (2.12%). Detailed background and discussion on each hate ideology can be found at the the SPLC Extremist Files page (SPLC, 2020).

They matched all SPLC hate groups with their corresponding Twitter accounts using the account names and bios. Then, for each ideological cluster above, they selected the three hate group accounts with the most followers, since these were likely to be the most visible and engaged. They collected all tweets, retweets, and replies from the timelines of the selected hate groups between January 1, 2015 and December 31, 2017, for a total of 4,748,226 tweets.

Finally, after filtering out tweets marked as not hate, there were 4,153 labeled implicit hate tweets remaining. Extreme class imbalance may challenge implicit hate classifiers. To address this disparity, the authors expand the minority classes, both with bootstrapping and out-of-domain samples. For bootstrapping, they trained a 6-way BERT classifier on the 4,153 implicit hate labels and ran it on 364,300 unlabeled tweets from the corpus. Then they randomly sampled 1,800 tweets for each of the three minority classes according to the classifications inferiority, irony, and threatening. Finally, they augmented this expansion with out-of-domain (OOD) samples from Kennedy et al. (2018) and Sap et al. (2020). By drawing both from OOD and bootstrapped in-domain samples, they sought to balance two key limitations: (1) bootstrapped samples may be inherently easier, while (2) OOD samples contain artifacts that allow models to benefit from spurious correlations. The expert annotators labeled this data, and by adding the minority labels from this process, they improved the class balance for a total of 6,346 implicit tweets.",Not discussed,,manual,Yes,"Stage 1: High Level Categorization

Amazon Mechanical Turk (MTurk) annotators completed the high-level labeling task. The authors provided them with a definition of hate speech (Twitter, 2021) and examples of explicit, implicit, and non-hateful content, and required them to pass a short five-question qualification check for understanding with a score of at least 90% in accordance with crowdsourcing standards. Three workers labeled each tweet, and they reached majority agreement for 95.3% of tweets, with perfect agreement on 45.6% of the data. Using the majority vote, the authors obtained consensus labels for 19,112 labeled tweets in total.

Stage 2: Fine-Grained Implicit Hate

To promote a more nuanced understanding of the 4,909 implicit hate tweets, the authors hired three research assistants to be the expert annotators. The authors trained them over multiple sessions by walking them through seven small pilot batches and resolving disagreements after each test until they reached moderate agreement. On the next round of 150 tweets, their independent annotations reached a Fleiss' Kappa of 0.61. Each annotator then continued labeling an independent partition of the data. Halfway through this process, the authors ran another attention check with 150 tweets and found that agreement remained consistent with a Fleiss' Kappa of 0.55. Finally, after filtering out tweets marked as not hate, there were 4,153 labeled implicit hate tweets remaining.

The expert annotators also labeled the expanded data, after which there are a total of 6,346 implicit tweets.

For each of the 6,346 implicit hate tweets, two separate annotators provided the message's target demographic group and its implied statement in free-text format. Implied statements were formatted as Hearst-like patterns of the form &lt;target{do, are, commit} &lt;predicate;, where &lt;targetmight be phrases such as immigrants, black folks.",Yes,"Seeking a representative sample, they identified group-specific salient content from each ideology by performing part of speech (POS) tagging on each tweet. Then they computed the log odds ratio with informative Dirichlet prior for each noun, hashtag, and adjective to identify the top 25 words per ideology. After filtering for tweets that contained one of the salient keywords, they ran the 3-way HateSonar classifier of Davidson et al. to remove content that was likely to be explicitly hateful. Specifically, they removed all tweets that were classified as offensive, and then ran a final sweep over the neutral and hate categories, removing tweets that contained any explicit keyword found in NoSwear or Hatebase.","6,346 implicit hate tweets",75% (~4760),12.5% (~793),Amazon Mechanical Turk workers in High Level Categorization; Three research assistants (expert annotators) in Fine-Grained Implicit Hate.,Yes,"Instructions and examples provided to Amazon Mechanical Turk workers included the definitions of hate speech, descriptions of explicit/implicit/not hate speech, and examples.",Yes,They paid annotators a fair wage above the federal minimum. The total annotation cost for Stage 1 and 2 was $15k.,General,N/A,Targeted,"race, gender",Yes,No,,,No,,No,,,No,Human
296,No,"(Maity et al., 2022)",A Multitask Multimodal Framework for Sentiment and Emotion-Aided Cyberbullying Detection,IEEE Internet Computing,India,"This article is the first attempt in investigating the role of sentiment and emotion information for identifying cyberbullying in the Indian scenario. From Twitter, a benchmark Hind–English code-mixed corpus called BullySentEmo has been developed as there was no dataset available labeled with bully, sentiment, and emotion. The developed dataset consists of both the modalities, tweet-text and emoji. In India, the majority of communication on different social media platforms is based on Hindi and English, and language switching is a common practice in digital communication. A multitask multimodal framework called MT-MM-Bert+VecMap based on BERT and VecMap embedding schemes with emoji modality, has been developed. The proposed multitask-multimodal framework outperforms all the single task and unimodal baselines with the highest accuracy values of 82.05(+/- 1.36)%, 77.87(+/- 1.93)%, and 58.05(+/-2.78)% for the cyberbully detection task, sentiment analysis task, and emotion recognition task, respectively.",Yes,2,11.03.2022,14.10.2022,Yes,Yes,"Cyberbullying is described as the serious, intentional, and repetitive acts of a person's cruelty toward others using various digital technologies.",Yes,"the sentiment class (positive/neutral/negative) and the emotion class based on six Eckman's emotion categories (Happiness, Sadness, Fear, Surprise, Anger, Disgust, and others).",No,,,,,BullySentEmo dataset,1,1,Twitter,Cyberbullying,Hindi-English,N/A,N/A,N/A,N/A,"The authors further annotated the CDC dataset proposed in Ghosh et al.’s work with three sentiment classes and seven emotion classes.

To create a BullySentEmo dataset, they added additional 1022 tweets to the old standalone cyberbully dataset, bringing the total number of tweets to 6084. These additional tweets have been scraped from Twitter with the help of the Twitter Search API based on some keywords related to cyberbullying, such as Chuthiya, Rendi, and Kamini. All these keywords are written in Hinglish and are frequently used in India for cyberbullying. The standalone Hindi-English code-mixed cyberbully dataset, which we have considered for further annotation with three sentiment classes and seven emotion classes, contains a total of 5062 tweets where 2456 tweets were marked as nonbully and the remaining 2606 tweets were marked as bully. The authors keep 578 nonbully tweets and 444 bully tweets out of the newly added 1022 tweets to maintain the balanced nature of the new dataset.",Not discussed,,manual,Yes,"Three annotators having proficient linguistic background in both Hindi and English were involved in data annotation. The authors have given them detailed instructions of annotations with examples and closely monitor the annotation process. They have worked in isolation to avoid any biasness. For each tweet, annotators had tagged two labels, one for the sentiment class (positive/neutral/negative) and another for the emotion class based on six Eckman's emotion categories (Happiness, Sadness, Fear, Surprise, Anger, Disgust, and others). For finalizing the annotation label of each tweet, the authors use a majority vote procedure to resolve conflicts between annotators. Using Cohen's Kappa score the interannotator agreement has been calculated to verify the quality of annotation.",Not discussed,,6084,10-fold cross-validation,10-fold cross-validation,The three annotators have proficient linguistic background in both Hindi and English.,Yes,The authors have given them detailed instructions of annotations with examples and closely monitor the annotation process.,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
298,No,"(Albadi et al., 2018)",Are they Our Brothers? Analysis and Detection of Religious Hate Speech in the Arabic Twittersphere,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),"Saudi Arabia, USA","In this work, the authors describe how they created the first publicly available Arabic dataset annotated for the task of religious hate speech detection and the first Arabic lexicon consisting of terms commonly found in religious discussions along with scores representing their polarity and strength. They then developed various classification models using lexicon-based, n-gram-based, and deep-learning-based approaches. A detailed comparison of the performance of different models on a completely new unseen dataset is then presented. They find that a simple Recurrent Neural Network (RNN) architecture with Gated Recurrent Units (GRU) and pre-trained word embeddings can adequately detect religious hate speech with 0.84 Area Under the Receiver Operating Characteristic curve (AUROC).",Yes,129,28.08.2018,14.10.2022,Yes,Yes,"Religious hate speech. We define as a speech that is insulting, offensive, or hurtful and is intended to incite hate, discrimination, or violence against an individual or a group of people on the basis of religious beliefs or lack of any religious beliefs.",Yes,"religious affiliations (if classified as religious hate speech): Muslims, Jews, Christians, Atheists, Sunnis, Shia, and/or other.",Yes,Github,,https://github.com/nuhaalbadi/Arabic_hatespeech,Yes,Religious Hate Speech Detection for Arabic Tweets,1,1,Twitter,Religious Hate Speech,Arabic,"XX.11.2017 - XX.11.2017, XX.01.2018 - XX.01.2018","XX.11.2017-XX.11.2017, XX.01.2018-XX.01.2018",N/A,N/A,"In November 2017, using the Twitter's search API, the authors collected 6000 Arabic tweets, 1000 for each of the six religious groups. They used this collection of tweets as the training dataset. Due to the unavailability of a hate lexicon and to ensure unbiased data collection process, they included in the search query only impartial terms that refer to a religion name or the people practicing that religion. Specifically, they did not use any religious slurs that are used to insult people of a particular religious affiliation. For example, when collecting tweets related to Islam, they used the Arabic equivalent of the keywords: Islam, Muslim, and Muslims. To minimize redundancy in the training dataset, they collected only original tweets by excluding retweets from their queries. They also didn't collect any reply tweets to ensure that the tweets gleaned are self-contained to maximize the ability of crowdsourced workers to make reliable judgments.

In January 2018, they collected another set of 600 tweets, 100 for each of the six religious groups, for the testing dataset. They employed the same methodology for collecting this set as they used for collecting the training set. They intentionally collected a completely new unseen data, two months after they had collected the training data, to ensure reliable classification results and that the developed classifiers can generalize well to new data.",Not discussed,,manual,Yes,"The authors designed a task on CrowdFlower', a crowdsourcing platform, to obtain annotations for the training and testing datasets. They allowed only Arabic speaking annotators with IP addresses from one of the Arabic-speaking Middle Eastern countries to access the task. Before starting the annotation process, annotators were provided with the authors’ definition and some examples of religious hate speech. To reduce subjective biases, annotators were specifically asked not to allow their personal beliefs or religious affiliation influence their judgment.

The authors asked annotators to read a tweet carefully before deciding if the tweet: a) included an instance of religious hate speech (we refer to this as hate class)b) didn't contain any instances of religious hate speech (we refer to this as -hate class)c) was unclear or unrelated to religious hate speech. If annotators decided that the tweet contained an instance of hate speech, they were asked to select one or more religious groups that the tweet was being hateful to. The religious affiliations provided in the second question were as follows: Muslims, Jews, Christians, Atheists, Sunnis, Shia, and/or other.

To ensure high quality annotations, the authors created a set of 100 test questions to be used in the Quiz Mode and Work Mode. In the Quiz Mode, annotators were asked a set of four test questionsonly those who scored an accuracy of 70% or more were able to qualify as annotators for our task. In the Work Mode, one test question was injected per page of work, and annotators who failed to maintain an accuracy of at least 70% throughout the task were disqualified and excluded from the task. Each page of work contained five tweets, one of which was a test question, and annotators didn't know which of the five tweets was the test question. For each tweet, we collected three trusted judgments from three different annotators. Untrusted judgments, from annotators whose accuracies have fallen down 70%, were excluded from our final results.",Not discussed,,6136,5569,567,CrowdFlower workers. Only Arabic speaking annotators with IP addresses from one of the Arabic-speaking Middle Eastern countries can access the task.,Yes,"Before starting the annotation process, annotators were provided with the authors’ definition and some examples of religious hate speech.",Not discussed,,Specific,religion,Targeted,religion,Yes,Yes,religion,religion,Yes,"Muslims, Jews, Christians, Atheists, Sunnis, Shia, other.",No,,,No,Human
299,No,"(Castillo et al., 2019)",Helping Students Detecting Cyberbullying Vocabulary in Internet with Web Mining Techniques,2019 International Conference on Inclusive Technologies and Education (CONTIE),Mexico,"This article presents a model for the analysis of data on the Internet, using Web mining, to find knowledge about large amounts of information in cyberspace. To test the proposed method, Web pages on Cyberbullying were analyzed as a case study. The procedure integrates a Web Scraper to locate and download information from the Internet, to recover the vocabulary are used techniques of Natural Language Processing (tokenization, cleaning of words without meaning, frequency of term, inverse frequency of the document, synonyms, stemming methods). To obtain knowledge, a dataset was constructed using semantic ontologies to define the predictive variables of Cyberbullying and supervised learning to define the variable to be predicted. To evaluate the efficiency of the model, algorithms of machine learning, AdaBoost and Neural Network were used. The results reveal a percentage of 97% accuracy in the detection of Cyberbullying vocabulary, which was approved through crossvalidation, achieving a time saving of 581% with parallel processing, compared to sequential processing.",Yes,1,30.10.2019,14.10.2022,Yes,Yes,"Castro point out that the Cyberbullying is a problem of harassment through digital media that is propagated in school and home (not exclusively); from another perspective Ortega & Gonzalez indicate that it is a form of violence suffered by children and adolescents through the use of Information and Communication Technologies (ICT) to harass and intimidate an individual through personal attacks, disclosure of confidential information or false.",No,,No,,,,,The dataset,1,1,N/A,Cyberbullying,English,N/A,N/A,N/A,N/A,"(1) It is proceeded to search web pages of Cyberbullying by the Web Scraper and 1333 URLs were selected, making a performed copy to hard disc (phase 1 and phase 2).
(2) The vocabulary was obtained through the techniques of LP and Semantic Web, specifying the number of recovered words and execution time, comparing the sequential processing and genetic strategy (parallel processing) (phase 3).
(3) Once the vocabulary is gotten, the features of Cyberbullying (Categorization of the linguistic corpus of cyberbullying) are established, through the construction of semantic ontologies through graphs (phase 4).
(4) In the precision testing a supervised learning was used with algorithms with greater similarity to the related work (AdaBoost and Neuronal Network).
(5) Completed the tests an analysis of the results was made pointing the algorithm that showed the best results in the accuracy's percentage and the conditions for the vocabulary of Cyberbullying detection.",Not discussed,,N/A,Not discussed,,Not discussed,,1333 web pages,70% (~933),30% (~400),N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
300,No,"(Maity & Saha, 2021)",BERT-Capsule Model for Cyberbullying Detection in Code-Mixed Indian Languages,Natural Language Processing and Information Systems,India,"In this work, the authors have created a benchmark corpus for cyberbullying detection against children and women in Hindi-English code-mixed language. Both these languages are the medium of communication for a large majority of India, and mixing of languages is widespread in day-to-day communication. They have developed a model based on BERT, CNN along with GRU and capsule networks. Different conventional machine learning models (SVM, LR, NB, RF) and deep neural network based models (CNN, LSTM) are also evaluated on the developed dataset as baselines. The model (BERT+CNN+GRU+Capsule) outperforms the baselines with overall accuracy, precision, recall and F1-measure values of 79.28%, 78.67%, 81.99% and 80.30%, respectively.",Yes,2,20.06.2021,14.10.2022,Yes,Yes,"Cyberbullying is defined through malicious tweets, texts or other social media posts via various digital technologies as the serious, intentional and repeated actions of a person's cruelty towards others.",No,,No,,,,,Hindi-English code-mixed annotated dataset for cyberbullying detection,1,1,Twitter,Cyberbullying,Hindi-English,XX.07.2020 - XX.11.2020,XX.07.2020 - XX.11.2020,N/A,N/A,"With the help of Twitter Search API, the authors collected tweets from Twitter. They have scraped approximately 90K raw tweets between July 2020 to November 2020 based on specific hashtags and keywords related to women's attacks like MeToo, r\*ndi, JusticeForSushantSinghRajput, nepotism, IndiagainstAbuse, AliaBhatt, bitch etc.",Not discussed,,manual,Yes,"After the authors preprocessed the raw tweets, two human annotators carried out the data annotation task. For annotation, the authors follow the guidelines used in Hee et al. To check the quality of annotation carried out by two annotators, they have calculated the inter-annotator agreement (IA) using Cohen's Kappa coefficient.",Not discussed,,5562,75% (~4172),15% (~834),Two human annotators who have linguistic background and proficiency in both Hindi and English,Yes,The authors follow the guidelines used in Hee et al.,Not discussed,,Specific,gender,Non-targeted,N/A,Yes,Yes,Gender,gender,Yes,women,No,,,No,Human
301,No,"(Chen et al., 2017)",Presenting a labelled dataset for real-time detection of abusive user posts,Proceedings of the International Conference on Web Intelligence,Ireland,"In this work, the authors present a dataset of user comments, using crowdsourcing for labelling. Since abusive content can be ambiguous and subjective to the individual reader, they propose an aggregated mechanism for assessing different opinions from different labellers. In addition, instead of the typical binary categories of abusive or not, they introduce a third class of 'undecided' to capture the real life scenario of instances that are neither blatantly abusive nor clearly harmless. They have performed preliminary experiments on this dataset using best practice techniques in text classification. Finally, they have evaluated the detection performance of various feature groups, namely syntactic, semantic and context-based features. Results show these features can increase the classifier performance by 18% in detection of abusive content.",Yes,17,23.08.2017,14.10.2022,Yes,Yes,"For the purposes of this work, we consider abusive content as content that has negative consequences, through the apparent deliberate targeting of an individual or group.",Yes,"abusive, non-abusive, undecided level of abuse from 1 to 4 in which 1 is very slightly abusive, 2 is slightly abusive, 3 is strongly abusive and 4 is very strongly abusive.",No,,,,,The labelled dataset,1,1,a general news website,Abusiveness,English,N/A,XX.08.2015 - XX.09.2016,N/A,N/A,"The authors collected data from a general news website that provides an open forum for discussion and debate, anchored by news stories. News stories are editorially divided into at least one of the following eight categories: Local, Politics, International, Opinion, Family, Culture, Technology and Business. News stories can be tagged into multiple categories.

In the corpus, they gathered user-generated comments from 3,765 news articles between August 2015 and September 2016. They chose to collect comments when news articles have been published for a long time (more than 10 hours) to make sure that user reading of and commenting on that article has reached a saturation level.

In addition to the actual comment text, they collected other metadata: (1) the number of 'likes and 'dislikes for the comment, (2) the username of the person who posted the comment, (3) whether the user is linked to Facebook or to Twitter (4) whether the comment is a reply to the previous comment or a direct response to a news story.",Not discussed,,manual,Yes,"The authors used CrowdFlower to conduct the labelling process. Every comment was individually displayed to the labeller with an explanation of what the authors considered abusive content. Each labeller was asked 'Is the comment abusive or not?' Instead of using binary labels - 'yes' or 'no' for abusive or non-abusive, they treated this labelling task as a ternary problem with the inclusion of a third label choice - 'undecided'.

The decision to introduce an undecided' category was to reflect the fact that the assessment of a comment as abusive or not can be ambiguous, and labellers can disagree. The authors suggest that flagging a post as 'undecided' can be considered as flagging risk. For comments that the labeller considered abusive, a second question captured the severity of the abuse. They required labellers to apply a scale to the level of abuse from 1 to 4 in which 1 is very slightly abusive, 2 is slightly abusive, 3 is strongly abusive and 4 is very strongly abusive. Each comment was labelled by six labellers and not all labellers labelled every comment.

To minimise the potential bias of the test questions, the authors carefully selected twenty comments that were obviously distinguishable. These test questions were randomly interspersed in the real labelling tasks (for quality control). In addition, they restricted the labellers to those located in the same region as the news website, to allow for knowledge of local language expressions. A minimum judgement time per comment was set at 3 seconds.

In total, 89 participants worked on the labelling task. 9 were eliminated due to their low labelling accuracy, which is defined as less than 60% accuracy on the pre-defined testing questions.

The authors defined the following rules in order to determine the decision of the final label for each comment:

• If the comment receives at least 3 votes for ‘undecided', or the comment has the same votes for 'abusive’ and 'nonabusive', the final label is 'undecided';

• Otherwise: - If the comment receives at least 3 votes for ‘abusive'. the final label is 'abusive’- If the comment receives at least 3 votes for 'nonabusive’. the final label is 'non-abusive’;",Yes,"The authors used lexicon-based filtering to increase the likelihood of abusive comments in their sample with the assumption that profane words are indicative of abusive content. They used three public profane word lists, namely, CMU(https://www.cs.cmu.edu/ biglou/resources/bad-words.txt), Github(https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en), and Noswearing(http://www.noswearing.com/dictionary). The word lists contain 1383, 376 and 349 abusive words respectively. The authors observed that, using CMU, the largest words list, nearly 24% of comments in corpus include at least one profane word. However, using Github and Noswearing, as smaller word lists, filtered just 2% of comments from corpus each. They wished to avoid over restriction of the sample on specific words, so they used the CMU list to complete the pre-filtering because it contains the widest list of potentially profane words.

Prior to sampling, they first eliminated approximately 10% of comments in the corpus based on their length. This was suggested by Sood et al. in order to eliminate the comments that are either too short to meaningfully interpret or too long to digest quickly. They then filtered the remaining data using the CMU list. 76% of comments do not have any profane word, with decreasing percentages as the number of profane word matches increases, up to just 0.56% comments containing at least 4 profane words. They then sampled our dataset for labelling by randomly selecting 400 comments from each of these groupings of comments. In total, there are 2000 comments across 5 groups.",2000,10-fold cross-validation,10-fold cross-validation,CrowdFlower workers,Yes,Every comment was individually displayed to the labeller with an explanation of what the authors considered abusive content.,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
302,No,"(Aryuni et al., 2020)",An Early Warning Detection System of Terrorism in Indonesia from Twitter Contents using Naïve Bayes Algorithm,2020 International Conference on Information Management and Technology (ICIMTech),Indonesia,"The objective of this research is to analyze content posted in Twitter and to review whether post and conversation on Twitter will be highly related to terrorism intention or another way around. This study deployed Naïve Bayes classification technique which identified Twitter contents in Indonesian national language. The method has been processed text pre-processing, and dataset divided with hold out technique. Result of F-measure value indicates that 76% and 77% of texts are associated with the accuracy level of terrorism based on macro-averaging and micro-averaging indicators. The finding is contributed to the scanty literature on the early warning detection method in Indonesian language and assist the government to target the extremists' organizations.",Yes,2,13.08.2020,14.10.2022,Yes,Yes,Terrorism,No,,No,,,,,The dataset,1,1,Twitter,Terrorism,Indonesian,N/A,N/A,N/A,N/A,The dataset contains 100 Indonesian language tweets that was taken from Twitter using tools like Twitter Application Programming Interface (API) and Python script with the Tweepy library.,Not discussed,,manual,Not discussed,,Not discussed,,100,70,30,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
304,No,"(Wang et al., 2022)",Political Hate Speech Detection and Lexicon Building: A Study in Taiwan,IEEE Access,Taiwan,"The current study proposes an approach to build a political hate speech lexicon and train artificial intelligence classifiers to detect hate speech. The academic and practical contributions include the collection of a Chinese hate speech dataset, creating a Chinese hate speech lexicon, and developing both a deep learning-based and a lexicon-based approach to detect Chinese hate speech.",Yes,0,21.03.2022,14.10.2022,Yes,Yes,"The definitions by Facebook, YouTube, and Basile, et al. argued that hate speech consists of discriminatory content targeting a person or a group of people based on their attributes, such as age, race, ethnicity, national origin, caste, religious affiliation, disability, gender, sexual orientation and gender identity, serious disease, and expression. Hate speech may target people with specific expressions, such as political standpoint expressions. However, to the best of our knowledge, no previous studies considered malicious criticism to supporters and politicians of an opposing stance as hate speech. The principle of freedom of speech should protect political speech, discussion, and argument. Nevertheless, malicious criticism and attacks on a person, politician, or their supporters, based on their political standpoint, may destroy the harmony of cyberspace, making it a polarized space or an echo chamber. Thus, attacks of malicious language toward people, based on their political standpoint, should also be considered a type of hate speech.",Yes,"three categories: hate speech, offensive speech, and normal speech",No,,,,,Chinese hate speech dataset,4,4,LINE Today,Political Hate Speech,Chinese,N/A,N/A,N/A,N/A,"A. STUDY 1: INITIAL LEXICON BUILDING

The authors used a web crawler to extract the online user comments to Taiwanese political news from LINE Today, a news aggregator that integrates news from various news media. Unlike other news aggregators, LINE Today is also a social media platform on which people can comment on the news. LINE is a freeware application for instant text, voice, and video messages on mobile phones, tablets, and personal computers. It is the most popular instant communications application in Taiwan. In the first study, they crawled 11,917 comments to political news.

B. STUDY 2: EXTENDING THE HATE SPEECH DATASET

The primary purpose of study 2 is to extend the hate speech dataset. In study 1, they only obtained 1,069 hate speech samples, which is not enough for any hate speech deep learning analysis. They also identified that the hate speech proportion was approximately 8.93%. (If they hope to collect a dataset of 5,000 hate speech, they have to annotate approximately 55,000 speech samples, which is a resource-consuming task.) Therefore, they developed an efficient approach to extend the hate speech dataset. In study 2, they crawled 100,000 news comments from LINE Today. They used the 113 terms obtained by study 1 to filter the collected comments. Among the 100,000 news comments, 8,773 comments that included hate speech terms from lexicon A were considered potential hate speech, while the remaining 91,227 comments were considered normal speech.

C. STUDY 3: DEEP LEARNING MODEL

The authors used the 4,496 hate speech comments obtained from studies 1 and 2 (1,069 from study 1 and 3,427 from study 2). They randomly selected 4,478 normal speech samples (collected from studies 1 and 2) to construct a dataset of 8974 comments, composed of a balanced number of normal speech and hate speech samples. They divide the dataset, using 80% for training data and 20% for testing data. The maximum word length of the news comments was set to 40only the first 40 Chinese words were included for news comments THAT WERE longer than 40 words.

D. STUDY 4: EVALUATION OF DETECTION PERFORMANCE OF BERT MODEL

In study 4, they verified the detection performance of the BERT model fine-tuning from study 3. They used a web crawler to extract another 100,939 online users' comments to Taiwanese political news from LINE Today.",Not discussed,,"automated, manual",Yes,"A. STUDY 1: INITIAL LEXICON BUILDING

The authors recruited three annotators to categorize these comments to political news as normal speech or hate speech. To assist the annotators with the categorization of comments, we developed an annotation assistance system, which allowed annotators to view the news headline, news reports, and users' comments.

They provided the annotators with definitions, guidelines, and examples of hate speech and normal speech. They asked annotators to divide speech as hate speech, offensive speech, and normal speech. Each comment to political news was annotated manually into the following three categories:

(1) Hate Speech: A sentence with an abusive intention on specific attributes of a group or individual, such as political beliefs, party membership, race, gender, age, sexual orientation, or gender identity, but not including satire or humorous comments.

(2) Offensive Speech: A sentence with irrational expression or the creation of opposing comments.

(3) Normal Speech: A sentence with neutral, positive, constructive, and non-offensive expression.

In this study, they only consider hate speech. If a speech sample was considered hate speech by at least two annotators, it was considered hate speech in the study. After annotation, they obtained 1,069 (8.93%) hate speech, and the other 10,848 (91.07%) comments were considered normal speech.

To assess inter-annotator agreement, they adopt the Fleiss' kappa statistic, which provides an overall agreement measure of more than two annotators for a categorical rating (unlike Cohen's Kappa, which only provides a measure of pairwise agreement).

B. STUDY 2: EXTENDING THE HATE SPEECH DATASET

The authors recruited three annotators to categorize these comments to political news as normal speech or hate speech. Only comments categorized as hate speech by at least two annotators were considered hate speech. The Fleiss' kappa statistic was 0.986, which was an almost perfect agreement. Among the 8,773 potential hate speech comments, only 3,427 comments were annotated manually as hate speech.

C. STUDY 3: DEEP LEARNING MODEL

Annotations from study 1+2.

D. STUDY 4: EVALUATION OF DETECTION PERFORMANCE OF BERT MODEL

They used the fine-tuning BERT model in study 3 to tag the collected comments. Of the 100,939 comments, the BERT model tagged 11,331 comments as potential hate speech.

The authors recruited three annotators to categorize these comments to political news as normal speech or hate speech. Among the 11,331 potential hate speech samples, 7,927 were rated as hate speech by at least two annotators and were therefore considered as hate speech in the study. The other 3,404 were considered as normal speech.

The other performance indicators, such as accuracy, recall, and F1-score, were not available because the authors did not have sufficient resources to hire annotators to categorize all 100,393 comments. To estimate the accuracy, recall, and F1-score of the deep learning model, they randomly sampled 1000 comments and recruited three annotators to categorize them.",Yes,"B. STUDY 2: EXTENDING THE HATE SPEECH DATASET

The authors developed an efficient approach to extend the hate speech dataset. In study 2, they crawled 100,000 news comments from LINE Today. They used the 113 terms obtained by study 1 to filter the collected comments. Among the 100,000 news comments, 8,773 comments that included hate speech terms from lexicon A were considered potential hate speech, while the remaining 91,227 comments were considered normal speech.

D. STUDY 4: EVALUATION OF DETECTION PERFORMANCE OF BERT MODEL

They used the fine-tuning BERT model in study 3 to tag the collected comments. Of the 100,939 comments, the BERT model tagged 11,331 comments as potential hate speech.

(Within all 100,393 comments, )To estimate the accuracy, recall, and F1-score of the deep learning model, they randomly sampled 1000 comments and recruited three annotators to categorize them.","For study 1/2/3/4: 11,917; 100,000; 8,974; 100,393",Study 3: 80% (~7179),Study 3: 20% (1795); Study 4: 1000,N/A,Yes,"To assist the annotators with the categorization of comments, we developed an annotation assistance system, which allowed annotators to view the news headline, news reports, and users' comments.They provided the annotators with definitions, guidelines, and examples of hate speech and normal speech.",Not discussed,,Specific,political,Targeted,political,Yes,No,,,Yes,political standpoint,No,,,No,Human
305,No,"(Arslan et al., 2019)",Overwhelmed by negative emotions? maybe you are being cyber-bullied!,Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing,France,"In this paper, first, the authors carry out a message-level cyberbullying annotation on an Instagram dataset. Second, they use the correlations on the Instagram dataset annotated with emotion, sentiment and bullying labels. Third, they build a message-level emotion classifier automatically predicting emotion labels for each comment in the Vine bullying dataset. Fourth, they build a session-based bullying classifier with the use of n-grams, emotion, sentiment and concept-level features. For both emotion and bullying classifiers, they use Linear Support Vector Classification. The results show that ""anger"" and ""negative"" labels have a positive correlation with the presence of bullying. Concept-level features, emotion and sentiment features in different levels contribute to the bullying classifier, especially to the bullying class. Their best performing bullying classifier with n-grams and concept-level features (e.g., polarity, averaged polarity intensity, moodtags and semantics features) reaches to an F1-score of 0.65 for bullying class and a macro average F1-score of 0.7520.",Yes,14,08.04.2019,14.10.2022,Yes,Yes,"Cyberbullying is defined as ""an aggressive, intentional act carried out by a group or individual, using electronic forms of contact, repeatedly and over time against a victim who cannot easily defend him or herself"".",Yes,"emotion: anger, fear, joy, sadness, other, no emotion sentiment: positive, negative, neutral bullying labels: bullying and no bullying.",No,,,,,"emotion dataset: 1000-comment Instagram dataset (newly annotated) + WASSA-2017;

cyberbullying dataset: Vine dataset",3,1,"Instagram, Vine",Cyberbullying,English,N/A,N/A,N/A,N/A,"A portion of Instagram dataset (Hosseinmardi et al.) was annotated per post. The authors randomly selected 10 media sessions (a media session is the thread of comments following a picture.) from the Instagram dataset. In total, 1000 Instagram comments were obtained from 10 sessions (i.e., 5 bullying and 5 no bullying sessions) .",Not discussed,,manual,Yes,"Two annotators from linguistics annotated 1000 Instagram comments, which were obtained from 10 sessions, with emotion, sentiment and bullying labels. The annotation was addressed using the following emotion, sentiment and bullying labels: anger, fear, joy, sadness, other, no emotion, positive, negative, neutral, bullying and no bullying. The authors computed the inter-annotator agreement on a subset of the annotated dataset using Cohen's Kappa.",Yes,"The authors randomly selected 10 media sessions (a media session is the thread of comments following a picture.) from the Instagram dataset. In total, 1000 Instagram comments were obtained from 10 sessions (i.e., 5 bullying and 5 no bullying sessions) .",emotion dataset: 2808 messages cyberbullying dataset: 970 sessions,10-fold cross-validation,10-fold cross-validation,Two annotators from linguistics,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
307,No,"(Haq et al., 2020)",USAD: An Intelligent System for Slang and Abusive Text Detection in PERSO-Arabic-Scripted Urdu,Complexity,"Pakistan, Saudi Arabia","This study proposes USAD (Urdu Slang and Abusive words Detection), a lexicon-based intelligent framework to detect abusive and slang words in Perso-Arabic-scripted Urdu Tweets. Furthermore, due to the nonavailability of the standard dataset, the authors also design and annotate a dataset of abusive, offensive, and slang word Perso-Arabic-scripted Urdu as their second significant contribution for future research. The results show that their proposed USAD model can identify 72.6% correctly as abusive or nonabusive Tweet. Additionally, the authors have also identified some key factors that can help the researchers improve their abusive language detection models.",Yes,7,30.11.2020,14.10.2022,Yes,Yes,"Abusive, offensive, and slang word",No,,No,,,,,the dataset,1,1,Twitter,Abusiveness,Urdu,XX.10.2019 - XX.12.2019,XX.10.2019 - XX.12.2019,N/A,N/A,"For abusive and slang words lexicon, the authors collected more than 5000 Tweets and replies posted by famous politicians, journalists, analysts, and intellectuals on different topics during October 2019 and December 2019. The tweets are then saved into a text fle with a UTF-8 encoding scheme. After applying the preprocessing and data cleansing steps, an abusive words lexicon is created manually. The abusive word lexicon is composed of 2533 abusive words of Urdu language posted in 3 months period.

For testing, the authors build a dataset composed of 1200 Tweets and replies posted on different topics during the same period, i.e., October 2019 and December 2019. For data cleansing, the same preprocessing steps are used.",Not discussed,,manual,Yes,"After data cleansing, the authors manually annotate the dataset into abusive and nonabusive classes for result comparison.",Not discussed,,1200,N/A,1200,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
308,No,"(Price et al., 2020)",Six Attributes of Unhealthy Conversations,Proceedings of the Fourth Workshop on Online Abuse and Harms,"UK, Australia, USA, South Africa","The authors present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either 'healthy' or unhealthy', in addition to binary labels for the presence of six potentially unhealthy' sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. The authors argue that there is a need for datasets which enable research based on a broad notion of 'unhealthy online conversation'. They build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. They explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.",Yes,16,20.11.2020,14.10.2022,Yes,Yes,"In this paper, we broadly characterise a healthy online public conversation as one where posts and comments are made in good faith, are not overly hostile or destructive, and generally invite engagement. Such a conversation may include robust engagement and debate, and is generally (though not always) focused on substance and ideas. Importantly, though, healthy contributions to online conversations are not necessarily friendly, grammatically correct, well constructed, intellectual, substantive, or even free of any vulgarity.",Yes,"six potentially 'unhealthy' sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation.",Yes,Github,,https://github.com/conversationai/unhealthy-conversations,Yes,The Unhealthy Comments Corpus (UCC),1,1,Globe and Mail news site (sampled from the SFU Opinion and Comment Corpus dataset),Unhealthy Conversations,English,N/A,N/A,N/A,N/A,"The dataset comprises randomly chosen comments from the Globe and Mail news site (sampled from the SFU Opinion and Comment Corpus dataset) (Kolhatkar et al., 2019), of 250 characters or less.",Not discussed,,manual,Yes,"Comment scores were crowdsourced using Figure Eight (now Appen). The annotation job consisted of 588 crowdworkers (annotators) providing 244468 judgements on 44355 comments. Each annotator was asked to identify for each comment whether it was healthy and if any of the attributes were present, in the form of a standard questionnaire. Annotators were not given any wider context or additional information about where a comment was posted or how it was engaged with by other users.

To both accommodate and attempt to resolve meaningful disagreement, the authors applied a dynamic judgement method which requests additional annotations for those comments on which there was insufficient consensus either yes or no with a confidence of less than 75%). All comments were annotated at least three times, and more annotators were added, up to a limit of five annotators per comment until sufficient consensus was reached.

The exact wording of each question on the questionnaire went through multiple iterations, tested by smaller scale experiments to evaluate effectiveness. The quality of the resulting data was evaluated manually by the research team, calculating the proportion of perceived mistaken annotations and their 'severity': to what extent a judgement was obviously wrong', as opposed to an understandable alternative reading of a comment.

The primary quality control mechanism was to collate a set of 'test comments', for which the authors had manually established the correct answers. Annotators encountered one test comment per batch of seven comments they reviewed, without knowing which of the seven was the test comment, and their running accuracy on these test comments was defined as their 'trustworthiness score'. The task required that annotators maintain a trustworthiness score of more than 78%. If an annotator dropped below this level, they were removed from the annotator pool for this task, and all of their prior annotations were discarded. The removed 'bad' annotator judgements were replaced by newly collected trusted judgements as necessary. The authors restricted the test comments to what were (in our view) clear and definitive examples of the attributes, such that one would fail on the test comments only if one has an incorrect understanding of what is meant by a particular attribute.",Not discussed,,44355,35503,4425,"With the comment corpus being in English, and given the subtlety of the attributes, higher quality annotations were likely to be achieved by annotators with first-language proficiency in English. The best proxy for this available on the Figure Eight platform was to restrict the country of origin of our annotators to a limited subset of countries with a large English-speaking population (as either an official language or primary second language), in particular: the United States, the United Kingdom, South Africa, Sweden, New Zealand, Norway, Netherlands, Denmark, Canada, and Australia.",Yes,"Annotators were provided with the standard questionnaire, which includes what are the characteristics of a healthy conversation, what a healthy conversation does not necessarily require all posts and comments to be, and a set of of questions targeting each attributes of unhealthy conversations. The questionnaire is available in the appendix.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
309,No,"(Alakrot et al., 2018)",Towards Accurate Detection of Offensive Language in Online Communication in Arabic,Procedia Computer Science,Ireland,"The authors present the results of predictive modelling for the detection of anti-social behaviour in online communication in Arabic, such as comments which contain obscene or offensive words and phrases. They collected and labelled a large dataset of YouTube comments in Arabic which contains a broad range of both offensive and inoffensive comments. They used this dataset to train a Support Vector Machine classifier and experimented with combinations of word-level features, N-gram features and a variety of pre-processing techniques. They summarise the pre-processing steps and features that allow training a classifier which is more precise, with 90.05% accuracy, than classifiers reported by previous studies on Arabic text.",Yes,67,15.11.2018,14.10.2022,Yes,Yes,"Offensive language. ""Several studies in the early years of the Web defined the term flaming as hostile intentions characterised by words people. Some studies suggest that flaming is a social or cultural tendency. However, others suggest that of profanit.""",No,,Yes,Other,OneDrive,https://goo.gl/27EVbU,Yes,dataset of YouTube comments in Arabic,1,1,YouTube,Offensiveness,Arabic,XX.07.2017 - XX.07.2017,XX.XX.2015-XX..07.2017,N/A,N/A,"The authors collected comments on YouTube and build a dataset of 15,050 comments, suitable for training predictive models. This dataset was collected in July 2017, the method of collection is by selecting some YouTube channels that upload videos about celebrities in the Arab world. These videos display celebrities on the media in controversial footage. This type of footages provoke viewers, leading some of them to utilise offensive/abusive language in their posts. These videos were uploaded in the period from 2015 to 2017. Out of the corpus of comments on 150 videos, they selected 9 videos with a high number of comments for building the dataset. They assume that a greater number of comments may indicate more intense debate and higher diversity of offensive language used.",Not discussed,,manual,Yes,"The labelling process was accomplished by three annotators from three different Arab countries. The intention was to ensure that comments labelled as offensive are understood as such by people from different Arab regions. The number of positives is 5,817 comments (39% of the entire dataset). These are comments that at least two of the three annotators consider offensive. The inter-annotator agreement is 71%.",Yes,"Out of the corpus of comments on 150 videos, they selected 9 videos with a high number of comments for building the dataset. They assume that a greater number of comments may indicate more intense debate and higher diversity of offensive language used.","15,050",10-fold cross-validation,10-fold cross-validation,Three annotators from three different Arab countries,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
310,No,"(Aulia & Budi, 2019)",Hate Speech Detection on Indonesian Long Text Documents Using Machine Learning Approach,Proceedings of the 2019 5th International Conference on Computing and Artificial Intelligence,Indonesia,"In this research, the authors explore in detecting hate speech on Indonesian long documents using machine learning approach. They build a new Indonesian hate speech dataset from Facebook. The experiment showed that the best performance obtained by Support Vector Machine (SVM) as its classifier algorithm using TF-IDF, char quad-gram, word unigram, and lexicon features that yield f1-score of 85%.",Yes,14,19.04.2019,14.10.2022,Yes,Yes,"The term ""hate speech"" was defined as any communication that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity.",No,,No,,,,,Indonesian hate speech dataset from Facebook,1,1,Facebook,Hate Speech,Indonesian,XX.09.2018 - XX.10.2018,Implicitly before XX.10.2018,N/A,N/A,"Data collection was done by scraping both status/posts and notes from social media Facebook. The authors focused on collecting posts that talk about political issues. Some Indonesian political related keywords such as Jokowi, Prabowo, etc. are used to find related data. After collecting the initial dataset, frequently used words, terms, or hashtags are extracted to get more keywords for the next data finding. They focused on retrieving data that are posted in 2018 with a minimum length of characters is 1000. The data collecting process was held in September - October 2018. After removing duplicate data, they have 1276 unique data in their dataset.",Not discussed,,manual,Yes,"The dataset was manually labeled by three annotators. The annotators should mark each data as ""hate speech"" (HS) or ""non-hate speech"" (Non-HS). Before data annotation was conducted, the authors prepared a guideline for annotators to ensured that they understand the definition of hate speech that was used in this study.

The authors applied 100% agreement to decide the label for each data. Data with agreement level less than 100% were discarded from the dataset. After the labeling process, they got 906 valid data, consist of 229 hate speech data and 677 non-hate speech data.",Not discussed,,906,10-fold cross-validation,10-fold cross-validation,The three annotator consist of males and females aged between 20 - 26 years old with different backgrounds and domicile.,Yes,"Before data annotation was conducted, the authors prepared a guideline for annotators to ensured that they understand the definition of hate speech that was used in this study.",Not discussed,,General,N/A,Targeted,"race, religion, nationality, gender, disability, sexuality",Yes,No,,,No,,No,,,No,Human
311,No,"(Gao et al., 2017)",Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised Two-path Bootstrapping Approach,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),USA,"To address various limitations of supervised hate speech classification methods including corpus bias and huge cost of annotation, the authors propose a weakly supervised two-path bootstrapping approach for an online hate speech detection model leveraging large-scale unlabeled data. This system significantly outperforms hate speech detection systems that are trained in a supervised manner using manually annotated data. Applying this model on a large quantity of tweets collected before, after, and on election day reveals motivations and patterns of inflammatory language.",Yes,61,27.11.2017,14.10.2022,Yes,Yes,"When we annotate system predicted tweet samples, we essentially adopt the same definition of hate speech as used in (Waseem and Hovy, 2016), which considers tweets that explicitly or implicitly propagate stereotypes targeting a specific group whether it is the initial expression or a metaexpression discussing the hate speech itself (i.e. a paraphrase).

Explicit hate speech is easily identifiable by recognizing a clearly hateful word or phrase. In contrast, implicit hate speech employs circumlocution, metaphor, or stereotypes to convey hatred of a particular group, in which hatefulness can be captured by understanding its overall compositional meanings.",No,,No,,,,,"10 million sampled tweet set;

62 million unlabeled tweet set;

1000 annotated tweet set;

annotated tweets (Waseem and Hovy, 2016)",4,3,Twitter,Hate Speech,English,01.10.2016 - 24.10.2016 25.10.2016 - 15.11.2016,01.10.2016 - 24.10.2016 25.10.2016 - 15.11.2016,N/A,N/A,"The authors randomly sampled 10 million tweets from 67 million tweets collected from Oct. 1st to Oct. 24th using Twitter API. These 10 million tweets were used as the unlabeled tweet set in bootstrapping learning. Then they continued to collect 62 million tweets spanning from Oct.25th to Nov.15th, essentially two weeks before the US election day and one week after the election. The 62 million tweets will be used to evaluate the performance of the bootstrapped slur term learner and LSTM classifier. The timestamps of all these tweets are converted into EST. By using Twitter API, the collected tweets were randomly sampled to prevent a bias in the data set.",Not discussed,,"automated, manual",Yes,"Automatic Data Labeling of Initial Data

The authors use 20 manually selected slur terms to match with a large unlabeled tweet collection in order to quickly construct the initial small set of hateful tweets. They obtained the initial list of slurs from Hatebase, the Racial Slurs Database , and a page of LGBT slang terms. They ranked the slur terms by their frequencies in tweets, eliminating ambiguous and outdated terms. The slur ""gypsy"", for example, refers to derogatorily to people of Roma descent, but currently in popular usage is an idealization of a trendy bohemian lifestyle. The word ""bitch"" is ambiguous, sometimes a sexist slur but other times innocuously self-referential or even friendly. For these reasons, they only selected the top 20 terms they considered reliable. They use both the singular and the plural form for each of these seed slur terms.

Slur Term Learner

They assign a score to each unique unigram that appears 10 or more times in hateful tweets, and the score is calculated as the relative ratio of its frequency in the labeled hateful tweets over its frequency in the unlabeled set of tweets. Then the slur term learner recognizes a unigram with a score higher than a certain threshold as a new slur. Specifically, they use the threshold score of 100 in identifying individual word slur terms. The newly identified slur terms will be used to match with unlabeled tweets in order to identify additional hateful tweets. A tweet that contains one of the slur terms is deemed to be a hateful tweet.

The LSTM Classifier

They aim to recognize implicit hate speech expressions and capture composite meanings of tweets using a sequence neural net classifier. The input to the LSTM classifier is a sequence of words. The LSTM classifier is trained using the automatically labeled hateful tweets as positive instances and randomly sampled tweets as negative instances, with the ratio of POS:NEG as 1:10. Then the classifier is used to identify additional hateful tweets from the large set of unlabeled tweets. The LSTM classifier will deem a tweet as hateful if the tweet receives a confidence score of 0.9 or higher. Both the low POS:NEG ratio and the high confidence score are applied to increase the precision of the classifier in labeling hateful tweets and control semantic drift in the bootstrapping learning process. To further combat semantic drift, they applied weighted binary cross-entropy as the loss function in LSTM.

Human Annotations

When they annotate system predicted tweet samples, they essentially adopt the same definition of hate speech as used in (Waseem and Hovy, 2016), which considers tweets that explicitly or implicitly propagate stereotypes targeting a specific group whether it is the initial expression or a metaexpression discussing the hate speech itself (i.e. a paraphrase). In order to ensure their annotators have a complete understanding of online hate speech, they asked two annotators to first discuss over a very detailed annotation guideline of hate speech, then annotate separately. This went for several iterations. Then they asked the two annotators to annotate the 1,000 tweets that were randomly sampled from all the tweets tagged as hateful by the supervised LSTM classifier. The two annotators reached an inter-agreement Kappa score of 85.5%. Because one of the annotators become unavailable later in the project, the other annotator annotated the remaining sampled tweets.",Not discussed,,"62 million automatically labelled; 1000 human annotated; ~16k annotated tweets in a recent study (Waseem and Hovy, 2016)",62 million; 16k,1000,N/A,Yes,"In order to ensure their annotators have a complete understanding of online hate speech, the authors asked two annotators to first discuss over a very detailed annotation guideline of hate speech, then annotate separately. This went for several iterations.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
312,No,"(Leite et al., 2020)",Toxic Language Detection in Social Media for Brazilian Portuguese: New Dataset and Multilingual Analysis,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Brazil, UK","In this paper, the authors propose a new large-scale dataset for Brazilian Portuguese with tweets annotated as either toxic or non-toxic or in different types of toxicity. They present the dataset collection and annotation process, where they aimed to select candidates covering multiple demographic groups. State-of-the-art BERT models were able to achieve 76% macro-F1 score using monolingual data in the binary case. The authors also show that large-scale monolingual data is still needed to create more accurate models, despite recent advances in multilingual approaches. An error analysis and experiments with multi-label classification show the difficulty of classifying certain types of toxic comments that appear less frequently in our data and highlights the need to develop models that are aware of different categories of toxicity.",Yes,32,04.12.2020,14.10.2022,Yes,Yes,"Our definition of toxic is similar to the one used by the Jigsaw competition, where comments containing insults and obscene language are also considered, besides hate speech.",Yes,"non-toxic, LGBTQ+phobia, obscene, insult, racism, misogyny and xenophobia.",Yes,Github,,https://github.com/JAugusto97/ToLD-Br,Yes,Toxic Language Dataset for Brazilian Portuguese (ToLD-Br),1,1,Twitter,Toxicity,Portuguese,XX.07.2019 - XX.08.2019,N/A,N/A,N/A,"The authors used the GATE Cloud's Twitter Collector to collect posts on the Twitter platform from July to August 2019. They used two different strategies to select tweets for ToLD-Br, aiming to increase the probability of obtaining posts with toxic content, given that the volume of toxic tweets is significantly smaller than data without offensive language.

The first strategy searches for tweets that mention predefined hashtags or keywords. They chose predefined terms highly likely to belong to a toxic tweet in Brazilian Twitter, such as gay (""Gay tem que apanhar"" - ""Gay should be beaten up""), mulherzinha (""Mulherzinha, vai lavar louça"" - ""Sissy, go wash dishes""), and nordestino (""Nordestino preguiçoso"" - ""Lazy Northeastern\*). However, using this strategy alone may hinder learning a model capable of generalising the concept of toxicity beyond the scope of keywords. Consequently, another strategy was adopted: they scraped tweets that mention influential users like Brazil's president Jair Bolsonaro and soccer player Neymar Jr, prone to receive abuse (around 50 influential users were monitored). Tweets collected through this method have no restrictions in terms of keywords and should broaden the scope of the data.

They collected more than 10M unique tweets and randomly selected 21K examples to compose the annotated corpus. They note that 12, 600 of these posts (60%) comes from the first strategy - predefined keywords - and the remaining are tweets from threads of predefined users.",Yes,"The data was pseudoanonymised before being sent for annotation, with all @ mentions replaced by @user.",manual,Yes,"The annotation process started by choosing volunteers to perform the task of assigning labels for each example. For this, the authors made a public consultation at the Federal University of São Carlos (Brazil) to find candidate annotators (129 volunteers registered for the task). From these candidates, 42 were selected based on their demographic information, aiming to balance annotation bias as the interpretation of toxicity may vary. Each annotator labelled 1, 500 tweets, selecting one of the following categories: LGBTQ+phobia, obscene, insult, racism, misogyny and/or xenophobia (or leaving it blank for none). Each tweet was annotated by three different annotators. Inter-annotator agreement is calculated in terms of Krippendorf’s α.",Yes,The authors randomly selected 21K examples to compose the annotated corpus,~21k,80% (~16800),10% (~2100),"42 annotators: 18 male, 24 female22 heterosexual, 12 bisexual, 5 homosexual, 3 pansexual25 White, 9 Brown, 5 Black, 2 Asian, 1 non-declared.

The age of the annotators varies between 18 and 37 years, with most of them in the range between 19 and 23.",Not discussed,,Not discussed,,Specific,"gender, sexuality, race, nationality",Non-targeted,N/A,Yes,Yes,"Gender, Sexuality, Race, Nationality, Ethnicity.  Note: the categories are non-toxic, LGBTQ+phobia, obscene, insult, racism, misogyny and xenophobia.","gender, sexuality, race, nationality, ethnicity.  Note: the categories are non-toxic, LGBTQ+phobia, obscene, insult, racism, misogyny and xenophobia.",No,,No,,,No,Human
313,No,"(Fuchs & Schäfer, 2020)",Normalizing misogyny: hate speech and verbal abuse of female politicians on Japanese Twitter,Japan Forum,Germany,"In this article the authors present results from an explorative analysis of instances of misogynist or sexist hate speech and abusive language against female politicians on Twitter, applying computational corpus-linguistic tools and methods, supplemented by a qualitative in-depth study of verbal abuse of four prominent female politicians in Japan, namely Renhō, Tsujimoto Kiyomi, Yamao Shiori, and Koike Yuriko, thereby fruitfully combining quantitative-statistical and qualitative-hermeneutic approaches.",Yes,32,06.01.2020,14.10.2022,Yes,Yes,"Waseem et al. (2017) distinguish between explicit and implicit forms of abusive language, with the explicit abusive language being obviously derogatory whereas implicit forms usually hide behind sarcasm or irony (cf. also Schmidt and Wiegand 2017). Hate speech and overt insults clearly belong to forms of explicit abusive language. Whereas insults (such as calling somebody an ""idiot"") directly target an individual without inferring to its ethnic or gender identity (Sponholz 2018), hate speech can be defined as a discriminatory form of verbal abuse based on group identity (such as gender or ethnicity) (Unger 2013; Sponholz 2018). Davidson et al. (2017) define hate speech ""as language that is used to expresses (sic) hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group."" All forms of explicit abusive language are hurtful and derogatory to the individual attacked, are always colloquial and emotional, and are not aiming at entering a dialogue with the person addressed.",No,,Yes,Other,upon request,"“Due to personal rights, we are unable to publicize our full dataset for the reproduction of our results. However, we are more than willing to provide other researchers with as much data and information as we can upon request.”",Yes,The dataset,1,1,Twitter,"Misogyny, Hate speech, Abusive language",Japanese,01.01.2018 - 15.04.2018,01.01.2018 - XX.04.2018 (“between the beginning of January of 2018 and mid-April of 2018”),N/A,N/A,"The text corpus -the Twitter data set used in this study- was collected between the beginning of January of 2018 and mid-April of 2018, using Twitter's streaming Application Programming Interface (API). To gain insights into the everyday discrimination and hate speech against female politicians, they intentionally chose a time period in which no elections were held in Japan for collecting the data to investigate the general attitude towards female politicians on Twitter. The data was collected based on a filter of nineteen preselected female politician's names as well as certain keywords (i.e., derogatory nicknames) that are associated with their names in the mass media or on the Internet. With this method, a Twitter corpus consisting of 9,449,645 words was created.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,gender,Targeted,"race, gender",No,Yes,Gender,gender,Yes,Female politicians,Yes,"four prominent female politicians in Japan, namely Renhō, Tsujimoto Kiyomi, Yamao Shiori, and Koike Yuriko",About a person,No,Human
314,No,"(Salminen et al., 2018)",Anatomy of online hate: Developing a taxonomy and machine learning models for identifying and classifying hate in online news media,"12th International AAAI Conference on Web and Social Media, ICWSM 2018","Qatar, Finland","The authors manually label 5,143 hateful expressions posted to YouTube and Facebook videos among a dataset of 137,098 comments from an online news media. They then create a granular taxonomy of different types and targets of online hate and train machine learning models to automatically detect and classify the hateful comments in the full dataset. The contribution is twofold: 1) creating a granular taxonomy for hateful online comments that includes both types and targets of hateful comments, and 2) experimenting with machine learning, including Logistic Regression, Decision Tree, Random Forest, Adaboost, and Linear SVM, to generate a multiclass, multilabel classification model that automatically detects and categorizes hateful comments in the context of online news media. The authors find that the best performing model is Linear SVM, with an average F1 score of 0.79 using TF-IDF features. They validate the model by testing its predictive ability, and, relatedly, provide insights on distinct types of hate speech taking place on social media.",Yes,96,25.06.2018,14.10.2022,Yes,Yes,"Hate speech, defined as hateful comments toward a specific group or target (Walker 1994).

Mondal et al. (2017, p. 87) define hate speech as ""An offensive post, motivated, in whole or in a part, by the writer's bias against an aspect of a group of people."" p

In this work, we focus on the general concept of hate, not exclusively on hate speech.",Yes,"The taxonomy has 13 main categories and 16 subcategories (29 in total). The main categories include targets and language, 9 describing targets and 4 the type of language.  Targets: Financial Power (Corporation, Wealthy), Political Issues (Terrorism, Politics, Ideology), Racism & Xenophobia (Anti-white, Anti-black, Xenophobia), Religion (Anti-Islam, Anti-Semitist), Specific Nation(s), Specific Person, Media (Towards media company, Other), Armed Forces (Police, Military), Behavior (Humanity, Other) (Note: Subtargets are in brackets)  Hateful Language: Accusations, Humiliation, Swearing, Promoting Violence",No,,,,,The dataset,1,1,"YouTube, Facebook",Online Hate,English,XX.07.2017 - XX.10.2017,N/A,N/A,N/A,"The authors collect data from a major online news and media company with an international audience. This media company is highly active in social media, posting several videos per day on YouTube and Facebook and typically receiving thousands of comments per video. By using official APIs, they pull 137,098 comments from videos posted on YouTube and Facebook, in the period of July-October, 2017. The commentators are from 175 countries, although here the authors focus on English comments only. In the dataset, 79,439 (46%) comments are from Facebook, 57,659 (54%) from YouTube. YouTube Analytics does not provide country information at the comment level, but the aggregate numbers show that 38.3% of commentators are of unknown origin (a general limitation of YouTube data collection), 34.9% are from the United States, after which the relative share drops drastically, India being the second largest identified country with 4.2% of commentators. United Kingdom (3.6%) and Canada (3.0%) are the largest after India. It is likely that are most commentators are immigrants because the news organization is reporting on non-American issues and many commentators make references to their home countries, such as India (2,575 times mentioned), Philippines (642), Pakistan (1,231), etc. Therefore, the commentators are likely to be ethnically varied, from many countries in the world.",Not discussed,,manual,Yes,"The authors applied open coding (Glaser & Strauss 1967), so that one of the authors coded the material until saturation was reached (i.e., no new categories emerged). During the coding process, categories were reorganized and added, and some subcategories merged into larger ones. This iterative nature of qualitative coding intends to improve the quality of the categories (Corbin and Strauss 1990).

The taxonomy was developed with the following guidelines in mind: 1) Read through the comments, identify themes and sub-themes. 2) While creating the categories, consider the hate target and the meaning of the comment. 3) When appropriate, apply hierarchy by first labeling the main theme, then a subtheme. 4) When classifying, include comments that are purposeful, i.e. intentionally hurtful. The last consideration was made because if hostility is not the purpose of the comment, it should not be classified as hateful. For example, ""Trump is a bad president"" is not hateful, but ""Trump is an orange buffoon"" is.

They considered linguistic attributes when annotating, as we are dealing with text. Swearing, aggressive comments, or mentioning the past political or ethnic conflicts in a nonconstructive and harmful way, were classified as hateful. When there was uncertainty about an instance, it was discussed with other researchers to avoid a biased opinion. Finally, they utilized a coding dictionary so that, after identifying certain cue words for a category, such as ""Hitler [was right],"" they search the dataset with the cue words to find more observations for the corresponding category.

After saturation, two other researchers independently coded a random sample of 200 comments using the established taxonomy. They found substantial agreement (score = 75.3% The agreement score was calculated by dividing the number of labels where two or more coders agreed by the number of possible values. A script was created to calculate this for each coded row, and the row-based agreements were averaged to get the overall agreement.",Not discussed,,"5,143 labels",67% (~3446),33% (~1697),"One of the authors coded the material until saturation (in open coding) was reached. After saturation, two other researchers independently coded a random sample of 200 comments using the established taxonomy.",Yes,"The authors established the taxonomy using manual open codingthe taxonomy was developed with the following guidelines in mind: 1) Read through the comments, identify themes and sub-themes. 2) While creating the categories, consider the hate target and the meaning of the comment. 3) When appropriate, apply hierarchy by first labeling the main theme, then a subtheme. 4) When classifying, include comments that are purposeful, i.e. intentionally hurtful.

After saturation, two other researchers independently coded a random sample of 200 comments using the established taxonomy.",Not discussed,,Specific,"religion, race, nationality, class, political, organization/institution, other",Non-targeted,N/A,Yes,Yes,"Religion, Race, Ethnicity, Nationality, Class, Political Issue, Media, Armed Forces, Behavior","religion, race,nationality, class, political, organization/institution, other",Yes,"Corporation, Wealthy, Terrorism, Politics, Ideology, Anti-white, Anti-black, Xenophobia, Anti-Islam, Anti-Semitist, media company, Other, Police, Military, Humanity",No,,About a person,No,Human
315,No,"(Leon-Paredes et al., 2019)",Presumptive Detection of Cyberbullying on Twitter through Natural Language Processing and Machine Learning in the Spanish Language,"IEEE CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies, CHILECON 2019","Ecuador, Mexico","This research deploys a Spanish cyberbullying prevention system (SPC), which relies on Natural Language Processing (NLP) methods and different machine learning techniques (Naive Bayes, Support Vector Machine and Logistic Regression), using Twitter as the basis for the extraction of knowledge bases or corpus. Several precision metrics and variable corpus sizes are used for the training. The learning results reach a maximum accuracy of 93%, verified through the application of three study cases.",Yes,19,13.11.2019,14.10.2022,Yes,Yes,"Bullying, defined as an act that threatens a person's holistic well-being, becomes cyberbullying when it is done over Internet…",No,,No,,,,,Small Corpus; Medium Corpus; Large Corpus,3,3,Twitter,Cyberbullying,Spanish,N/A,N/A,N/A,N/A,"Taking advantage of the fact that the SPC's tests will be based on Twitter and the advantages of API Twitter, a corpus of training was formed through the extraction, processing and analysis of tweets.

After extracting the 960,578 tweets, cleaning techniques are applied to eliminate retweets, abbreviations, emojis, polysemy, links or other linguistic factors that may distort the context of the message and reflect false positives or negatives.

To handle various training rates, the corpus created was divided into 3 alternatives:

• Small Corpus: a total of 25,304 tweets were obtained, 10,906 with presumptive cyberbullying.

• Medium Corpus: made up of 229,801 tweets of which 84,800 reflect presumptive cyberbullying.

• Large Corpus: integrated with a total of 960,578 tweets, 416,567 correspond to presumptive cyberbullying.",Not discussed,,automated,Yes,"The process for the conformation of the corpus is initiated giving weight to key words considered as pejorative or insults according to the ""Inventario general de insultos"". As expected, this book does not consider Ecuadorian terms, so local synonyms were included. Each weight is determined as the average between the score established by three psychologists, being able to take values between 0 and 1 (semi-automatic approach). In addition, following the recommendations of a previous research, where the authors determine patterns in Spanish, it was established that any grammatical structure composed of an imperative, an affirmation and a pejorative/insult word is directly associated with instances of cyberbullying.

With the preprocessed dataset, the semantic rules mentioned are used and, when a grammatical match is found, the content is tagged as ""presumptive cyberbullyingotherwise, ""without cyberbullying"". The only limitation of the corpus is that the irony of the phrases is not analyzed, because verbs and contextual structures are not considered.",Not discussed,,"Small Corpus (25,304); Medium Corpus (229,801); Large Corpus (960,578)","training percentages are 5%, 10%, 15%, 20% and so on until reaching 50%","test percentages are 95%, 90%, 85% and so on until reaching 50%",N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
316,No,"(Perez Almendros et al., 2020)",Don't Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities,Proceedings of the 28th International Conference on Computational Linguistics,UK,"In this paper, the authors introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. The authors furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. The analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.",Yes,37,08.12.2020,14.10.2022,Yes,Yes,"An entity engages in PCL (Patronizing and Condescendig Language) when its language use shows a superior attitude towards others or depicts them in a compassionate way. This effect is not always conscious and the intention of the author is often to help the person or group they refer to (e.g. by raising awareness or funds, or moving the audience to action).",Yes,"Span-Level PCL Categories: Unbalanced power relations; Authority voice; Shallow solution; Presupposition; Compassion; Metaphor; The poorer, the merrier.",Yes,Github,,"https://github.com/Perez-AlmendrosC/dontpatronizeme  If you would like to use the Don't Patronize Me! dataset for research porpuses, please, fill in the following form and we will send it to you as soon as possible: https://forms.gle/VN8hwbdGYkf5KHiKA",Yes,Don't Patronize Me! dataset,1,1,News on Web (NoW) corpus,"Patronizing Language, Condescending Language",English,N/A,Implicitly between 2010 and 2018,N/A,N/A,"The Don't Patronize Me! dataset currently contains 10,637 paragraphs about potentially vulnerable social groups. These paragraphs have been selected from general news stories and have been annotated with labels that indicate the type of PCL language that is present, if any. The paragraphs have been extracted from the News on Web (NoW) corpus (Davies, 2013). To this end, the authors first selected ten keywords related to potentially vulnerable communities widely covered in the media and susceptible of receiving a condescending or patronizing treatment: disabled, homeless, hopeless, immigrant, in need, migrant, poor families, refugee, vulnerable and women. Next, they retrieved paragraphs in which these keywords are mentioned, choosing a similar number of paragraphs for each of the 10 keywords and each of the 20 English speaking countries that are covered in the NoW corpus. All the selected paragraphs come from news stories that were published between 2010 and 2018.",Not discussed,,manual,Yes,"To annotate the dataset, a two-step process has been followed. In the first step, annotators determined which paragraphs contain PCL. Subsequently, in the second step, the annotators indicated which text spans within these paragraphs contain PCL and they labelled each of these text spans with a particular PCL category.

Step 1: Paragraph-Level Identification of PCL

The aim of this annotation step is to decide for each paragraph whether or not it contains PCL. This annotation step proved more difficult than expected, stemming from the often subtle and subjective nature of PCL. To mitigate this, the authors decided to annotate the paragraphs with three possible labels: 0, meaning that the paragraph does not contain PCL, 1, meaning that it is considered to be a borderline case, or 2, meaning that it clearly contains PCL. They computed the Kappa Inter-Annotator Agreement (IA) between two main annotators (annI and ann2) across the three labels.

To maximize the amount of information captured by the annotations, and in particular obtain a finer-grained assessment about borderline cases, they combined the labels provided by the two annotators into a 5-point scale, as follows:

• Label 0: both annotators assigned the label 0 (0 + 0).

• Label 1: one annotator assigned the label 0 and the other assigned the label 1 (0 + 1).

• Label 2: both annotators assigned the label 1 (1 + 1).

• Label 3: one annotator assigned the label 2 and the other assigned the label 1 (2 + 1).

• Label 4: both annotators assigned the label 2 (2 + 2).

Note how partial disagreement between the annotators is thus reflected in the final label. The cases of total disagreement, where one annotator labeled the instance as clearly not containing PCL and the other annotated it as clearly containing PCL (0 + 2), were annotated by ann3. After this supplementary annotation, the paragraph is either labelled as 1, if the third annotator considered the paragraph not to contain PCL, as 2, if they considered it to be a borderline case, or as 3, if they considered the paragraph to clearly contain PCL. In this way, the labels 0 and 4 remain reserved for clear-cut cases. For the experimental analysis presented in this paper, the authors treated paragraphs with final labels 0 and 1 as negative examples (i.e. as instances not containing PCL) and paragraphs with final labels 2, 3 and 4 as positive examples (i.e. as instances containing PCL).

Step 2: Identifying Span-Level PCL Categories

Those paragraphs labelled as containing PCL in Step 1 are collected for further annotation. The aim of this second step is to specify which text spans within these paragraphs contain PCL and to identify which PCL categories these text spans belong to. For this step, the authors used the BRAT rapid annotation tool (Stenetorp et al., 2012). Note that each paragraph might contain one or more text spans with PCL, which may be assigned to the same or to different categories. They compute the IAA for each category.",Yes,"The authors first selected ten keywords related to potentially vulnerable communities widely covered in the media and susceptible of receiving a condescending or patronizing treatment: disabled, homeless, hopeless, immigrant, in need, migrant, poor families, refugee, vulnerable and women. Next, they retrieved paragraphs in which these keywords are mentioned, choosing a similar number of paragraphs for each of the 10 keywords and each of the 20 English speaking countries that are covered in the NoW corpus.","10,637",10-fold cross-validation,10-fold cross-validation,"Three expert annotators, with backgrounds in communication, media and data science.",Yes,The authors provided explanations and examples for each category label in the paper.,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
318,No,"(Faris et al., 2022)",Hate Speech Detection using Word Embedding and Deep Learning in the Arabic Language Context,9th International Conference on Pattern Recognition Applications and Methods,"Jordan, Spain","The objective of this paper is to propose a smart deep learning approach for the automatic detection of cyber hate speech. Particularly, the detection of hate speech on Twitter on the Arabic region. Hence, a dataset is collected from Twitter that captures the hate expressions in different topics at the Arabic region. A set of features extracted from the dataset based on a word embedding mechanism. The word embeddings fed into a deep learning framework. The implemented deep learning approach is a hybrid of convolutional neural network (CNN) and long short-term memory (LSTM) network. The proposed approach achieved good results in classifying tweets as Hate or Normal regarding accuracy, precision, recall, and F1 measure.",Yes,0,11.10.2022,14.10.2022,Yes,Yes,"Hate speech is the use of offensive, abusive, or insulting language towards an individual or a minority of people. The objective of hate speech is disseminating hatred and discrimination based on the grounds of race, sex, religion, or disability.",No,,No,,,,,The dataset,1,1,Twitter,Hate Speech,Arabic,N/A,N/A,N/A,N/A,"The dataset is collected from Twitter using Twitter streaming Application Programming Interface (API) and ""rtweet"" library. The collection of data covers different critical and debatable areas including sport, religion, racism, and journalism. The translated keywords for collecting the tweets: Alwahadat sport club, The Jordanian league, Faisaly Jordan, Islam and terrorism, damage Islam, Racism, Refugees, Freedom, media, homeland, Nahed Hattar, extreme. The total number of collected tweets after removing duplicates and irrelevant tweets is 3696.",Not discussed,,manual,Yes,"The collected tweets were annotated based on the overall perceived meaning of the tweet into Hate or Normal. The total number of Hate tweets is 843, while the number of Normal tweets is 791. The rest of the tweets accounts for 2062 and were annotated as Neutral, which does not exhibit neither Hate nor Normal orientations. The dataset is a combination of merely Hate and Normal classes, where the positive class is the Hate and the negative class is the Normal class.",Not discussed,,1634,80% (~1307),20% (~327),N/A,Not discussed,,Not discussed,,Specific,"religion, race, organization/institution, other",Targeted,"race, gender, religion, disability",Yes,Yes,"Religion, Race, sport, journalism.","religion, race, organization/institution, other",Yes,"Professions/Occupations The collection of data covers different critical and debatable areas including sport, religion, racism, and journalism.",No,,,No,Human
320,No,"(Kennedy et al., 2022)",Introducing the Gab Hate Corpus: defining and applying hate-based rhetoric to social media posts at scale,Language Resources and Evaluation,USA,"The authors present the Gab Hate Corpus (GHC), consisting of 27,665 posts from the social network service gab.com, each annotated for the presence of “hate-based rhetoric” by a minimum of three annotators. Posts were labeled according to a coding typology derived from a synthesis of hate speech definitions across legal precedent, previous hate speech coding typologies, and definitions from psychology and sociology, comprising hierarchical labels indicating dehumanizing and violent speech as well as indicators of targeted groups and rhetorical framing. The authors provide inter-annotator agreement statistics and perform a classification analysis in order to validate the corpus and establish performance baselines. The GHC complements existing hate speech datasets in its theoretical grounding and by providing a large, representative sample of richly annotated social media posts.",Yes,8,16.01.2022,14.10.2022,Yes,Yes,"Hate speech that is inclusive with respect to social scientific and legal precedent, which we call hate-based rhetoric.

Hate-based rhetoric is determined by the extent to which it is dehumanizing, attacking human dignity, derogating, inciting violence, or supporting hateful ideology, such as white supremacy. A necessary requirement under this definition is that hate-based rhetoric is explicitly targeting a social group (or an individual by virtue of their social group).

In the evaluation of slurs against group identity (race, ethnicity, religion, nationality, ideology, gender, sexual orientation, etc.), we define such instances as ""hate-based"" if they are used in a manner intended to woundthis naturally excludes the casual or colloquial use of hate slurs. As an example, the adaptation of the N-slur (replacing the ""-er"" with ""-a\*) often implies colloquial usage. In addition, phrases such as ""I hate my mother-in-law's guts"" should not be classified as hate speech as the target is not hated for their group identity.",Yes,‘‘assaults on human dignity’’ (HD)‘‘Calls for violence’’ (CV) Targeted population ‘‘Vulgarity and/or Offensive language’’ (VO) implicit vs. explicit,Yes,Other,OSF,https://osf.io/edua3/,Yes,The Gab Hate Corpus (GHS),1,1,Gab,Hate Speech,English,N/A,XX.01.2018 - XX.10.2018,N/A,N/A,"The authors downloaded Gab posts from the public dump of data by Pushshift.io\* (Gaffney, 2018). 28,000 posts were randomly sampled, stratified so that they were equally balanced across months from the time period of January, 2018 to October, 2018. The size of this sample was roughly supported by previous datasets, for example Davidson et al. (2017). Posts were only considered which had less than four non-hyperlink, non-hashtag tokens.",Not discussed,,manual,Yes,"Annotators were undergraduate research assistants (RAs) trained by first reading the typology and coding manual and then passing a test of about thirty messages that had been previously annotated and agreed upon by the authors.

Annotators were provided with a written guide to prevent secondary trauma, which encouraged annotators to pay attention to signs of hyperarousal, attend to changes in cognition, take breaks, and avoid picturing traumatic situations. It also encouraged annotators to contact researchers if they were experiencing symptoms of PTSD, which were also listed on the guide.

Annotators accessed the raw text of Gab posts via a secure online portal created for text annotation, and could halt at any time. The number of posts annotated per annotator (n = 18, M = 5109, Mdn = 4044) ranged from 288 to 13,543. Posts were discarded that were annotated by less than three annotators.

To evaluate inter-annotator agreement, the authors computed Fleiss's kappa for multiple annotators and the Prevalence-Adjusted and Bias-Adjusted Kappa (PABAK) for Fleiss.",Not discussed,,"27,665 posts","27,665 posts",5510,Undergraduate research assistants (RAs),Yes,"A coding manual was generated based on legal precedents surrounding hate speech in Germany, Australia, the Netherlands, and other countries that criminalize speech beyond the standards in the United States, which is well-known to protect many categories of speech (Howard, 2019).

This document contains the initial description of hate-based rhetoric, including its legal foundations and similarities to typologies in computational research, as well as examples used to instruct annotators on each category. The manual is available on OSF along with the dataset.",Not discussed,,General,N/A,Targeted,"race, religion, nationality, political, gender, sexuality, other",Yes,No,,,No,,No,,,No,Human
321,No,"(Fajri et al., 2021)",PS3: Partition-Based Skew-Specialized Sampling for Batch Mode Active Learning in Imbalanced Text Data,Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track,Netherlands,"Automatic hate-speech detection is facing two major challenges, namely i) the dynamic nature of online content causing significant data-drift over time, and ii) a high class-skew, as hate-speech represents a relatively small fraction of the overall online content. The first challenge naturally calls for a batch mode active learning solution, which updates the detection system by querying human domain-experts to annotate meticulously selected batches of data instances. However, little prior work exists on batch mode active learning with high class-skew, and in particular for the problem of hate-speech detection. In this work, the authors propose a novel partition-based batch mode active learning framework to address this problem. Their framework falls into the so-called screening approach, which pre-selects a subset of most uncertain data items and then selects a representative set from this uncertainty space. To tackle the class-skew problem, they use a data-driven skew-specialized cluster representation, with a higher potential to “cherry pick” minority classes. In extensive experiments we demonstrate substantial improvements in terms of G-Means, and F1 measure, over several baseline approaches and multiple datasets, for highly imbalanced class ratios.",Yes,2,25.02.2021,14.10.2022,Yes,Yes,"Hate speech was defined as a statement expressing hate or extreme bias towards a particular group, in particular defined via religion, race, gender, or sexual orientation. Offensive or hateful expressions directed towards individuals, without reference to a group-defining property, did not count as hate-speech.",No,,Yes,Github,,https://github.com/rmfajri/PS3,Yes,Representative Set (from 10 public datasets),11,1,"Twitter, Amazon review, Hotel review",Hate Speech,English,N/A,N/A,N/A,N/A,"The authors conducted the experiments on 10 publicly available text datasets from various domains such as hate-speech recognition, sentiment analysis, and product review. They mainly focus on skewed text data as they represent a large group of real world challenges in data science.",Not discussed,,"automated, manual",Yes,"The main objective of PS3 is to develop a scalable batch-model framework for the class-imbalance problem. The success of batch mode active learning depends on selecting representative samples as well as the batch size and total budget constraints.

PS3 operates in T iterations, where in each iteration the learner selects b instances to be labeled by the domain expert, which are subsequently added to the labeled set L to update the classifier. Their screening-based BMAL operates in a two-stage manner, by first screening for a candidate subset of most uncertain samples, and subsequently selecting a representative set from the candidate set to be labeled.

The authors performed a small preliminary study to assess the labeling ability of domain experts. The experiments were performed in the authors’ research lab in an academic setting. They asked 6 fluent English speakers without English linguistics background to label the pilot set (dataset Founta). The guidelines for the labeling procedure were provided to the annotators beforehand in order to ensure consistency with the ground-truth labels. Every hour each domain expert provided a vector of instances. The average length of the vector over domain experts, denoted as B, defines the average number of queries a domain expert can handle hourly. They ran all the experiments for 8 h as it would represent a full working day. On average, the annotators labeled ~2.667 sentences per minute. Therefore the BMAL budget was fixed to 180 instances per hour.",Yes,"The Partition-based Skew-Specialized Sampling (PS3) for batch-mode active learning framework has been specifically designed to tackle the class-imbalance problem in real-world applications, where annotators work over limited hours daily. The framework is comprised of two main components: i) Batch-mode imbalance learning that mainly focuses on finding a compromise between exploration and exploitation to effectively cover the uncertain space subject to a predefined budget. ii) Human-in-the-Loop, that models the real-domain expert behavior in a working environment.

PS3 follows a screening approach which, in each iteration, pre-selects a pool of uncertain samples of which a representative batch is then selected for annotation. In contrast to the objective-driven (or direct) approach, the screening approach generally scales much more gracefully to large data. The selection process incorporates a preference for the minority class, leading to a more balanced query set. In particular, they use Gaussian mixture models to capture the diversity within the uncertain set, and select a representative number from each cluster to be annotated. Since the choice of representatives is key in any partition-based active learning framework, they propose a novel data-driven partition representation, targeted at skewed data.",11863,2622,1181,pilot set: 6 fluent English speakers without English linguistics background,Yes,"The guidelines for the labeling procedure were provided to the annotators beforehand in order to ensure consistency with the ground-truth labels.

Annotators had to label ""hate-speech"" (y = 1) vs. ""no hate-speech"" (y = 0). Hatespeech was defined as a statement expressing hate or extreme bias towards a particular group, in particular defined via religion, race, gender, or sexual orientation. Offensive or hateful expressions directed towards individuals, without reference to a group-defining property, did not count as hate-speech.",Not discussed,,General,N/A,Targeted,"religion, race, gender, sexuality",Yes,No,,,No,,No,,,No,Human
325,No,"(Sap et al., 2020)",Social Bias Frames: Reasoning about Social and Power Implications of Language,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,USA,"Most semantic formalisms, to date, do not capture the pragmatic implications in which people express social biases and power differentials in language. The authors introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, they introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. They then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. The authors find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. This study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",Yes,166,05.07.2020,14.10.2022,Yes,Yes,"SOCIAL BIAS FRAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames, the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore, our formalism combines hierarchical categories of biased implications such as intent and offensiveness with implicatures described in free-form text such as groups referenced and implied statements.",Yes,"variables (not categories): offensiveness, intent to offend, Lewd, Group implications, Targeted group, Implied statement, In-group language",Yes,Website,,http://tinyurl.com/social-bias-frames,Yes,Social Bias Inference Corpus (SBIC),1,1,"Reddit, Twitter, Stormfront, Gab",Social Bias,English,N/A,N/A,N/A,N/A,"The authors draw from various sources of potentially biased online content to select posts to annotate. Since online toxicity can be relatively scarce, they start by annotating English Reddit posts, specifically three intentionally offensive subReddits and a corpus of potential microaggressions from Breitfeller et al. (2019). By nature, the three offensive subreddits are very likely to have harmful implications, as posts are often made with intents to deride adversity or social inequality. Microaggressions, on the other hand, are likely to contain subtle biased implications a natural fit for SOCIAL BIAS FRAMES.

In addition, they include posts from three existing English Twitter datasets annotated for toxic or abusive language, filtering out @-replies, retweets, and links. They mainly annotate tweets released by Founta et al. (2018), who use a bootstrapping approach to sample potentially offensive tweets. They also include tweets from Waseem and Hovy (2016) and Davidson et al. (2017), who collect datasets of tweets containing racist or sexist hashtags and slurs, respectively.

Finally, they include posts from known English hate communities: Stormfront (de Gibert et al., 2018) and Gab, which are both documented white-supremacist and neo-nazi communities (Bowman-Grieve, 2009Hess, 2016), and two English subreddits that were banned for inciting violence against women (r/Incels and r/MensRightsFingas, 2017Center, 2012).",Not discussed,,manual,Yes,"The authors design a hierarchical annotation framework to collect biased implications of a given post on Amazon Mechanical Turk (MTurk).

For each post, workers indicate whether the post is offensive, whether the intent was to offend, and whether it contains lewd or sexual content. Only if annotators indicate potential offensiveness do they answer the group implication question. If the post targets or references a group or demographic, workers select or write which onesper selected group, they then write two to four stereotypes. Finally, workers are asked whether they think the speaker is part of one of the minority groups referenced by the post.

The authors collect three annotations per post, and restrict our worker pool to the U.S. and Canada. They ask workers to optionally provide coarse-grained demographic information.",Not discussed,,"147,139 tuples",75% (~110354),12.5% (~18392),"In the final annotations, the worker pool was relatively gender-balanced and age-balanced (55% women, 42% men, &lt;1% non-binary; 36±10 years old), but racially skewed (82% White, 4% Asian, 4% Hispanic, 4% Black). The worker pool was restricted to the U.S. and Canada.",Yes,"Snippet of the annotation task used to collect SBIC is available in Appendix. The collection of structured annotations for our framework is broken down into questions pertaining to offensiveness, intent of the post, targeted group and minority speaker. Workers can see short explanations for the questions while annotating.",Yes,paying workers above minimum wage ($7–12),Specific,"race, gender, sexuality, religion, age, body, class, political, other",Non-targeted,N/A,Yes,Yes,"Race/ethnicity, Gender/Sexuality, Religion, Age/body, Mental or physical disabilities/disorders, Class/political/lifestyle, crime/violence/tragedy victims","race, gender, sexuality, religion, age, body, class, political, other",No,,No,,,No,Human
327,No,"(Adikara et al., 2020)",Detection of cyber harassment (cyberbullying) on Instagram using naïve bayes classifier with bag of words and lexicon based features,Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology,Indonesia,"The focus of this research is to detect cyberbullying on Instagram comment into two classes, cyberbullying or non-cyberbullying. The detection process consists of several steps, starts with preprocessing, followed by feature extraction, and the last is classification or in this case, cyberbullying detection. In this research, Naïve Bayes classifier with Bag of Words and Lexicon based features is employed to detect the cyberbullying. The Bag of Words features are extracted from the terms occurred in the comment and Lexicon-based features are extracted by using a dictionary or commonly known as sentiment lexicon. Since Indonesian is a low resource language, it is interesting and challenging to investigate this topic by using Indonesian dataset. In this experiment, the highest evaluation results are obtained by combining Bag of Word features and Lexicon-based features than using the features independently. The authors use 5-fold cross-validation and the system yields accuracy 0.872, precision 0.948, recall 0.824, and f-measure 0.874.",Yes,6,26.11.2020,14.10.2022,Yes,Yes,"Cyberbullying or in other terms, online bullying, electronic bullying, and Internet harassment with many but similar definitions as the willful, repeated harm, imbalance of power to others with the use of electronic devices or electronic communication technologies.",No,,No,,,,,The dataset,1,1,Instagram,Cyberbullying,Indonesian,N/A,N/A,N/A,N/A,The data used in this study is gathered randomly from Instagram comments in Bahasa Indonesia (Indonesian).,Not discussed,,manual,Not discussed,,Not discussed,,350 comments,50,300,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
328,No,"(Kim et al., 2022)",Understanding and identifying the use of emotes in toxic chat on Twitch,Online Social Networks and Media,"South Korea, USA","People continuously find new forms of hateful expressions that are easily identified by humans, but not by machines. One such common expression is the mix of text and emotes, a type of visual toxic chat that is increasingly used to evade algorithmic moderation and a trend that is an under-studied aspect of the problem of online toxicity. This research analyzes chat conversations from the popular streaming platform Twitch to understand the varied types of visual toxic chat. Emotes were sometimes used to replace a letter, seek attention, or for emotional expression. The authors created a labeled dataset that contains 29,721 cases of emotes replacing letters. Based on the dataset, they built a neural network classifier and identified visual toxic chat that would otherwise be undetected through traditional methods and caught an additional 1.3% examples of toxic chat out of 15 million chat utterances.",Yes,3,01.01.2022,14.10.2022,Yes,Yes,"Toxic chat. ""Toxic chat, a hateful speech or an offensive language via chat, is known to have a negative effect on communication and emotion.""",No,,No,,,,,The dataset,1,1,Twitch,"Toxicity, Visual Toxic Chat",English,XX.03.2018 - XX.06.2018 (three-month time from March 2018),N/A,The authors manually inspected the country each streamer resided in and identified those streamers in English-speaking countries.,N/A,"For data collection, the authors identified top-100 streamers from the public lists like TwitchMetrics and examined their recent video logs. The authors manually inspected the country each streamer resided in and identified those streamers in English-speaking countries. Any non-individual accounts (such as e-sports brand accounts) were excluded for consistency in analysis. For the chosen 54 streamers, they gathered logs from the most recent 100 videos and their accompanying chats via the npm JavaScript package. The data collection took a three-month time from March 2018. Each utterance (i.e., all input separated by a user entering the enter key) was considered as the unit of conversational analysis and call this chat. In total, they collected 103,182,314 chats in 4333 videos from 54 channels. This finding corresponds to, on average, 23,813 chats per video.

After pre-processing and annotation, the authors augmented the data collection process by gathering an extensive collection of toxic word dictionaries and then synthetically replacing a single letter of any chosen word with a emote. In doing so, they may utilize the frequency distribution of the substituted letters and emotes appearing in our Twitch chat data. Five data sources were used on toxic chat: (1) a collection of swear words from a free online repository, (2) a group of bad words from a CMU lab, (3) the list of blocked words related to profanity by FrontGate, (4) the list of Google blacklisted words by Free Web Headers, and (5) the list of blocked words suggested to YouTube moderators by Free Web Headers. From the compiled collection of toxic words from these resources, they generated an additional 18,236 emoted-annotated toxic words that will be used for training a neural model.",Not discussed,,manual,Yes,"Before launching the crowdsourcing task, a pilot test was run to gain insights for labeling. The authors randomly chose 600 emote-annotated chats and hired 18 internal group annotators to label these chats. At least three annotators marked each chat utterance, classifying the emote usage into either: substitution, non-substitution, and do not know. For the substitution cases, annotators made their best guesses and provided which alphabets might have been replaced. The authors have asked, ""How familiar are we with Twitch as a preliminary question?"" to associate the quality of labeling with their familiarity with Twitch.

This pilot test revealed several findings. First, many restored words were not necessarily appropriate dictionary words because people commonly use abbreviations, slang, and memes. This finding calls for a challenge in utilizing standard word dictionaries in the model. Second, familiarity with Twitch was critical in the restoration quality. Labelers, who have never used Twitch, naturally could not guess community-specific terms. If a model were to utilize a dictionary matching scheme, the community-specific context would have been missed. This finding calls for the importance of knowing the community culture in the labeling process. Third, labelers took disproportionately different amounts of time to complete the taskTwitch users took on average 25 min to label 200 chats, whereas nonusers took on average 59 min. This finding suggests the different levels of burdens and skills required in the task.

The authors conducted larger-scale labeling with the Amazon Mechanical Turk (MTurk) platform following the pilot test insights. They created an assignment (HIT) on the emote restoration task over 20 individuals and asked participants' familiarity with Twitch (i.e., ""Are you a frequent user of Twitch?""). Then the participants were asked to restore 'emoted' text into all text words. Based on this pre-test, the authors decided to hire only users familiar with Twitch and paid participants 1 USD for restoring 50 emoted words, which on average took 6 min, yielding an hourly rate of 10 USD. There was no constraint on participants related to age, gender, race, or socioeconomic status. The HIT recruited English speaking participants who were familiar with Twitch.

They hired 3000 annotators who were each assigned 50 chat utterances to label, yielding a total of 150,000 decisions. Each chat utterance was given to at least three users to break a tie when needed. All 3000 users were familiar with Twitch, and they passed a screener that asked them a question about their knowledge of Twitch culture: They asked the meaning of the ‘Kappa’ emote, and only those who gave a correct answer (i.e., sarcasm) were invited as annotators.

The task was to provide labels for an emote-annotated wordturkers could choose (1) an emote used to express emotion without any substitutions or (2) an emote is substituting specific letters. We have also asked them to make their best guesses on the original words in the latter case.

The MTurk task yielded 11,485 labels, which is not enough to build a neural network model, nor all of them are related to toxic chat. The authors hence augmented the data collection process by gathering an extensive collection of toxic word dictionaries and then synthetically replacing a single letter of any chosen word with a emote.",Yes,"From 103 million chats, the authors pre-processed the data in the following steps. First, they removed all messages written by chatbots. To identify chatbots, they targeted chat accounts that post repetitive messages through data alignment and tested whether the poster is using some macro and hence, is a chatbot via the Twitch API. This ratio corresponded to 2.16% of all data. Second, they removed text-only or emotes-only chats because the goal is to analyze conversations involving both emotes and text, corresponding to 66.9% and 18.3% of all data, respectively. Third, they removed chats where the same emote appears three consecutive times. This step was necessary because no English words contain the same letters three or more consecutive times. Therefore, if emotes were used more than three times continuously within the chat, it is excluded. This step corresponded to 14.8% of all data. Fourth, they removed duplicated chats or chats of identical content and length. Of the total data, 3,552,397 (3.44%) of the conversations were selected in this manner.","29,721 emoted words","29,721 emoted words","15,290,530 chats",The HIT recruited English speaking participants who were familiar with Twitch.,Not discussed,,Yes,1 USD for restoring 50 emoted words,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
329,No,"(Mathur et al., 2018)",Did you offend me? Classification of Offensive Tweets in Hinglish Language,Proceedings of the 2nd Workshop on Abusive Language Online (ALW2),India,"This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish (derived by the blending of Hindi with the English language) tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.",Yes,77,31.10.2018,14.10.2022,Yes,Yes,"Social media is rife with such offensive content that can be broadly classified as abusive and hate-inducing on the basis of severity and target of the discrimination. Hate speech (Davidson et al., 2017) is an act of offending a person or a group as a whole on the basis of certain key attributes such as religion, race, sexual orientation, gender, ideological background, mental and physical disability. On the other hand, abusive speech is offensive speech with a vague target and mild intention to hurt the sentiments of the receiver.",Yes,"Offensive: hate inducing, or abusive",Yes,Github,,https://github.com/pmathur5k10/Hinglish-Offensive-Text-Classification,Yes,Hinglish Offensive Tweet (HOT) dataset; English Offensive Tweet (EOT) dataset,2,1,Twitter,"Offensiveness, Hate speech, Abusiveness",Hindi-English,XX.11.2017 - XX.02.2018,XX.11.2017 - XX.02.2018,Indian subcontinent (The tweets were mined by imposing geolocation restriction),N/A,"HOT was created using the Twitter Streaming API by selecting tweets having more than 3 Hinglish words. The tweets were collected during the interval of 4 months of November 2017 to February 2018. The tweets were mined by imposing geolocation restriction such that tweets originating only in the Indian subcontinent were made part of the corpus. Inspired by the work of Rudra et al. (2016), tweets were mined from popular Twitter hashtags of viral topics popular across the news feed.

Bali et al. (2014) pointed out that Indian social media users have high activity on Facebook pages of a few listed prominent public entities. Hence, the authors crawled tweets and responses from Twitter handles of sports-persons, political figures, news channels and movie stars. The collected corpus of tweets initially had 25667 tweets which was filtered down to remove tweets containing only URL's, only images and videos, having less than 3 words, non-English and nonHinglish scripts and duplicates.",Not discussed,,manual,Yes,"The annotation of HOT tweets were done by three annotators having sufficient background in NLP research. The tweets were labeled as hate speech if they satisfied one or more of the conditions: (i) tweet used sexist or racial slur to target a minority, (ii) undignified stereotyping or (iii) supporting a problematic hashtags such as #ReligiousSe\*m. The label chosen by at least two out of three independent annotators was taken as final ground truth for each tweet. In case of conflict amongst the annotators, an NLP expert would finally assign the ground truth annotation for ambiguous tweets. In this way, 386 tweets needed expert annotation, while 2803 tweets were labeled through consensus of annotators with an average value of Cohen Kappa's inter-annotator agreement k = 0.83.",Not discussed,,14509 (EOT); 3189 (HOT),11608 (EOT); 2551 (HOT),2944 (EOT); 637 (HOT),"Three annotators who have sufficient background in NLP research.

An NLP expert would finally assign the annotation in case of conflict.",Not discussed,,Not discussed,,General,N/A,Targeted,"religion, race, sexuality, gender, political, disability",Yes,No,,,No,,No,,,No,Human
330,No,"(Mall et al., 2020)",Four Types of Toxic People: Characterizing Online Users’ Toxicity over Time,"Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society","Qatar, India, Finland","In this research, the authors design two metrics to identify toxic user types: F score that captures the changes in a user’s toxicity, and G score that captures the direction of the shift taking place in the user’s toxicity pattern. They apply these metrics to a dataset of 4M user comments from Reddit by defining four toxic user types based on the toxicity scores of a user’s comments: (a) Steady Users whose toxicity scores are steady over time, (b) Fickle-Minded Users that switch between toxic and non-toxic commenting, (c) Pacified Users whose commenting becomes less toxic in time, and (d) Radicalized Users that become gradually toxic. Findings from the Reddit dataset indicate that fickle-minded users form the largest group (31.2%), followed by pacified (25.8%), radicalized (25.4%), and steadily toxic users (17.6%). The results suggest that the most typical behavior type of toxicity is switching between toxic and non-toxic commenting.",Yes,7,25.10.2020,14.10.2022,Yes,Yes,Online toxicity can be defined as actions of a user or users that create a negative atmosphere for the other users of social media.,Yes,"Categorizing User Behaviors based on Estimated Toxicity Metrics: Stable Users, Fickle-Minded Users, Pacific Users, Radical Users.",No,,,,,Reddit dataset,1,1,Reddit,Toxicity,English,N/A,XX.XX.2019 - XX.08.2017,N/A,N/A,"To investigate the toxicity of users over time, the authors opt for collecting data from Reddit. For data collection, they focus on the 10 largest subreddits with the highest number of subscribers (http://redditlist.com). For each subreddit, they retrieve the 10 discussions with the highest number of comments starting from 2009 until August of 2017. So, for each subreddit, they had a total of 90 discussions per year, and a total of 900 submissions over the period of 9 years. They focused on submissions with the highest number of comments because the comment number usually implies the existence of more discussions. On average, a discussion has 4, 202 comments.

Using Python's Reddit API Wrapper (PRAW), they collected all the comments associated with each discussion and ensured that the comment objects retrieved from the API included (a) the time stamp, (b) ID of the comment, (c) Username of the person that posted the comment, and (d) the comment text. After collecting the comments, they removed the long comments exceeding 3, 000 characters since these pose issues for the toxicity scoring method that they used. The potential impact of removing these comments in the results is likely very small. They ended up with a large text collectioni.e., a corpus of comments and discussions from the Top 10 subreddits on Reddit.",Yes,the ID of the comment and the username of person that posted the comment are removed.,"automated, manual",Yes,"The authors utilize the Perspective API to score the comments collected for this research. Upon obtaining access to the API from Alphabet, they tested it by inputting requests to score comments. The version of the API at the time of the study had two main types of models: (a) alpha models and (b) experimental models. In this research, we use the alpha category's toxicity model that returns a toxicity score between 0 and 1, where 1 indicates maximum toxicity. According to the API documentation, the returned scores are toxicity probability, i.e., how likely a comment is perceived to be toxic.

To retrieve the toxicity scores, they sent 4, 028, 324 million comments to the Perspective API. Overall, they were able to successfully score 3, 727, 889 comments, representing 92.54% of the comments in the dataset. According to the Perspective API documentation, failure to provide scores can be due to non-English content, and lengthy comments. To further establish the validity of the toxicity scores given by the Perspective API, they performed a manual rating of a random sample of 150 comments. An independent human rater determined if a comment is toxic or not toxic, and they compared this rating to the score given by Perspective API.

They used the threshold of 0.50, so that comments scored below that threshold by Perspective API are considered non-toxic and comments above 0.50 are considered toxic, the human rater also classifying on this binary range. They obtained a simple percentage agreement of 86.7%. We also computed Cohen's Kappa (k) that considers the the probability of agreement by chance in the ratings. Here, they observed 135 agreements (90.0% of the observations), whereas the number of agreements expected by chance would have been 118.5 (79.0% of the observations). The Kappa metric was k = 0.524, indicating a 'moderate' strength of agreement.",Not discussed,,"3, 727, 889 comments; 741, 994 unique users",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
331,No,"(Sahlgren et al., 2018)",Learning Representations for Detecting Abusive Language,Proceedings of the 2nd Workshop on Abusive Language Online (ALW2),Sweden,"This paper discusses the question whether it is possible to learn a generic representation that is useful for detecting various types of abusive language. The approach is inspired by recent advances in transfer learning and word embeddings, and the authors learn representations from two different datasets containing various degrees of abusive language. The authors compare the learned representation with two standard approaches; one based on lexica, and one based on data-specific n-grams. The experiments show that learned representations do contain useful information that can be used to improve detection performance when training data is limited.",Yes,19,31.10.2018,14.10.2022,Yes,Yes,"Abusive language. ""Abusive language is prevalent on the Internet of today. Many users of social media can attest to the frequent occurrence of negative slurs, racist and sexist comments, hate speech, cyberbullying, and outright threats. …In the case of Stormfront, which has been classified as a hate site (Levin, 2002), we can expect to find a wide diversity of abusive language, ranging from negative slurs of various sorts (racial, sexual, religious, political) to explicit threats and targeted hate.""",No,,No,,,,,"Stormfront datasetReddit dataset;

Existing datasets: Twitter HatespeechWikipedia AggressionWikipedia AttackWikipedia Toxicity",6,2,"Stormfront, Reddit, Twitter, Wikipedia",Abusiveness,English,N/A,N/A,N/A,N/A,"The three types of embeddings are trained on two different data setsa collection of roughly 5 million posts from the white supremacist website Stormfront, and a random sample of approximately 54 million comments from the discussion forum Reddit. Both datasets were crawled specifically for these experiments.

Four different datasets are used to evaluate the viability of the different representations for detecting various forms of abusive language. The authors delimit all datasets to 10,000 data points (by random sampling) for efficiency reasons, and in order to use an equal amount of data in all experiments.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"race, gender, religion, political",Yes,No,,,No,,No,,,No,Human
332,No,"(Assimakopoulos et al., 2020)",Annotating for hate speech: The MaNeCo corpus and some input from critical discourse analysis,"LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings",Malta,"This paper presents a novel scheme for the annotation of hate speech in corpora of Web 2.0 commentary. The proposed scheme is motivated by the critical analysis of posts made in reaction to news reports on the Mediterranean migration crisis and LGBTIQ+ matters in Malta, which was conducted under the auspices of the EU-funded C.O.N.T.A.C.T. project. Based on the realization that hate speech is not a clear-cut category to begin with, appears to belong to a continuum of discriminatory discourse and is often realized through the use of indirect linguistic means, it is argued that annotation schemes for its detection should refrain from directly including the label'hate speech,' as different annotators might have different thresholds as to what constitutes hate speech and what not. In view of this, the authors suggest a multi-layer annotation scheme, which is pilot-tested against a binary ±hate speech classification and appears to yield higher inter-annotator agreement. Motivating the postulation of our scheme, they then present the MaNeCo corpus on which it will eventually be used; a substantial corpus of on-line newspaper comments spanning 10 years.",Yes,17,11.05.2020,14.10.2022,Yes,Yes,"Online hate speech might often contain offensive language, but not all offensive language used on the internet can be considered hate speech. That is because hate speech, in its specialised legal sense, is typically tied to the specific criterion of incitement to discriminatory hatred in the several jurisdictions where it is regulated, while the discriminatory attitude needs to specifically target a group that is legally protected.",Yes,"hierarchical annotation scheme:  positive, negative or neutral attitude (if negative) target (individual/group) How is the attitude expressed in relation to the target group? [ Derogatory term / Generalisation / Insult / Sarcasm (including jokes and trolling) / Stereotyping / Suggestion / Threat ]",Yes,Other,upon request,The pilot dataset is available from the authors upon request.,Yes,The MaNeCo corpus,1,1,Times of Malta (the newspaper with the highest circulation in Malta),Hate Speech,"English, Maltese, Maltenglish",XX.01.2017 - XX.01.2017,XX.04.2008 - XX.01.2017,N/A,N/A,"At the moment, the MaNeCo corpus comprises original data donated to the authors by the Times of Malta, the newspaper with the highest circulation in Malta. More specifically, it contains - in anonymised form - all the comments from the inception of the Times of Malta online platform (April 2008) up to January 2017 (when the data was obtained). This amounts to over 2.5 million user comments (over 124 million words), which are written in English, Maltese or a mixture of the two languages, even though the newspaper itself is English-language. The aim is to eventually populate it with data from other local news outlets too to ensure an equal representation of opinions irrespective of a single news portal's political and ideological affiliations. That said, even this dataset on its own is an invaluable resource for hate speech annotation, since it also includes around 380K comments that have been deleted by the newspaper moderators, and which are obviously the ones that tend to be particularly vitriolic. In this regard, seeing that there are generally ""much fewer hateful than benign comments present in randomly sampled data, and therefore a large number of comments have to be annotated to find a considerable number of hate speech instances"" (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data.",Yes,Comments are anonymised before donated.,manual,Yes,"Piloting the new scheme: A total of 24 annotators took part in this pilot study. The participants, who were mostly academics and students ranging between 21 and 60 years of age, were divided into two gender- and age-balanced groups of 12. The first group was asked to label items using simple binary annotation (‡hate speech) on the basis of the definition of hate speech provided within the Maltese Criminal Code, while the second used our proposed multi-level annotation scheme. Both groups were presented with 15 user-generated comments from the Maltese C.O.N.T.A.C.T. corpus in random order.

All individuals in both annotation conditions performed the task independently. Obviously, in order to meaningfully compare the levels of inter-annotator agreement for the two schemes, the authors needed to have an equal number of classes across the board. To achieve this they screened the annotations received for the multi-level condition, with a view to inferring a binary #hate speech classification in this case too.",Not discussed,,in progress,N/A,N/A,N/A,Yes,"The scheme that the authors developed for hate speech annotation in the MaNeCo corpus is hierarchical in nature and expected to lead to the identification of discriminatory comments along the aforementioned scale by Cortese without focusing on such impressionistic categories as intensity or degree of hatefulness, as follows:

1\. Does the post communicate a positive, negative or neutral attitude? [Positive / Negative / Neutral]

2\. If negative, who does this attitude target? [Individual / Group]

(a) If it targets an individual, does it do so because of the individual's affiliation to a group? [Yes / No] If yes, name the group.

(b) If it targets a group, name the group.

3\. How is the attitude expressed in relation to the target group? Select all that apply. [ Derogatory term / Generalisation / Insult / Sarcasm (including jokes and trolling) / Stereotyping / Suggestion / Threat ]",Not discussed,,Specific,other,Non-targeted,N/A,Yes,Yes,"Individual, Group",other,Yes,"Individual, group",Yes,"Individual, group",About a person,No,Human
333,No,"(Vidgen et al., 2021)",Introducing CAD: the Contextual Abuse Dataset,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"UK, USA","Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. The authors introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. The authors report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.",Yes,20,06.06.2021,14.10.2022,Yes,Yes,Online abuse,Yes,"a hierarchical taxonomy of abusive content: six primary categories and additional secondary categories.  The primary categories are:  Abusive (Identity-directed abuse, Affiliation-directed abuse and Person-directed abuse)  Non-abusive (Neutral, Counter Speech and Non-hateful Slurs)",Yes,Other,Zenodo,https://github.com/dongpng/cad_naacl2021  The data + models have been uploaded to Zenodo: http://doi.org/10.5281/zenodo.4881008,Yes,Contextual Abuse Dataset (CAD),1,1,Reddit,Abusiveness,English,01.02.2019 - 31.07.2019,01.02.2019 - 31.07.2019,N/A,N/A,"The authors identified 117 subreddits likely to contain abusive content, which they filtered to just 16, removing subreddits which (1) had a clear political ideology, (2) directed abuse against just one group and (3) did not have recent activity. 187,806 conversation threads were collected over 6 months from 1st February 2019 to 31st July 2019, using the PushShift API (Gaffney and Matias, 2018).",Yes,"The authors follow this guidance and do not provide any direct quotes. They also minimized how many entries they collected from each user so that each one comprises only a small part of the total dataset. At no point did any of the research team contact any Reddit users, minimizing the risk that any harm could be caused to them. Further, they decided not to review any profile information about the users, substantially minimizing the risk that any personally identifiable information is included in the dataset.",manual,Yes,"The titles main body of posts were treated separately, resulting in 1,394 post titles, 1,394 post bodies and 23,762 comments being annotated (26,550 entries in total). All entries were assigned to at least one of the six primary categories. Entries could be assigned to several primary categories and/or several secondary categories. The dataset contains 27,494 distinct labels.

All entries were first independently annotated by two annotators. Annotators underwent 4 weeks training and were either native English speakers or fluent. Annotators worked through entire Reddit conversations, making annotations for each entry with full knowledge of the previous content in the thread. All disagreements were surfaced for adjudication. The authors used a consensus-based approach in which every disagreement was discussed by the annotators, facilitated by an expert with reference to the annotation codebook. This is a time-consuming process which helps to improve annotators' understanding, and identify areas that guidelines need to be clarified and improved. Once all entries were annotated through group consensus they were then reviewed in one-go by the expert to ensure consistency in how labels were applied. This helped to address any issues that emerged as annotators' experience and the codebook evolved throughout the annotation process.",Yes,"The authors used stratified sampling to reduce the sample to 1,394 posts and 23,762 comments (25,156 in total) for annotation.","23,417 entries","13,584 (58%)","5,307 (22.7%)","The dataset includes annotations from 12 trained analysts, who were either native English speakers or fluent. They were recruited through a competitive process. They underwent 4 weeks of training, including numerous one-to-one sessions. Work was conducted over 12 weeks, with each annotator working between 10 and 20 hours each week. Of the 12 annotators who contributed to the final dataset, 11 consented to provide information about their demographics. Age: 7 annotators were 18-29, 3 were 30-39 and 1 was 40-49. Gender: 4 were female and 7 were male. Ethnicity: 8 were white, 1 Latino, 1 of Middle Eastern ethnic origin and 1 was mixed. National identity: 7 were British, 1 American, 1 Ecuadorean, 1 Jordanian and 1 Polish. Social media use: 9 used social media more than once per day, and 2 use it once per day. Exposure to online abuse: All annotators had witnessed online abuse in the previous year, with 10 stating they had witnessed it more than 3 times and 1 stating they had witness it 2-3 times. Disagreements were adjudicated through group discussion with an expert in abusive online content. They are a post-doctoral researcher with extensive experience.",Yes,The codebook is available along with the dataset. It contains detailed instructions for each category label (primary and secondary) and examples for some key concepts.,Not discussed,,Specific,"gender, disability, race",Non-targeted,N/A,Yes,Yes,Identity,other,Yes,Affiliation,Yes,see primary category: Person,"About a person,To a person",Yes,Human
335,No,"(Karim et al., 2020)",Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network,2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA),"Germany, Ireland, Netherlands","In this paper, the authors provide several classification benchmarks for Bengali, an under-resourced language. They prepared three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis. They built the largest Bengali word embedding models to date based on 250 million articles, which we call BengFastText. We perform three experiments, covering document classification, sentiment analysis, and hate speech detection. They incorporate word embeddings into a Multichannel Convolutional-LSTM (MC-LSTM) network for predicting different types of hate speech, document classification, and sentiment analysis. Experiments demonstrate that BengFastText can capture the semantics of words from respective contexts correctly. Evaluations against several baseline embedding models, e.g., Word2Vec and GloVe yield up to 92.30%, 82.25%, and 90.45% F1-scores in case of document classification, sentiment analysis, and hate speech detection, respectively during 5-fold cross-validation tests.",Yes,24,06.10.2020,14.10.2022,Yes,Yes,"Hate speech as defined by the Encyclopedia of American Constitution is ""any communication that disparages a person or a group based on some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics""…",Yes,"type of hate: political, religious, personal, geopolitical and gender abusive",No,,,,,Dataset for hate speech detection; Dataset for document classification; Dataset for sentiment analysis,3,3,"Wikipedia, Daily Prothom Alo, Daily Jugontor, Daily Nayadiganta, Anandabazar Patrika, Dainik Jugasankha, BBC, Deutsche Welle, NTV, ETV Bangla, ZEE News, books, blogs, sports portal, Twitter, Facebook, LinkedIn",Hate Speech,Bengali,N/A,N/A,N/A,N/A,"Bengali articles were collected from numerous sources from Bangladesh and India including a Bengali Wikipedia dump, Bengali news articles (Daily Prothom Alo, Daily Jugontor, Daily Nayadiganta, Anandabazar Patrika, Dainik Jugasankha, BBC, and Deutsche Welle), news dumps of TV channels (NTV, ETV Bangla, ZEE News), books, blogs, sports portal, and social media (Twitter, Facebook pages and groups, LinkedIn). We also categorized the raw articles for ease preprocessing for a later stage. Facebook pages (e.g., celebrities, athletes, sports, and politicians) and newspaper sources were scrutinized because composedly, they have about 50 million followers, and many opinions, hate speech and review texts come or spread out from there. All text samples were collected from publicly available sources. Altogether, the raw text corpus consists of 250 million articles.",Not discussed,,"automated, manual",Yes,"(1) Collected data samples are annotated in a semi-automated way. Inspired by the creative political slang, authors attempted to annotation. They prepare 175 abusive terms that are commonly used to express hates. Subsequently, they assign the label 'hate' if at least one of these terms exists in the text.
(2) The annotations are further validated and corrected by three experts into one of two categories: hate and non hate/inoffensive. To reduce possible bias, each label was assigned based on a majority voting on the annotator's independent opinions. Fortunately, certain types of hate were easy to identify and annotate based on CSPlang that are nonstandard word for Bengali, which conveys a positive or negative attitude towards a person, a group of people, or an issue that is the subject of discussion in political discourse and can be easily annotated as a political hates.
(3) Finally, non-hate statements were removed from the list, and hates were further categorized into political, personal, gender abusive, geopolitical, and religious hates in which 3.5% of annotated texts were classified as hate speech, resulting in 35,000 statements labeled as hate.",Yes,"To prepare the dataset for hate speech detection, they used a bootstrapping approach, which starts with an initial search for specific types of texts or articles containing common slurs and terms used about targeting characteristics. They manually identify frequently occurring terms in the texts containing hate speech and references to specific entities. They annotated 100,000 statements, texts, or articles, which directly or indirectly express hate speech.","35,000 statements",80% (~28000),20% (~7000),Three experts (one South Asian linguist and two native speakers),Not discussed,,Not discussed,,Specific,"religion, gender, political, other",Targeted,"race, gender, sexuality, nationality, religion, other",No,Yes,"Religion, Gender, political, personal","religion, gender, political, other",No,,Yes,"Personal.  see type of hate: political, religious, personal, geopolitical and gender abusive",About a person,No,Human
336,No,"(Wulczyn et al., 2017)",Ex Machina: Personal Attacks Seen at Scale,Proceedings of the 26th International Conference on World Wide Web,USA,"The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. The authors show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. They apply their methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, their methodology allows them to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.",Yes,631,03.04.2017,14.10.2022,Yes,Yes,Personal attack,Yes,target: the recipient or a third party if being reported or quoted if it’s another kind of attack or harassment.,Yes,Github,,https://github.com/ewulczyn/wiki-detox,Yes,Wikipedia comment corpus; random dataset (a subset from the corpus); blocked dataset (sample from blocked users),3,3,Wikipedia,Personal Attacks,English,13.01.2016 (from a public dump),Implicitly between 2004 and 2015,N/A,N/A,"Wikipedia comment corpus: The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015.

Every Wikipedia page, including articles and user pages, has an accompanying ""talk page"" that can be used for communicating with other users. Discussion pages pertaining to user pages are said to belong to the user talk namespace, while discussions pertaining to articles belong to the article talk namespace. Although there are 35 talk namespaces in total, the authors focus on these two throughout the paper since they contain at least an order of magnitude more discussion pages and comments than the others. The authors pursue an alternative approach to generating a corpus of comments, which involves processing the ""revision history"", which represents the history of edits on a page as a sequence of files. There is a separate file, called a revision, corresponding to the state of the article after each edit. They can compute a diff between successive revisions of a talk page to see what text was added as a result of each edit. For completeness, they include the text added in these types of edits in the corpus. In practice, they processed the revision history from a public dump of English Wikipedia made available on 2016-01-13. To generate the set of diffs from the revision history, they used the existing mwdiffs python package along with the standard longest-common-substring diff algorithm. For the purpose of this study they define a talk page comment as the concatenation of the Media Wiki markup added during an edit of a talk page. They also compute a clean, plaintext version of each comment by stripping out any html or MediaWiki markup, which they use in the crowd-sourcing task.

While manually inspecting the data, they found that a large portion of comments left on talk pages (20%-50%, depending on the namespace) were clearly administrative in nature and generated using a bot or template. In this study they are interested in comments made by human users in the context of discussions. After using a regular expression to filter out all messages from these users, they still observed a large number of administrative comments with little or nor modification on many user talk pages. They generated a another set of regular expressions to remove the most commonly occurring comments of this nature.",Not discussed,,manual,Yes,"The crowdsourcing process involves: 1. generating a corpus of Wikipedia discussion comments, 2. choosing a question for eliciting human judgments, 3. selecting a subset of the discussion corpus to label, 4. designing a strategy for eliciting reliable labels.

The questions the authors posed to get human judgments on whether a comment contains a personal attack are as follows:

Does the comment contain a personal attack or harassment?

• Targeted at the recipient of the message (i.e. you suck).

• Targeted at a third party (i.e. Bob sucks).

• Being reported or quoted (i.e. Bob said Henri sucks).

• Another kind of attack or harassment.

• This is not an attack or harassment.

In addition to identifying the presence of an attack, we also try to elicit if the attack has a target or whether the comment quotes a previous attack.

Before settling on the exact phrasing of the question, they experimented with several variants and chose the one with the highest inter-annotator agreement on a set of 1000 comments.

They used the Crowdflower crowdsourcing platform. As a first step to ensuring data quality, each annotator was required to pass a test of 10 questions. These questions were randomly selected from a set that the authors devised to contain balanced representation of both attacking and non-attacking comments. Annotators whose accuracy on these test questions fell below a 70% threshold would be removed from the task. This improved the annotator quality by excluding the worst ~ 2% of contributors. Under the Crowdflower system, additional test questions are randomly interspersed with the genuine crowdsourcing task (at a rate of 10%) in order to maintain response quality throughout the task.

In order to get reliable estimates of whether a comment is a personal attack, each comment was labeled by at least 10 different Crowdflower annotators. This allows the authors to aggregate judgments from 10 separate people when constructing a single label for each comment. Finally, they applied several data cleaning steps to the Crowdflower annotations. This included removing annotations where the same worker labeled a comment as both an attack and not an attack and removing comments that most workers flagged as not being English. They evaluated the quality of the crowd-sourcing pipeline by measuring inter-annotator agreement.",Yes,"To ensure representativeness, the authors undertook the standard approach of randomly sampling comments from the full corpus ( referred as the random dataset). Through labeling a random sample, they discovered that the overall prevalence of personal attacks on Wikipedia talk pages is around 1%.

To allow training of classifiers, they need to create a corpus that contains a sufficient number and variety of examples of personal attacks. In order to obtain these, they enhance our random dataset by also sampling comments made by users who where blocked for violating Wikipedia's policy on personal attacks. In particular, they consider the 5 comments made by these users around every block event. They call this the blocked dataset and note that it has a much higher prevalence of attacks (approximately 17%).",115737 random dataset (a subset from the corpus): 37611 blocked dataset (sample from blocked users): 78126,60% (~69442),20% (~23147),"Crowdflower annotators, 4053 in total.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,"About a person,To a person",No,Human
338,No,"(Charitidis et al., 2020)",Towards countering hate speech against journalists on social media,Online Social Networks and Media,Greece,"In this work, the authors focus on countering hate speech that is targeted to journalistic social media accounts. To accomplish this, a group of journalists assembled a definition of hate speech, taking into account the journalistic point of view and the types of hate speech that are usually targeted against journalists. The authors then compile a large pool of tweets referring to journalism-related accounts in multiple languages. In order to annotate the pool of unlabeled tweets according to the definition, they follow a concise annotation strategy that involves active learning annotation stages. The outcome of this paper is a novel, publicly available collection of Twitter datasets in five different languages. Additionally, we experiment with state-of-the-art deep learning architectures for hate speech detection and use our annotated datasets to train and evaluate them. Finally, they propose an ensemble detection model that outperforms all individual models.",Yes,27,01.05.2020,14.10.2022,Yes,Yes,"The proposed definition is formed after extensive discussions with journalists from the European Journalism Centre through a small focus group and continuous evaluation and feedback from journalists. After looking at hundreds of hateful tweets and several meetings, it was decided that the presence of hate speech should be concluded by answering to two simple key questions. These questions refer to the tweet content and they are presented below:

• Does it target a person or group?

• Does it contain a hateful attack? 1. Violent speech 2. Support for death/disease/harm 3. Statement of inferiority relating to a group they identify with (like LGBTQI) 4. Call for segregation

A positive answer to both bullets should make the annotator flag the post as hate speech. The second question can refer to any of the four subcategories that are listed above.",No,,Yes,Other,Zenodo,Available after request  https://zenodo.org/record/3520152#.XcL0OnUzY5k  https://zenodo.org/record/3520148#.XcL04XUzY5k  https://zenodo.org/record/3520150#.XcL1C3UzY5k  https://zenodo.org/record/3520156#.XcLIGHUzY5k  https://zenodo.org/record/3520157#.XcL1G3UzYSk,Yes,Hate speech and personal attack dataset (En/De/Es/Fr/Gr),5,5,Twitter,Hate Speech,"English, German, Spanish, French, Greek",01.10.2018 - 08.05.2019,N/A,N/A,N/A,"The data collection process started with the creation of a list of journalism-related Twitter accounts. The authors identify a larger number of journalist-related accounts by using Twitter lists. Twitter lists are curated groups of Twitter accounts and are usually centered around specific themes. In order to find Twitter lists related to journalism, they collected the Twitter accounts of well-known news outlets and journalism-related organizations and automated the process of fetching all list members. Despite the fact that the majority of the collected accounts are related to either journalists or news outlets, they also noticed the presence of non-journalistic accounts, which could potentially tamper with the journalistic focus of our data collection. To tackle this issue, they manually filtered the irrelevant accounts. Since annotating (for relevance with journalism) the full list of accounts would involve a considerable manual effort, they prioritized the annotation of the most popular accounts, since those accounts usually attract a larger number of tweets. In the first iteration of account collection, they annotated the accounts using the following classes: a) journalist, b) news outlet, c) irrelevant. The initial seed consists of 200 manually collected journalism-related accounts for each language.

Having a validated list of journalism-related Twitter accounts for each language, the authors set up, as the next step, a mechanism to collect tweets from the feeds of these accounts. To this end, the Twitter Search API is used, which returns a sample of Tweets posted in the past 7 days. The API is rate-limited at 180 requests per 15-min window when user authentication is used and at 450 requests per 15-min window when application authentication is used. They used application authentication having also in mind that each call can return a maximum of 100 tweets. Despite these limitations, by effectively utilizing the API within the imposed call rate limitations, they manage to collect a large number of tweets that is sufficient for creating a sizeable, journalism-oriented hate speech database. Typically, the Twitter Search API query consists of a series of keywords that should be contained in the set of returned tweets, along with account-based search operators. They opted for using search queries that do not restrict the tweet contents and only used account-based search operators to limit results. More specifically, they used the to: (e.g., to:BBCNews) and the ""@"" (e.g. @BBCNews) operators in order to collect tweets authored in reply to and tweets mentioning those specific accounts.

Specifically, every 15 minutes, the module sequentially performed N = 450 - N(safe) API requests, evenly distributed across the 15-minute window. N(safe) [0, 450) corresponds to a safety parameter that is used to avoid pushing the API to its limits. We use N(safe) = 200 and thus a maximum of I = 250 calls is performed per every 15-min window. Since each call is associated with one account, and the number of calls that can be performed within each 15-min window is much smaller than the pool of target accounts, an account prioritization/selection mechanism is implemented. This prioritization approach fetched data from all accounts and measured an estimate of the rate of incoming tweets. Using this estimate, the available API calls are distributed on accounts, which are expected to have received a sufficient number of new tweets since the last time that they were queried. Specifically, whenever a new API call is performed, an estimated number of available new tweets is calculated for each account based on the account's estimated incoming tweet rate and the time that has passed since the last time it was fetched. Then, one account is randomly selected among those whose estimated number of available new tweets is larger than a user-specified threshold (for which reasonable values should lie close to 100, the maximum number of results per request). After a call is performed, the incoming tweet rate for each account is updated by dividing the number of returned tweets with the time difference between the newest and the oldest tweet (a very low rate is assigned in case a call returns zero results). Moreover, the last fetched timestamp is recorded to facilitate the calculation of the estimated number of incoming tweets. Initially, all accounts are assigned a fixed, high incoming tweet rate to ensure that all accounts will be fetched at least once. This approach ensured that all accounts were queried proportionally to their actual, dynamic incoming tweet rate, thus maximizing the amount of tweets that can be collected.

They apply this approach for every language {EN, DE, ES, FR, GR}, mining tweets from the corresponding pool of journalistic Twitter accounts and store the retrieved tweets in a mongoDB database.",Not discussed,,manual,Yes,"The annotation process consists of two sampling stages, the initial sampling stage and the active learning sampling stage.

In the initial sampling stage, they use two approaches and select a defined number of tweets from the unlabelled pool to generate the initial annotation batch for each language. The initial sampling batch of each language is submitted for human annotation.

The active learning sampling stage, generates all the subsequent batches, for further annotation and dataset expansion, through an iterative process. In the first iteration, they use labels generated from the initial sampling stage, and they employ an active learning sampling approach to generate the second batch from the unlabelled pool. They then submit the second batch for annotation and expand the labels from the first batch to the second by appending the new annotated batch. They repeat this process until there are no more tweets available in the unlabelled pool or they exhaust the annotation budget.",Yes,"The authors consider two different approaches for generating the annotation batches.

The first and the most popular approach in the literature, is using keyword-based sampling and thus creating the annotation batch with tweets containing specific keywords. In the hate speech context, these keywords are usually offensive words or words that can be used to express hatred. The second approach, which is novel in the literature, is to use existing hate speech datasets in order to train hate speech detection models and, in a subsequent step, apply these models to the pool of unlabeled tweets and sample the tweets with higher hate speech probability. For this task they train a CNN model.

To moderate bias for the keyword-based approach, they compile a large list of keywords. These lists included offensive slang, phrases and words that could potentially express hate when used in the appropriate context and keywords related to several forms of possible discrimination (religion, gender, refugees etc). For the dataset-based approach, they adopt three different approaches to tackle bias. First, they use datasets from works that have less strict definition of hate speech compared to the theirs. Second, in cases where the dataset-based approach is not applicable due to lack of existing datasets they combine multiple different datasets with diverse definitions and scopes. Third, they use a loose classification threshold in the resulting CNN model, sampling tweets from a wide range of hate speech probability. Finally, to further reduce bias in the generated annotation batch, they concatenate random sampled tweets for both keyword-based and dataset-based approaches.",Hate speech and personal attack dataset (En/De/Es/Fr/Gr): 92022/43735/37688/39109/61481,(En/De/Es/Fr/Gr): 73855/34987/30150/29109/49184,(En/De/Es/Fr/Gr): 18167/9047/7538/7822/12297,Five annotators (one for each language) and one supervisor,Not discussed,,Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,Yes,journalists,No,,,No,Human
339,No,"(Ibrahim et al., 2018)",Imbalanced Toxic Comments Classification Using Data Augmentation and Deep Learning,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),Egypt,"In this paper, the authors use data from Wikipedia talk page edits to train multi-label classifier that detects different types of toxicity in online user-generated content. They present different data augmentation techniques to overcome the data imbalance problem in the Wikipedia dataset. The proposed solution is an ensemble of three models: convolutional neural network (CNN), bidirectional long short-term memory (LSTM) and bidirectional gated recurrent units (GRU). They divide the classification problem into two steps, first they determine whether or not the input is toxic then they find the types of toxicity present in the toxic content. The evaluation results show that the proposed ensemble approach provides the highest accuracy among all considered algorithms. It achieves 0.828 F1-score for toxic/non-toxic classification and 0.872 for toxicity types prediction.",Yes,60,17.12.2018,14.10.2022,Yes,Yes,"Toxic comments. …The abusive online content can fall in multiple categories of toxicity such as identity-based hate, threat, insult or obscene.",Yes,"six types of toxicity: toxic, severe toxic, obscene, threat, insult, and identity hate.",No,,,,,"No Augmentation (Wikipedia’s talk page edits dataset);

Unique Words Augmentation;

Unique Words and Random Mask;

Unique Words, Random Mask and Synonyms Replacement",4,3,Wikipedia,Toxicity,English,N/A,N/A,N/A,N/A,"For models training and evaluation, the authors used Wikipedia's talk page edits dataset provided by Toxic Comment Classification Kaggle competition. The dataset contains 159571 records of Wikipedia comments which have been labeled by human raters for toxic behavior. The data has six types of toxicity (six classes) which are: toxic, severe toxic, obscene, threat, insult and identity hate. They used 80% of the records for training, 10% for validation during models parameters tuning and 10% for testing.

To overcome the classes imbalance problem, they applied data augmentation on training set by generating extra new samples from the comments that belong to the rare categories (severe toxic, threat and identity hate). They used the following methods to create new comments from existing ones.

• Unique Words Augmentation: For each comment of the minority classes, they remove duplicate words from it and create a new comment with only unique words.

• Random Mask: From a single comment, they create different new comments by randomly removing up to 20% of the original comment words.

• Synonyms Replacement: They create a new comment from an existing one by replacing the original comment words with synonyms.",Not discussed,,manual,Not discussed,,Not discussed,,159571,80% (~127657),10% (~15957),N/A,Not discussed,,Not discussed,,Specific,other,Non-targeted,N/A,Yes,Yes,identity,other,No,,No,,,No,Human
340,No,"(Abebaw et al., 2022)",Multi-channel Convolutional Neural Network for Hate Speech Detection in Social Media,Advances of Science and Technology,"Ethiopia, Austria","Recently, deep neural network-based hate speech detection models, particularly conventional single-channel Convolutional Neural Network (CNN), have achieved remarkable performance. However, the effectiveness of the models depends on the type of language they are trained on and the training data size. The authors argue that the effectiveness of the models could further be enhanced if using multi-channel CNN models even for under-resourced languages that have limited training data size. This is because the single-channel CNN might fail to consider the potential effect of multiple channels to generate better features, which is not well investigated for hate speech detection. Therefore, in this work, the authors explore the use of multi-channel CNN to extract better features from different channels in an end-to-end manner on top of a word2vec embedding layer. Tested on a new small-scale Amharic hate speech dataset containing 2000 annotated social media comments, the experimental results show that the proposed multi-channel CNN model outperforms the single-channel CNN models but underperform from the baseline Support Vector Machine (SVM) with an average F-score of 81.3%, 78.2%, and 92.5% respectively. The finding of the study implies that the proposed MC-CNN model can be used as an alternative solution for hate speech detection using a deep learning approach when dataset scarcity is an issue.",Yes,1,01.01.2022,14.10.2022,Yes,Yes,"One feature that we can observe and adapt in all of the definitions above is that hate speech is an attack on people's identities. Taking this into account, we developed a working definition of hate speech for this study to offer a common specific boundary for the Amharic hate speech labeling procedure. Hence, we define hate speech as textual social media comments that promotes discrimination against individuals or groups based on their nationality, ethnic and religious affiliation, gender, or disability.",No,,No,,,,,the dataset,1,1,Facebook,Hate Speech,Amharic,N/A,15.04.2019 - 15.12.2019,N/A,N/A,"Since there is no benchmark dataset for Amharic hate speech detection, the authors built their own using the Ethiopian Broadcasting Corporation (EBC) Facebook page (https://www.facebook.com/EBCzena) and some chosen individual Facebook pages that publish hateful comments. They extracted selected comments/posts pertaining to race, religion, and ethnicity using the Facepager API, resulting in a set of 30,000 comments between April 15, 2019 and December 15, 2019.",Not discussed,,manual,Yes,"Three annotators manually annotated the selected samples (5000) as ""Hate"" or ""not-Hate"" from which 2,000 (1,600 for training and 400 for testing) examples were chosen according to the label agreement. The average Cohen's Kappa agreement score was 80%, indicating a good agreement.",Yes,"A total of 5,000 comments/posts were chosen at random for annotation, while the remaining 25,000 were not.",2000,1600,400,Three annotators (two candidate PhD. in Linguistics and one MSc. in Law),Yes,"The Ethiopian government's hate speech and misinformation prevention and suppression proclamation, as well as the authors’ definition of hate speech and the hate speech characterization lists, were provided to the annotators. Accordingly, a speech is labeled as ""Hate"" when:

• ""the speech targets a group or individual as a member of a group (ethnicity, race, religion)""

• ""the speech content in the message expresses hatred""

• ""the speech causes a harm""

• ""the speaker intends harm or bad activity""

• ""the speech incites bad actions""

• ""the speech is either public and directed at a member of the group""

• ""the context makes violent response possible"".",Not discussed,,General,N/A,Targeted,"nationality, race, religion, gender, disability",Yes,No,,,No,,No,,,No,Human
341,No,"(Zannettou et al., 2020)",A quantitative approach to understanding online antisemitism,"Proceedings of the 14th International AAAI Conference on Web and Social Media, ICWSM 2020","Germany, USA","In this paper, the authors present a large-scale, quantitative study of online antisemitism. They collect hundreds of million posts and images from alt-right Web communities like 4chan's Politically Incorrect board (/pol/) and Gab. Using scientifically grounded methods, they quantify the escalation and spread of antisemitic memes and rhetoric across the Web. They find the frequency of antisemitic content greatly increases (in some cases more than doubling) after major political events such as the 2016 US Presidential Election and the ""Unite the Right"" rally in Charlottesville. They extract semantic embeddings from the corpus of posts and demonstrate how automated techniques can discover and categorize the use of antisemitic terminology. They additionally examine the prevalence and spread of the antisemitic ""Happy Merchant""meme, and in particular how these fringe communities influence its propagation to more mainstream communities like Twitter and Reddit. Taken together, the results provide a datadriven, quantitative framework for understanding online antisemitism. The methods serve as a framework to augment current qualitative efforts by anti-hate groups, providing new insights into the growth and spread of hate online.",Yes,69,08.06.2020,14.10.2022,Yes,Yes,"Antisemitism. ""Antisemitism in particular is seemingly a core tenet of alt-right ideology, and has been shown to be strongly re- lated to authoritarian tendencies not just in the US, but in many countries (Dunbar and Simonova 2003; Frindte, Wet- tig, and Wammetsberger 2005).""",No,,Yes,Other,upon request,The data will be made available upon request.,Yes,"/pol/ dataset, Gab dataset,

Existing dataset: Twitter dataset and Reddit dataset (Zannettou et al. 2018b);

The Donald (T_D) dataset (separated from the reddit dataset)",5,5,"4chan, Gab, Reddit, Twitter",Antisemitism,English,N/A,XX.07.2016 - XX.01.2018 (/pop/) XX.08.2016 - XX.01.2018 (Gab) XX.07.2016 - XX.07.2017 (Twitter and Reddit),N/A,N/A,"/pol/: To obtain data from /pol/ posts the authors use the same crawling infrastructure as discussed in (Hine et al. 2017), while for the images they use the methodology discussed in (Zannettou et al. 2018b). Specifically, they obtain posts and images posted between July 2016 and January 2018, hence acquiring 67,416,903 posts and 5,859,439 images.

Gab: To obtain data from Gab, the authors use the same methodology as described in (Zannettou et al. 2018a) and (Zannettou et al. 2018) for posts and images, respectively. Overall, they obtain 35,528,320 posts and 1,125,154 images posted between August 2016 and January 2018.

During this work, the authors only collect publicly available data posted on /pol/ and Gab. They make no attempt to de-anonymize users and they follow best ethical practices as documented in (Rivers and Lewis 2014).

(For antisemitic memes) They additionally examine two mainstream Web communities, Twitter and Reddit, and compare their influence with /pol/ and Gab. For Twitter and Reddit, they use the dataset from (Zannettou et al. 2018), which includes all the posts from Reddit and Twitter, between July 2016 and July 2017, that include an image that is a meme as dictated by the KYM dataset and their processing pipeline. The final dataset consists of 581K tweets and 717K Reddit posts that include a meme. In this work, we focus on the Happy Merchant meme (Know Your Meme 2018c), which is an important hate-meme to study in this regard for several reasons. First, it represents an unambiguous instance of antisemitic hate, and second, it is extremely popular and diverse in /pol/ and Gab.",Not discussed,,manual,Yes,"To assess the extent that these terms are used in hateful/racist contexts the authors perform a small-scale manual annotation. Specifically, they collect 100 random posts from /pol/ and Gab for the words ""jew,"" ""white,"" and ""black"" and annotate them as hateful/racist or non-hateful/racist. For each of these posts, an author of the paper inspects the post and, according to the tone and terminology used, labels it as being hateful/racist or not.",Not discussed,,N/A,N/A,N/A,An author of the paper inspects small-scale sample.,Not discussed,,Not discussed,,Specific,"race, gender",Targeted,"race, political",No,Yes,"Race, ethnicity (Antisemitism)",race,Yes,Antisemitism,No,,,No,Human
342,No,"(Das et al., 2021)",Bangla hate speech detection on social media using attention-based recurrent neural network,Journal of Intelligent Systems,Bangladesh,"This article proposed encoder–decoder-based machine learning model, a popular tool in NLP, to classify user’s Bengali comments from Facebook pages. A dataset of 7,425 Bengali comments, consisting of seven distinct categories of hate speeches, was used to train and evaluate the model. For extracting and encoding local features from the comments, 1D convolutional layers were used. Finally, the attention mechanism, LSTM, and GRU-based decoders have been used for predicting hate speech categories. Among the three encoder–decoder algorithms, attention-based decoder obtained the best accuracy (77%).",Yes,21,09.04.2021,14.10.2022,Yes,Yes,"Hate speech. ""It is a platform where people are easily harassed or targeted by others, expressing hate in sexism, racism, political, or any other forms. Cyber oppression, online nuisance, and blackmailing using these social sites are also increasing rapidly.""",Yes,"categories: (a) Hate Speech, (b) Aggressive Comment, (c) Religious Hatred, (d) Ethnical Attack, (e) Religious, (f) Political Comment, and (g) Suicidal Comment.  (Among them, hateful comments are labeled as Hate speech, Aggressive comment, Religious hatred, and Ethnical attack. The non-hateful classes are Religious Comment and Suicidal comment. Political Comment is considered to be the one with both hateful and non-hateful comments of politics. )",No,,,,,Bengali comments,1,1,Facebook,Hate Speech,Bengali,N/A,N/A,N/A,N/A,"The data were extracted using Facebook API. In this study, public pages of celebrity, politician, model, actor, singer, news portal, and players were chosen because the hate spread through their hateful comments on such public pages. Initially, at least one page from these specifications was selected for the creation of data collection. The comments were collected from some most popular public pages in Bangladesh: ProthomAlo news, Bangladesh Pratidin and Ittefaq, Independent news, Mashrafe Bin Mortaza, Shakib Al Hasan, SalmoN TheBrownFish, Tasinnation, Tahsan, Model Arif Khan, Naila Nayem, and some other models. Due to privacy protection, only public comments are collected without the information of the commenters. These kinds of pages belong to celebrities on Facebook, political pages of the ruling party's official page, and Bangladesh's famous cricketers. These pages have around 0.077, 0.15, 3.5, and 6 million followers, respectively, and are found every day with an average of 2 posts with 6k reactions and 300 comments per post on social media. After creating a dataset from different Facebook pages, the raw text was processed to clean the punctuations, bad characters, stemming, etc.",Yes,"Due to privacy protection, only public comments are collected without the information of the commenters.",manual,Yes,"Here, a manual algorithm was developed to detect abusive Bangla text for experimenting. In this procedure, the work aims to discover a new way to detect hate speech. For this reason, the data were divided into seven categories: (a) Hate Speech, (b) Aggressive Comment, (c) Religious Hatred, (d) Ethnical Attack, (e) Religious, (f) Political Comment, and (g) Suicidal Comment. Among them, hateful comments are around 6,020, which are labeled as Hate speech, Aggressive comment, Religious hatred, and Ethnical attack. The hateful comments were divided into four classes, and the non-hateful classes are Religious Comment and Suicidal comment. Political Comment is considered to be the one with both hateful and non-hateful comments of politics. For a clear understanding of hateful speech, seven classes were created totally.",Not discussed,,7425,80% (~5940),20% (~1485),N/A,Not discussed,,Not discussed,,Specific,"religion, race, political",Targeted,"gender, race, political",Yes,Yes,"Religion, Ethnicity","religion, race",No,,No,,,No,Human
343,No,"(Mubarak & Darwish, 2019)",Arabic Offensive Language Classification on Twitter,Social Informatics,Qatar,"In this paper, the authors focus on building effective offensive tweet detection. They show that they can rapidly build a training set using a seed list of offensive words. Given the automatically created dataset, they trained a character n-gram based deep learning classifier that can effectively classify tweets with F1 score of 90%. They also show that they can expand their offensive word list by contrasting offensive and non-offensive tweets.",Yes,25,11.11.2019,14.10.2022,Yes,Yes,offensive language,No,,No,,,,,"the automatically created dataset;

a labeled set of 1,100 tweets (existing dataset)",2,1,Twitter,Offensiveness,Arabic,"XX.03.2014 - XX.03.2014, XX.04.2016 - XX.04.2016",N/A,N/A,N/A,"To establish a baseline, the authors used the seed list of offensive words containing 415 words to classify the test set of 1,100 tweets. Classification simply means that the authors consider a tweet containing a word or phrase in the list as offensive and non-offensive otherwise.

In the first setup (Word-List Setup), they automatically tagged a set of 175M Arabic tweets that they collected in March 2014. They extracted 3.3M tweets that had one or more offensive words from the word list, which is roughly 1.9% of all the tweets. To contrast offensive tweets with non-offensive ones, they randomly picked 33.3M tweets (10 times larger than the set of offensive tweets) from the dataset that did not match any of our offensive words. The intuition is that most tweets don't contain offensive language, and thus the overwhelming majority of random tweets are not offensive. In all, the authors used 36.6M automatically tagged tweets to train a fastText classifier using character n-grams ranging length between 3 and 6 characters.

In the other two setups (Classifier:fast Text & Classifier:SVM Setups), the authors obtained 100M tweets that they collected in April 2016. They automatically tagged these tweets using their previously trained fastText and SVM classifiers. FastText tagged 177k tweets as offensive (1.3% of the tweets) and the remaining tweets as non-offensive. On the other hand, the SVM classifier tagged 5.92M tweets as offensive (5.86% of the tweets).

They computed the valence scores of all words for all three setups.",Not discussed,,automated,Yes,"The authors used wordlist, or  to classify the tweets.",Yes,"They extracted 3.3M tweets (from 175M) that had one or more offensive words from the word list, which is roughly 1.9% of all the tweets. To contrast offensive tweets with non-offensive ones, they randomly picked 33.3M tweets (10 times larger than the set of offensive tweets) from the dataset that did not match any of our offensive words.",36.6M; 1100 as a test set,36.6M,1100,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
344,No,"(Wich et al., 2021)",Are Your Friends Also Haters? Identification of Hater Networks on Social Media: Data Paper,Companion Proceedings of the Web Conference 2021,Germany,"A promising approach to improve hate speech detection is to integrate social network data as additional features in the classification. Unfortunately, there is a lack of datasets containing text and social network data to investigate this phenomenon. Therefore, the authors of this work develop an approach to identify and collect hater networks on Twitter that uses a pre-trained classification model to focus on hateful content. The contributions of this article are (1) an approach to identify hater networks and (2) an anonymized German offensive language dataset that comprises social network data. The dataset consists of 4,647,200 labeled tweets and a social graph with 49,353 users and 122,053 edges.",Yes,4,19.04.2021,14.10.2022,Yes,Yes,"Hate speech. ""Hate speech is a broad and complex phe- nomenon and comprises various sub-types (e.g., anti-Semitism, misogyny, racism), making automatic detection difficult.""",No,,Yes,Other,upon request,Authors’ statement about the resources: please contact us via e-mail or https://in.tum.de/social/team/maximilian-wich/,Yes,"offensive language dataset in German;

existing datasets: the datasets of GermEval Shared Task on the Identification of Offensive Language 2018 and GermEval Task 2, 2019 shared task on the identification of offensive language",3,1,Twitter,Hate Speech,German,15.03.2020 - 15.08.2020,N/A,N/A,N/A,"The authors select the seed users that serve as a starting point for the network crawling phase. In total, they select 9 seed users from different sources: (1) GermEval 2019 dataset, (2) German right-wing dataset, and (3) manual exploration of Twitter.

After selecting the 0th seed users, they iteratively collect their social network. For this purpose, they use 4 different types of social relations:

• Friends network: users followed by seed user • Mutual network: intersection of friends and followers of a seed user • Retweet network (in-degree): retweeters of a seed user • Retweet network (out-degree: users retweeted by a seed user

To collect the retweet network, they extract the 500 most recent tweets of a user and analyze whom they have retweeted and who has retweeted the tweets of the user. The result is a list of usernames that have a relationship to the seed users. In the next step, the authors gather the 100 most recent tweets from their timeline to classify them with the hate classifier and calculate the users' offensiveness score.

The authors select all intersecting haters - an intersecting hater has relations to at least two seed usersand 50 other users with the highest offensiveness score. Haters with a score of 1.0 are excluded because manual exploration has shown that these are either bots or users with only a few tweets. Regarding the non-hater seeds, they define a range for the score between 0.25 and 0.5 for intersecting non-haters, aiming to choose seeds that are close to haters. A further restriction is a limit of a maximum of 1,000 followers. It aims to exclude popular profiles that interact with many non-hateful users.

These identified haters serve as seed users for the next cycle. In this paper's scope, they apply this cycle two times, meaning that they collect the 1st and and degree hater network.",Yes,"To protect users’ privacy, the authors anonymized the data and replaced all usernames with anonymous identifiers.",manual,Yes,"The sampled data are annotated by three annotators with expert knowledge in hate speech. Most of the data is annotated by two persons. The third person annotates only these tweets that received diverging annotations from the other two annotators. Since the annotators are allowed to skip a tweet and tweets containing only link(s) are ignored, some sampled tweets are not annotated and others have only one annotation instead of two or three. The interrater reliability of the annotators is measured with Krippendorff's Alpha.",Yes,"To increase the portion of offensive and hateful content in the sample, the authors apply two different sampling strategies:

• S1: They randomly sample 10 tweets from 50 haters and 50 non-haters - in total 1,000 tweets.

• S2: Firstly, they randomly select 5 1st degree haters and 5 1st degree non-haters. Secondly, they sample 10 tweets from 50 users belonging to the social networks of the is degree haters. They also apply this for the non-haters. In total, S2 comprises 1,000 tweets.","1,356",N/A,"1,356",Three annotators with expert knowledge in hate speech,Not discussed,,Not discussed,,General,N/A,Targeted,"race, religion, gender",Yes,No,,,No,,No,,,No,Human
345,No,"(Gehman et al., 2020)",RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,Findings of the Association for Computational Linguistics: EMNLP 2020,USA,"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. The authors investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. They create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, they find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. They empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, they analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. This work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",Yes,180,16.11.2020,14.10.2022,Yes,Yes,"Operationalizing Toxicity: …we rely on PERSPECTIVE API, an automated tool for toxic language and hate speech detection. In our analyses, we label a prompt as toxic if it has TOXICITY ≥ 0.5, and non-toxic otherwise.",No,,Yes,Website,,https://toxicdegeneration.allenai.org,Yes,"REALTOXICITYPROMPTS

Existing corpus: OPENWEBTEXT CORPUS (OWTC)OPENAI-WT",3,1,Reddit,Toxicity,English,N/A,N/A,N/A,N/A,"The authors select the prompts from sentences in the OPENWEBTEXT CORPUS (Gokaslan and Cohen, 2019), a large corpus of English web text scraped from outbound URLs from Reddit, for which they extract TOXICITY scores with PERSPECTIVE API. To obtain a stratified range of prompt toxicity, they sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,11), for a total of 100K sentences. They then split sentences in half, yielding a prompt and a continuation, both of which they also score for toxicity. The final dataset includes 100K naturally occurring prompts.",Not discussed,,automated,Yes,"The authors rely on PERSPECTIVE API, an automated tool for toxic language and hate speech detection. In the analyses, they label a prompt as toxic if it has TOXICITY ≥ 0.5, and non-toxic otherwise.",Yes,"To obtain a stratified range of prompt toxicity, the authors sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,11), for a total of 100K sentences.",100K prompts,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,Yes,Human
347,No,"(Omar et al., 2020)",Comparative Performance of Machine Learning and Deep Learning Algorithms for Arabic Hate Speech Detection in OSNs,Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2020),Egypt,"In this paper, the authors constructed a standard Arabic dataset that can be used for hate speech and abuse detection. In contrast to most previous work the datasets were collected from one platform, the proposed dataset is collected from more social network platforms (Facebook, Twitter, Instagram, and YouTube). To validate the effectiveness of the proposed datasets twelve machine learning algorithms and two deep learning architecture were used. Recurrent Neural Network (RNN) outperformed other classifiers with an accuracy of 98.7%.",Yes,21,24.03.2020,14.10.2022,Yes,Yes,"Hate speech. ""Hate speech includes any type of online abuse concepts like cyberbullying, discrimination, abusive language, profanity, flaming, toxicity, and harassment.""",No,,No,,,,,standard Arabic dataset for hate speech and abusive detection,1,1,"Facebook, Twitter, YouTube, Instagram",Hate Speech,Arabic,N/A,N/A,N/A,N/A,"The authors selected the most four popular online social network platforms Facebook, Twitter, YouTube, and Instagram. Arabic pages and accounts containing controversial issues and opinions, whether religious, political or sports were chosen, as were the pages of some of the controversial personalities. To collect the text data from these platforms the authors developed a web crawler that automatically scrolls down on each page then gathers all the posts, tweets, and comments and saves them in a text file. Non-Arabic characters, emojis, and URLs were later removed.",Not discussed,,manual,Yes,"The dataset was labeled by three native Arabic speakers. To facilitate their task, the authors built an online webpage to help the annotators access the dataset remotely, from any device, and at any time. All samples in the dataset were manually annotated to hate or not hate.",Not discussed,,20000,80% (~16000),20% (~4000),"The dataset was labeled by three native Arabic speakers representing varied cultures, ages, genders, and jobs",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
350,No,"(Esposito & Breeze, 2022)",Gender and politics in a digitalised world: Investigating online hostility against UK female MPs,Discourse & Society,Spain,"This paper investigates digital discursive practices of hostility against women in UK politics through quantitative and qualitative analysis of a corpus of Twitter data retrieved across the 3 weeks preceding the UK General Elections in December 2019. A mixed-methods approach was designed. First, the authors used quantitative semantic analysis to compare the large datasets of tweets about female and male MPs, with a view to detecting possible gendered patterns. They then triangulated their quantitative findings with an in-depth critical discursive analysis of the tweets mentioning female MPs. Rather than showing gendered patterns across the board, the results from the quantitative analysis brought out large inter-individual differences. Some female MPs received comments containing more lexis related to appearance, sexual history and violence, as well as more emotional or extreme language. Critical analysis of the hostile and abusive messages targeting women reveals them to be deeply embedded in a social perception of women’s political activity as breaching the rules of gender performativity.",Yes,0,15.03.2022,14.10.2022,Yes,Yes,"Semiotic violence in the cybersphere, an umbrella-term recently introduced by Krook (2020) to refer to the forms of gender-based violence which mobilise semiotic resources to injure, discipline and subjugate women.",Yes,"All identifiable words in the text are classified within a multi-tier structure with 21 major semantic fields subdivided into subcategories to obtain a more fine-grained classification. (by Wmatrix4 using USAS tagger)  The authors identified as potentially relevant the semantic fields related to Emotion and Intensity, on the one hand, and subcategories related to Intelligence, Clothes, Anatomy and Sexual relations, on the other.",No,,,,,The dataset,1,1,Twitter,Hostility,English,N/A,20.11.2019 - 11.12.2019,N/A,UK General Election (12 December 2019),"Ten politicians were selected for inclusion in the study: three women from each of the main parties, Labour and Conservative, and two men from each party for the purposes of quantitative comparison. The sample was designed to include politicians of different ethnicities from both parties. The MPs were chosen on the grounds that they were not party leaders, but were prominent during the pre-election period, with positions as ministers or as members of the opposition front bench, and received a large amount of attention in Twitter. Various Cabinet reshuffles after the election mean that several of these figures only briefly occupied prominent positions.

Tweets in which these politicians were mentioned by name were collected using searchTwitter function in the Twitter package in R, and cleaned to avoid duplication. The tweets were then saved as text files and uploaded to Wmatrix4 for lexical and semantic processing. The dataset contains all Twitter data including mentions of the frontbench politicians listed above, retrieved from 20 November to 11 December 2019, that is, immediately before the General Election of 12 December 2019. This comprised a total of 2,671,178 words/135,452 tweets.",Not discussed,,automated,Yes,"The datasets were uploaded to Wmatrix4 (Rayson, 2008). Wmatrix4 annotates all the words in the corpus using the USAS tagger, which was originally loosely based on McArthur's Longman Lexicon of Contemporary English (McArthur, 1981). This means that all identifiable words in the text are classified within a multi-tier structure with 21 major semantic fields subdivided into subcategories to obtain a more fine-grained classification. For example, field 'E', covering words related to the emotions, has 22 subdivisions indicating the type, valence (positive or negative) and intensity of the emotion in question. The advantage of using a dictionary-based semantic tagger rather than an automatic sentiment analysis tool (such as Syuzhet) or a machine learning approach is that the results are fully transparent (i.e. concordances for each tag can easily be obtained and analysed qualitatively). However, further processing, such as detection of grammatical negatives, has to be conducted manually (see also Breeze, 2020).

After numerous trials involving scrutiny of the tags associated with a number of areas of possible relevance to the research questions, the authors identified as potentially relevant the semantic fields related to Emotion and Intensity, on the one hand, and subcategories related to Intelligence, Clothes, Anatomy and Sexual relations, on the other. Within these, the authors conducted a large number of searches, focussing on subcategories of each semantic field and specific lexical items occurring there. The presence of subcategories and items was quantified, and concordances that were consensually deemed to be relevant were obtained for further qualitative analysis.",Not discussed,,"135,452 tweets",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,gender,Targeted,gender,No,Yes,Gender,gender,No,,Yes,UK female MPs,"About a person,To a person",No,Human
351,No,"(Cécillon et al., 2020)",WAC: A Corpus of Wikipedia Conversations for Online Abuse Detection,Proceedings of the Twelfth Language Resources and Evaluation Conference,France,"In this work, the authors propose an original framework, based on the the Wikipedia Comment corpus, with comment-level abuse annotations of different types. The major contribution concerns the reconstruction of conversations, by comparison to existing corpora, which focus only on isolated messages (i.e. taken out of their conversational context). This large corpus of more than 380k annotated messages opens perspectives for online abuse detection and especially for context-based approaches. The authors also propose, in addition to this corpus, a complete benchmarking platform to stimulate and fairly compare scientific works around the problem of content abuse detection, trying to avoid the recurring problem of result replication. Finally, they apply two classification methods to our dataset to demonstrate its potential.",Yes,4,11.05.2020,14.10.2022,Yes,Yes,"Abusive content. ""From this massive corpus, they sampled 3 smaller datasets that they annotated for different types of abuse:
• personal attack: abusive content directed at some- body’s person rather than providing evidence;
• aggression: malicious remark to a person or group on characteristics such as religion, nationality or gender;
• toxicity: comment that can make other people want to leave the conversation.""",Yes,"3 separate datasets annotated for Personal attack, Aggression and Toxicity",Yes,Other,figshare,https://figshare.com/articles/dataset/Wikipedia_Abusive_Conversations/11299118,Yes,"Wikipedia Abusive Conversations (WAC);

existing corpus: WCC, WikiConv, PreTox",4,1,Wikipedia,Abusiveness,English,WCC: from a dump made available in January 2016 WikiConv: from a dump made available in July 2018;,WCC: 2004-2015,N/A,N/A,"WAC is a combination of two corpora (WCC & WikiConv) and takes advantage of their complementarity. It is based on the messages and conversations structure from WikiConv (Hua et al., 2018) and the human annotations for 3 different types of abusive content from the WCC (Wulczyn et al., 2017). The textual elements constituting conversations in WAC are called ""messages"" like WikiConv as they correspond to WikiConv messages matched with a WCC annotation. WAC provides a large collection of conversations including at least one humanannotated message per conversation. It is divided into 3 datasets annotated for Personal attack, Aggression and Toxicity. In total, it contains approximately 193k conversations consisting of 4.9 million messages, among which 383k are annotated.

The reconstruction process is divided into 5 main steps. It begins with the extraction of the annotation from WCC. The second step is to retrieve messages from WikiConv. The third step consists in filtering these messages in order to keep only the relevant talk pages. The fourth step is the conversation reconstruction. The last step, the most important and difficult one, consists in uniquely identifying all the annotated messages in the conversation.",Not discussed,,manual,Yes,"The first step is to extract annotations from the WCC. This corpus provides 10 judgments per annotated comment. Each judgement provides multiple annotations depending on the dataset. The Personal attack dataset has 5 binary annotations: quoting_attack, recipient_attack, third_party_attack, other_attack and the more general attack. The Aggression and Toxicity datasets also provides such a general binary score (aggression and toxicity, respectively). Additionally, they provide an aggression_score and a toxicity_score) ranging from -2 (very abusive) to 2 (very healthy), 0 being neutral. The authors of this work aggregate these 10 judgments to determine the gold annotation of all the annotated messages. For the binary annotations, they compute the majority annotation among crowdworkers to determine the gold standard. For the scores, they compute the average value among all crowdworkers. In WAC, they call annotated messages the annotated comments extracted from WCC. Based on the 10 human judgments from WCC, the gold annotations for each of the 5 types of attack annotated is determined.",Yes,"Among the WikiConv talk pages retrieved, only a fraction contains a message that is annotated in WCC. This filtering step aims at keeping only the pages containing at least one such message, in order to retain only the relevant talk pages. However, it is important to understand that the annotated message can be in any of the conversations taking place on the concerned talk page. In order to perform such a filtering, the authors rely on the rev_id, the id of the revision from which the message was extracted. The retained pages are all the pages whose at least one message has the same rev_id as an annotated comment.","382,665 (PersonalAttack/Aggression/Toxicity:113,174/113,174/156,317)",60% (PersonalAttack/Aggression/Toxicity:~67904/~67904/~93790),20% (PersonalAttack/Aggression/Toxicity: ~22635/~22635/~31263),N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"religion, nationality, gender",Yes,No,,,No,,Yes,personal attack in a thread,To a person,No,Human
352,No,"(Vashistha & Zubiaga, 2020)",Online Multilingual Hate Speech Detection: Experimenting with Hindi and English Social Media,Information,UK,"The interest in the academic community to investigate automated means for hate speech detection has increased. In this study, the authors analyse six publicly available datasets by combining them into a single homogeneous dataset. Having classified them into three classes, abusive, hateful or neither, they create a baseline model and improve model performance scores using various optimisation techniques. After attaining a competitive performance score, they create a tool that identifies and scores a page with an effective metric in near-real-time and uses the same feedback to re-train the model. They prove the competitive performance of their multilingual model in two languages, English and Hindi. This leads to comparable or superior performance to most monolingual models.",Yes,22,22.12.2020,14.10.2022,Yes,Yes,"Hate speech is a characterisation of communication that is 'hateful', controversial, generates intolerance and in some way is divisive and demeaning. There is no legal definition of hate speech, but on several accounts accepted meaning of the term deals with communication in speech, behaviour or writing, remarks which are pejorative or discriminatory concerning a person or a group of persons, either directly or indirectly. Such remarks are based on their religion, ethnicity, nationality, descent, race, colour, gender or other identity factors.",Yes,"categories: abusive, hateful or neither",Yes,Github,,https://github.com/neerajvashistha/online-hate-speech-recog/tree/master/data/,Yes,"Three new datasets: EN, HI, HI-Code Mix;

Existing datasets: HASOC2019_EN, HASOC2019 - HI, TDavidson et al., ElSherif et al., Ousidhoum et al., SemEval 2019 Task 5, PMathur et al.",10,3,Twitter,Hate Speech,Hindi-English,N/A,N/A,N/A,N/A,"Six publicly available datasets (HASOC2019_EN, HASOC2019 - HI, TDavidson et al., ElSherif et al., Ousidhoum et al., SemEval 2019 Task 5, PMathur et al.) are manually curated and of modest size. For this study, these datasets were curated according to the guidelines provided by the author of the original datasets.

The data from all sources were collated and sorted into a homogeneous form. The class types (if necessary) are converted from original to hate, abusive or normal. To identify the source of the text, they labelled each record instance with specific dataset identifiers.

Originally, the Hindi language dataset contains only two classes, non-hate and hate, so they added more data in accordance with the above-mentioned datasets. The procedure is as follows:

1\. added and updated the profane word list provided by the author of original datasets with more words in code-mixed Hindi language.

2\. used this profane word list and added corresponding Hindi Devnagari scripted text against each code-mixed Hindi profane word.

3\. To assign classes to newly curated text from Twitter Public API, they followed the guideline below.

(a) Tweets involving sexist or racial slur to target a minority may contain abusive word and are annotated as hate.

(b) Tweets which represent undignified stereotypes are marked as hate.

(c) Tweets which contain problematic hashtags are marked as hate.

(d) Tweets which contain only abusive words are tagged as abusive.

(e) Other tweets are marked as normal.",Not discussed,,manual,Not discussed,,Not discussed,,"English-EN: 61,319 Hindi-HI: 9330 Hindi Code-Mix: 3161.",5-fold cross-validation,5-fold cross-validation,N/A,Yes,"To assign classes to newly curated text from Twitter Public API, the authors followed the guideline below.

(a) Tweets involving sexist or racial slur to target a minority may contain abusive word and are annotated as hate.

(b) Tweets which represent undignified stereotypes are marked as hate.

(c) Tweets which contain problematic hashtags are marked as hate.

(d) Tweets which contain only abusive words are tagged as abusive.

(e) Other tweets are marked as normal.",Not discussed,,General,N/A,Targeted,"religion, race, nationality, gender",Yes,No,,,No,,No,,,No,Human
353,No,"(Chai et al., 2020)",How to Keep an Online Learning Chatbot From Being Corrupted,2020 International Joint Conference on Neural Networks (IJCNN),"China, USA","Online learning can improve chatbots' conversational abilities. Although the online learning method has enhanced the diversity of chatbots' statements, it also brings opportunities for corruption. The chatbot may be corrupted to generate offensive responses such as racist and hate speech. The key component to keeping chatbots from being corrupted is offensive-response detection. Until now, the training datasets for offensive detection have focused only on individual response sentences, disregarding user input sentences. In this paper, the authors introduce a dialogue-based offensive-response dataset, which consists of 110K input-response chat records. The dataset fills the gap in response detection for chatbots. Then, they build two challenging tasks based on the dataset: an offensive-response detection task and a corrupted chatbot purification task. In addition, they propose a strong benchmark method for the tasks: an encoder-classifier model to detect input-response pairs and a one-shot reinforcement learning (RL) method to reduce rapidly the probability of generating offensive responses.",Yes,5,19.07.2020,14.10.2022,Yes,Yes,"OFFENSIVE RESPONSES

Offensive words: There are explicit profane words in the response sentence.

Offensive semantics: There are no explicit profane words in the response sentence, but the semantics of the sentence are offensive.

Inopportune response: There are no explicit offensive words or semantics in the response sentence, but it is offensive if the context of the input is considered. In other words, it will become a normal response when the input context changes.",Yes,"categories of offensive responses: offensive words, offensive semantics and inopportune responses.",Yes,Github,,https://github.com/chaiyixuan/Offensive-Responses-Dataset,Yes,Offensive Responses Dataset,1,1,SimiSimi chatbot,"Offensiveness, Offensive Response",Chinese,N/A,N/A,N/A,N/A,"SimSimi is a funny chatbot but may use low-level swear words during conversations. SimSimi Corpus (https://github.com/skdjfla/dgk_lost_conv/tree/master/results) is a Chinese dialogue dataset. It contains 500K single-turn input-response pairs. These utterances are chat histories between users and SimSimi. The authors randomly selected 110K input-response pairs from SimSimi Corpus, and then crowdsourced ten people to annotate whether the responses were offensive.",Not discussed,,manual,Yes,"The authors crowdsourced ten people to annotate whether the responses were offensive. If the response is offensive, then it is further annotated according to the following categories of offensive responses: offensive words, offensive semantics and inopportune responses. To ensure quality, they then manually filtered out the incorrectly labelled samples from the crowdsourcing results, leaving 106256 results.",Yes,The authors randomly selected 110K input-response pairs from SimSimi Corpus for annotation.,"106,256",70% (~74380),30% (~31877),Ten people crowdsourced by the authors,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,Yes,Non-human
354,No,"(Miao et al., 2020)",Detecting Troll Tweets in a Bilingual Corpus,Proceedings of the Twelfth Language Resources and Evaluation Conference,Israel,"This paper aims to detect troll tweets in both English and Russian assuming that the tweets are generated by some “troll farm.” The authors reduce this task to the authorship verification problem of determining whether a single tweet is authored by a “troll farm” account or not. They evaluate a supervised classification approach with monolingual, cross-lingual, and bilingual training scenarios, using several machine learning algorithms, including deep learning. The best results are attained by the bilingual learning, showing the area under the ROC curve (AUC) of 0.875 and 0.828, for tweet classification in English and Russian test sets, respectively. It is noteworthy that these results are obtained using only raw text features, which do not require manual feature engineering efforts. In this paper, they introduce a resource of English and Russian troll tweets containing original tweets and translation from English to Russian, Russian to English. It is available for academic purposes.",Yes,4,11.05.2020,14.10.2022,Yes,Yes,"Troll. ""During the past several years, a large amount of troll accounts has emerged with efforts to manipulate public opinion on social network sites. They are often involved in spreading misinformation, fake news, and propaganda with the intent of distracting and sowing discord.""",No,,Yes,Github,,https://github.com/Lin1202/TrollDetection,Yes,"Bilingual corpus of troll tweets: English, Russian, Russian translated from English (En2Ru), English translated from Russian(Ru2En)",4,4,Twitter,Trolling,"English, Russian",N/A,N/A,N/A,N/A,"The authors built a new dataset (Miao et al., 2019) based on the data provided by (Zannettou et al., 2018). In the original dataset, 61% are in English, 27% are in Russian. They removed tweets in other languages because the original dataset contains too few tweets in those languages, and this amount of data is insufficient for building an accurate classification model. Retweets and duplicates were filtered out to avoid over-fitting. After content filtering, they obtained 9,257 English and 4,307 Russian original tweets. In order to build a balanced dataset for binary classification, they randomly collected the same amount of normal tweets. The list of random tweets used the same hashtags as those of the troll tweets.

After addition of tweets from legitimate accounts, the dataset contains 18,514 English and 8,614 Russian original tweets.

In addition, they used Google Translate to translate the Russian corpus into English and English corpus into Russian. After these changes, the final dataset contains another 8,614 English tweets (translated from Russian tweets), and 18,514 Russian tweets (translated from English tweets).",Not discussed,,manual,Not discussed,,Not discussed,,"En: 18,514 Ru: 8,614 En2Ru: 18,514 Ru2En: 8,614;","80% (~14811, ~6891)","10% (~1851, ~861)",N/A,Not discussed,,Not discussed,,Specific,political,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
355,No,"(Chandra et al., 2021)","""A Virus Has No Religion"": Analyzing Islamophobia on Twitter During the COVID-19 Outbreak",Proceedings of the 32nd ACM Conference on Hypertext and Social Media,India,"The authors present the first large-scale quantitative study linking Islamophobia with COVID-19. In this paper, they present CoronaBias dataset which focuses on anti-Muslim hate spanning four months, with over 410,990 tweets from 244,229 unique users. They use this dataset to perform longitudinal analysis. They find the relation between the trend on Twitter with the offline events that happened over time, measure the qualitative changes in the context associated with the Muslim community, and perform macro and micro topic analysis to find prevalent topics. They also explore the nature of the content, focusing on the toxicity of the URLs shared within the tweets present in the CoronaBias dataset. Apart from the content-based analysis, they focus on user analysis, revealing that the portrayal of religion as a symbol of patriotism played a crucial role in deciding how the Muslim community was perceived during the pandemic. Through these experiments, they reveal the existence of anti-Muslim rhetoric around COVID-19 in the Indian sub-continent.",Yes,11,31.08.2021,14.10.2022,Yes,Yes,Islamophobia,No,,Yes,Github,,https://github.com/mohit3011/Analyzing-Islamophobia-on-Twitter-During-theCOVID-19-Outbreak,Yes,CoronaBias dataset,1,1,"Twitter, YouTube, OpIndia, BBC",Islamophobia,English,N/A,01.02.2020 - 31.05.2020,N/A,COVID-19,"For our study, the authors used the data collected from Twitter, Chen et al. [11] have presented a large corpus using specific keywords related to COVID-19 and have published the tweet IDs on an online repository. The authors of this study filtered these only to consider tweets written in Roman script (English) from February 1, 2020, till May 31, 2020. Since this work focuses on the Muslim community, they further filtered out tweets based on a curated set of keywords. They used positive, negative, and neutral terms so as to avoid bias in our dataset. Some examples of keywords of each type include tablighiheroes, muslimsaviours for positive keywords, muslimvirus, coronajihad as negative keywords and islam, muslim for neutral keywords. (Link to the entire keyword list can be found here https://github.com/mohit3011/Analyzing-Islamophobia-on-Twitter-During-theCOVID-19-Outbreak/blob/main/keywords-major.txt).

The filtered dataset (referred as ""CoronaBias dataset"" ) contains 410, 990 tweets with a month-wise distribution of number of tweets being: February: 105, 974, March: 70, 682, April: 161, 780, May: 72, 554. 244, 229 unique users in the dataset have tweeted at least once during the course of the study, out of which 2, 107 are verified accounts. The average word length for the tweets in the dataset is 34.

To answer RQ4 (What was the role played by external sources, especially the news media outlets? What was the nature of the content that was referenced in the tweets through URLs?), they collected data from 764 videos on YouTube, 231 unique articles on OpIndia, and 82 unique articles on BBC, which were referenced as URLs in the tweets present in the dataset.",Yes,The data from Twitter was collected from a publicly available source and the authors hydrated the tweet IDs using the official Twitter API.,"automated, manual",Yes,"They assigned each tweet one of the two labels - ‘Hateful’ or ‘Non-Hateful’.

The sheer number of tweets made it impossible to annotate the entire dataset manuallytherefore, the authors approached the problem of semi-automatic annotation of tweets in three ways - 1) Using Linguistic Inquiry and Word Count (LIWC) 2) Using Bidirectional Encoder Representations from Transformers (BERT) based method, 3) SVM based method.

Out of the three approaches, LIWC based method performed the worst. Alternatively, they used SVM and a BERT based model for this task due to its recent success in various downstream NLP tasks. To create the dataset used for training the models, they filtered out 2000 tweets from the CoronaBias dataset using a curated list of keywords that contained positive, negative, and neutral Muslim-related terms, to ensure a balanced training data. The authors used a modified version of the guidelines provided in a previous work for the annotation process. The filtered tweets were annotated on binary labels 1) Non-Hateful and 2) Hateful according to the stance of tweets concerning the Muslim community, by three annotators. The Fleiss' Kappa score for the annotation was 0.812, which translates to a near-perfect agreement among the annotators.",Yes,"To create the dataset used for training the models, they filtered out 2000 tweets from the CoronaBias dataset using a curated list of keywords that contained positive, negative, and neutral Muslim-related terms, to ensure a balanced training data.","410,990 (2000 manually labelled)",5-fold Cross-Validation,5-fold Cross-Validation,N/A,Yes,"The authors used a modified version of the guidelines provided in a previous work for the annotation process.

see [42] Bertie Vidgen and Taha Yasseri. 2020. Detecting weak and strong Islamophobic hate speech on social media. Journal of Information Technology & Politics 17, 1 (2020). 66-78.",Not discussed,,Specific,religion,Targeted,religion,Yes,Yes,Muslim (Islamophobia),religion,No,,No,,,No,Human
358,No,"(Suryawanshi et al., 2020)",Multimodal Meme Dataset (MultiOFF) for Identifying Offensive Content in Image and Text,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",Ireland,"Hate speech, offensive content and aggression content detection have been extensively explored in a single modality such as text or image. However, combining two modalities to detect offensive content is still a developing area. Memes make it even more challenging since they express humour and sarcasm in an implicit way, because of which the meme may not be offensive if we only consider the text or the image. Therefore, it is necessary to combine both modalities to identify whether a given meme is offensive or not. Since there was no publicly available dataset for multimodal offensive meme content detection, the authors leveraged the memes related to the 2016 U.S. presidential election and created the MultiOFF multimodal meme dataset for offensive content detection dataset. They subsequently developed a classifier for this task using the MultiOFF dataset. They use an early fusion technique to combine the image and text modality and compare it with a text- and an image-only baseline to investigate its effectiveness. The results show improvements in terms of Precision, Recall, and F-Score.",Yes,66,11.05.2020,14.10.2022,Yes,Yes,"Offensive meme. We define an offensive meme as a medium that spreads an idea or emotion which intends to damage the social identity of the target person, community, or lower their prestige.",No,,Yes,Other,Google Drive,"https://github.com/bharathichezhiyan/Multimodal-Meme-Classification-Identifying-Offensive-Content-in-Image-and-Text  -Use google drive link to access the data ""https://drive.google.com/drive/folders/1hKLOtpVmF45IoBmJPwojgq6XraLtHmV6?usp=sharing""",Yes,Multimodal Meme Dataset (MultiOFF),1,1,Kaggle,"Offensiveness, Offensive memes",English,N/A,N/A,N/A,2016 U.S. Presidential Election,"The initial dataset has been accessed from Kaggle. This dataset has image URLs and the text embedded in the images. The memes have been collected from social media sites, such as Reddit, Facebook, Twitter and Instagram.

The dataset from Kaggle has many images and may unrelated features such as a timestamp (date published), link (post URL), author, network, likes or upvotes. Those that did not serve the objective of the research were removed, i.e., only the URL link and text (caption) were used from the existing dataset. The captions contained a lot of unwanted symbols such as / /n or @. As this was hindering the readability of the text, all such symbols were removed from the text during the initial data pre-processing step. Furthermore, the observations in the form of long text posts were removed from the dataset and only the one with less than or equal to 20 sentences of text were kept. Each of the image URLs has been verified for its availability and the image has been obtained locally for training the classifiers for offensive content.",Not discussed,,manual,Yes,"The annotators, which used Google Forms, were given instructions to label if a given meme is offensive or non-offensive based on the image and text associated with it.

The annotation process has been done in two steps. In the first step, a set of 50 memes has been given to each of the eight annotators. As there was no ground truth defined, the majority of the vote has been considered as the gold standard and the Fleiss' kappa has been calculated for this majority vote. Initially, the maximum and minimum value of kappa lied in the interval between 0.2 and 0.3, which showed a ""fair agreement"" between the annotators. After the initial run, the authors asked the annotators for their feedback on the task. The issues that annotators faced while labelling the data were as follows:

(I) Annotators had a different interpretation of sarcastic memes. The majority of sarcastic memes had a conflict of opinion between the annotators. Example number two from Figure I is one such meme.

(II) As the annotators were unfamiliar with US politics, they were labelling the memes as offensive simply if their sentiments were hurt.

In an attempt to resolve these issues and concerns raised by the annotators, the authors updated the annotation guidelines and added V and VI in the given annotation guideline.

After improving the annotation guidelines, a set of 50 new memes were identified and distributed to each annotator. Similar to the first set of annotations, kappa was calculated, resulting in a ""moderate agreement"" between the annotators (0.4 and 0.5).

After achieving moderate agreement, the authors sent all the memes to the annotators. In this phase, each meme was annotated by only one annotator. The response provided by the annotators has been taken as the final ground truth.",Not discussed,,743,445,149,"Once pre-processing and annotation guidelines were made, only six male annotators volunteered for the task. To avoid gender bias, efforts were made to balance the gender ration of the annotation task. Finally, eight annotators (six male; two female) agreed to participate in the annotation campaign.",Yes,"The guidelines about the annotation task are as follows:

I The reviewer must review the meme  in two categories either offensive or Non-offensive.

II Memes can be deemed offensive if it intends the following: (a) Personal Attack  (b) Homophobic abuse (c) Racial abuse (d) Attack on Minority (e) Or Non-offensive otherwise

III Most of the memes come with an image and caption.

IV The reviewer must understand that images here are acting as context and play an important role in conveying the intention behind it. So indeed, images or text alone sometimes may not be meaningful.

V In case of doubt that if the meme is sarcastic, the benefit of the doubt should be given and it should be labelled as offensive.

VI While annotating the data, annotators should consider the population exposed to the content in the meme overall.",Not discussed,,Specific,"race, sexuality, other",Non-targeted,N/A,Yes,Yes,"Race, Sexuality, Minority","race, sexuality, other",No,,Yes,see guidelines:  II Memes can be deemed offensive if it intends the following: (a) Personal Attack  (b) Homophobic abuse (c) Racial abuse (d) Attack on Minority (e) Or Non-offensive otherwise,To a person,No,Human
359,No,"(Xenos et al., 2021)",Context Sensitivity Estimation in Toxicity Detection,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),"Greece, Sweden","User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs. The authors constructed and publicly release a dataset of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. They introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new dataset, they show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.",Yes,4,06.08.2021,14.10.2022,Yes,Yes,"Online fora are used to facilitate discussions, but hateful, insulting, identity-attacking, profane, or otherwise abusive posts may also occur. These posts are called toxic or abusive.",Yes,"categories: NON-TOXIC, UNSURE, TOXIC, or VERY TOXIC",Yes,Website,,Check http://nlp.cs.aueb.gr/publications.html for the link to download it.,No,"Civil Comments in Context (CCC) dataset;

Existing dataset: Civil Comments (CC) dataset",2,1,online comment forums,"Toxicity,  Context Sensitivity Estimation",English,N/A,N/A,N/A,N/A,"To build the dataset of this work, the authors used the also publicly available Civil Comments (CC dataset (Borkan et al., 2019). CC was originally annotated by ten annotators per post, but the parent post (the previous post in the thread) was not shown to the annotators. The authors randomly sampled 10,000 CC posts and gave both the target and the parent post to the annotators. They call this new dataset Civil Comments in Context (CCC). Combined with the original (out of context) annotations of the 10k posts from CC, the new dataset (CCC) contains 10k posts for which both in-context (IC) and out-of-context (OC) labels are available.",Not discussed,,manual,Yes,"Each CCC post was rated either as NON-TOXIC, UNSURE, TOXIC, or VERY TOXIC, as in the original CC dataset. The authors unified the latter two labels in both CC and CCC annotations to simplify the problem. To obtain the new in-context labels of CCC, they used the APPEN platform and five high accuracy annotators per post (annotators from zone 3, allowing adult and warned for explicit content), selected from 7 English speaking countries, namely: UK, Ireland, USA, Canada, New Zealand, South Africa, and Australia.

The free-marginal kappa (Randolph, 2010) of the CCC annotations is 83.93%, while the average (mean pairwise) percentage agreement is 92%. In only 71 posts (0.07%) an annotator said UNSURE, i.e., annotators were confident in their decisions most of the time. The authors exclude these 71 posts from the study, as they are too few. The average length of target posts in CCC is only slightly lower than that of parent posts. To obtain a single toxicity score per post, they calculated the percentage of the annotators who found the post to be insulting, profane, identity-attack, hateful, or toxic in another way (i.e., all toxicity sub-types provided by the annotators were collapsed to a single toxicity label). This is similar to arrangements in the work of Wulczyn et al. (2017), who also found that training using the empirical distribution (over annotators) of the toxic labels (a continuous score per post) leads to better toxicity detection performance, compared to using labels reflecting the majority opinion of the raters (a binary label per post).

Combined with the original (out of context) annotations of the 10k posts from CC, the new dataset (CCC) contains 10k posts for which both in-context (IC) and out-of-context (OC) labels are available.",Yes,"The authors randomly sampled 10,000 posts in CC.","~10,000",80% (~8000),10% (~1000),"The annotators are from 7 English speaking countries, namely: UK, Ireland, USA, Canada, New Zealand, South Africa, and Australia.",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
360,No,"(Safi Samghabadi et al., 2020)",Attending the Emotions to Detect Online Abusive Language,Proceedings of the Fourth Workshop on Online Abuse and Harms,USA,"In this paper, the authors present a new corpus for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. They also develop computational models to incorporate emotions into textual cues to improve aggression identification. They evaluate the proposed methods on a set of corpora related to the task and show promising results with respect to abusive language detection.",Yes,11,20.11.2020,14.10.2022,Yes,Yes,Abusive language,No,,Yes,Website,,https://ritual.uh.edu/curious-cat-corpus/  OneDrive link: https://uofh-my.sharepoint.com/:u:/g/personal/nsafisam_cougarnet_uh_edu/ER91zG7zmqJMq-tMLUxSDPEBYzjq6xwF-jpUr3osReecvw?e=AlMHXO,Yes,"Curious Cat dataset 

existing datasets: ask.fm KaggleWikipedia",4,1,"Curious Cat, ask.fm, Kaggle, Wikipedia","Abusiveness, Offensiveness",English,N/A,N/A,N/A,N/A,The authors crawled around 500K English question-answer pairs from 2K randomly chosen users of Curious Cat. They used the code available in https://github.com/NiloofarSafi/Detecting-Nastiness,Not discussed,,manual,Yes,"Four in-lab annotators annotated the data. Each row was tagged by three different annotators, and the final label assigned to each instance by majority voting. Based on the annotations, the Fleiss's kappa (Fleiss, 1971) score is 0.5 that shows a moderate agreement among the annotators.",Yes,"To avoid having bias through some specific swear words in the data, they did not use a particular list of bad words to find potentially offensive messages. Instead, they exploited the state-of-the-art classification method for abusive language detection on ask.fm (Samghabadi et al., 2017) because of two reasons: (1) The format of the data in Curious Cat and ask.fm is very similar, and (2) This method utilizes lexical features that make it capable of learning new words and phrases related to the offensive class. This model combines lexical, domain-specific, and emotion-related features and uses an SVM classifier to detect nastiness. They train that classifier on the full ask.fm dataset and apply it to Curious Cat to automatically label all rows of data. Therefore, they randomly selected 2,482 questionanswer pairs, where 60% were chosen from the negative/offensive labeled data, and 40% selected from the positive/neutral labeled data (they only considered the label of the questions).",2482 (question-answer pairs),70% (~1737),30% (~745),"Four in-lab annotators, including one graduate and three undergraduate students",Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
362,No,"(Mathew et al., 2019)",Spread of Hate Speech in Online Social Media,Proceedings of the 10th ACM Conference on Web Science,India,"In this paper, the authors perform the first study that looks into the diffusion dynamics of the posts made by hateful and non-hateful users on Gab (Gab.com). They collect a massive dataset of 341K users with 21M posts and investigate the diffusion of the posts generated by hateful and non-hateful users. They observe that the content generated by the hateful users tend to spread faster, farther and reach a much wider audience as compared to the content generated by normal users. They further analyze the hateful and non-hateful users on the basis of their account and network characteristics. An important finding is that the hateful users are far more densely connected among themselves. Overall, the study provides the first cross-sectional view of how hateful users diffuse hate content in online social media.",Yes,208,26.06.2019,14.10.2022,Yes,Yes,"Hate speech: We follow the definition used by ElSherief et al. to identify a post as hateful. The authors define hate speech as a ""direct and serious attack on any protected category of people based on their race, ethnicity, national origin, religion, sex, gender, sexual orientation, disability or disease"".",Yes,distinguish between hateful and non-hateful users,No,,,,,Gab dataset,1,1,Gab,Hate Speech,English,N/A,N/A,N/A,N/A,"In order to understand the diffusion dynamics in Gab, the authors collect a massive dataset of posts and users by following the crawling methodology described in Zannettou et al.. They use Gab's API to crawl the site using the well-known snowball strategy. They first obtain the data for the most popular user as returned by Gab's API and then collect the data for all their followers and followings. They collect different types of information as follows: 1) basic details about each user like username, score, account creation date; 2) all the posts of each user; 3) all the followers and followings for each users. They have only collected the publicly available data posted in Gab and make no attempt to de-anonymize the users.",Not discussed,,"automated, manual",Yes,"automated:

Lexicon based filtering. The authors created a lexicon10 of 45 highprecision unigrams and bigrams that are often associated with hate like 'kike (slur against Jews), 'paki' (slur against Muslims), 'beached whale' (slur against fat people). These hate words were initially selected from the Hatebase and Urban dictionary. Words such as 'banana', 'bubble' are present in Hatebase which could easily appear in benign context. In order to avoid ambiguity, they ran multiple iterations and carefully chose those keywords which were not ambiguous in Gab. They leverage these high precision keywords to identify explicit hate posts based on their textual content.

Extraction of hateful users. Using the high precision lexicon would miss out on several users who might be hateful in nature but are not selected as they did not post any content with words from our lexicon (like using images and videos). In order to capture such obscure hate users, they leverage the methodology used by Ribeiro et al.. They enumerate the steps of the methodology below.

• identify the initial set of hateful users as those who have written at least 10 posts, with at least one hateful keyword in each of them. This results in a set of 2,769 hateful users.

• create a repost network where nodes represent the users and edge-weights denote posting and reposting frequency.

• convert the repost network into a belief network by reversing the edges in the original network and normalizing the edge weights between 0 and 1.

• then run a diffusion process based on the DeGroot's learning model [19] on the belief network. They assign an initial belief value of 1 to the 2,769 users identified earlier and 0 to all the other users. The diffusion model aims to identify users who did not explicitly use any of the hateful keywords, yet have a high potential of being a hateful user due to homophily.

• They observe the belief values of all the users in the network after five iterations of the diffusion process and divide the users into four strata, [0, .25), [.25, .50), [.50, .75) and [.75, 1] according to their associated belief.

They define users whose belief values lie within [.75, 1] as hateful and those whose belief values lie within [0, .25] as non-hateful with the additional constraint that each of these users should have at least five posts. They do so since it is difficult to judge a person on the basis of a single post. They thus obtain a set of 2,290 hateful users and 58,803 non-hateful users, which comprises ~ 0.67% and ~ 17.23% of the entire dataset. They refer to the set of hateful and non-hateful users as KH (read 'Known hateful user') and NH (read 'Not hateful user') respectively henceforth.

manual:

They evaluate the quality of the final dataset of hateful and nonhateful accounts through human judgment. They ask four annotators to determine if a given account is hateful or non hateful as per their perception. Since Gab does not have any policy for hate speech, they use the guidelines defined by Twitter for this task. They provide the annotators with a class balanced random sample of 200 user accounts. Each account was evaluated by two independent annotators.",Yes,"To evaluate the quality of the labels, they provide the annotators with a class balanced random sample of 200 user account (a random sample of 200 accounts per class to keep the monetary cost manageable).","61,093 (2,290 hateful users and 58,803 non-hateful users)",N/A,N/A,The 4 annotators consisted of three undergraduate students with major in Computer Science and one PhD student in Social Computing.,Yes,"Since Gab does not have any policy for hate speech, the authors use the guidelines defined by Twitter for this (annotation) task.",Not discussed,,General,N/A,Targeted,"race, nationality, religion, gender, sexuality, disability",Yes,No,,,No,,No,,,No,Human
363,No,"(Pamungkas et al., 2020)",Do You Really Want to Hurt Me? Predicting Abusive Swearing in Social Media,Proceedings of the Twelfth Language Resources and Evaluation Conference,Italy,"Swearing plays an ubiquitous role in everyday conversations. Such occurrences can be linked to an abusive context. However, swearing is multifaceted and is often used in casual contexts, also with positive social functions. In this study, the authors explore the phenomenon of swearing in Twitter conversations, taking the possibility of predicting the abusiveness of a swear word in a tweet context as the main investigation perspective. They developed the Twitter English corpus SWAD (Swear Words Abusiveness Dataset), where abusive swearing is manually annotated at the word level. This collection consists of 1,511 unique swear words from 1,320 tweets. They developed models to automatically predict abusive swearing, to provide an intrinsic evaluation of SWAD and confirm the robustness of the resource. They also present the results of a glass box ablation study in order to investigate which lexical, syntactic, and affective features are more informative towards the automatic prediction of the function of swearing.",Yes,30,11.05.2020,14.10.2022,Yes,Yes,"Swearing is the use of taboo language (also referred to as bad language, swear words, offensive language, curse words, or vulgar words) to express the speaker's emotional state to their listeners.

Swearing in social media can be linked to an abusive context, when it is intended to offend, intimidate or cause emotional or psychological harm, contributing to the expression of hatred, in its various forms.

abusive swearing…those uses where swearing contributes to the construction of an abusive context such as name-calling, harassment, hate speech, and bullying, involving several sensitive topics including physical appearance, sexuality, race and culture, and intelligence, with intention from the author of tweet to insult or abuse a target (person or group of persons).",No,,Yes,Github,,https://github.com/dadangewp/SWAD-Repository,Yes,SWAD (Swear Words Abusiveness Dataset),1,1,Twitter,"Abusive Swearing, Abusiveness",English,N/A,N/A,N/A,N/A,The starting point was a corpus of tweets selected from the training set of Offensive Language Identification Dataset (OLID).,Not discussed,,manual,Yes,"All instances were annotated by two independent annotators (A1 and A2). The resulting disagreement was resolved by involving the third annotator (A3), labeling those instances where a disagreement between A1 and A2 was detected. Annotators were asked to annotate (with a binary option) whether the highlighted swear word (tagged with the &ltb and &ltb tags) can be considered abusive swearing, contributing to the construction of an abusive context (by using the tag ""yes"") or whether the swear word does not contribute to the construction of an abusive context (by using the tag ""no\*).

They first started a trial annotation on a portion of 100 tweets from the collection, to test their annotation guidelines and improve the understanding between annotators. During this trial annotation they also deepened our understanding of the offensiveness notion, which underlies the definition of offensive language driving the whole OLID annotation process. There is a crucial difference between the coarse notion of offensive language as defined in OLID and the concept of abusive language this study is interested in. According to the OLID definition a tweet can be considered offensive only because of the presence of profanities, even if no occurrence of abusive swearing can be detected. Such considerations has driven the authors’ decision to annotate the abusiveness of swear words on tweets belonging to both classes (offensive and not-offensive) of the OLID data. Another issue discovered during the trial annotation consisted in some cases where the swear word is used for indirect insult: the swear word itself is used to insult, but the overall context of the tweet is not abusive.

Therefore, in the final annotation guidelines, they decided to include the author intention to resolve the swear word context, especially to deal with this kind of swear word use. They consider abusive swearing those uses where swearing contributes to the construction of an abusive context such as name-calling, harassment, hate speech, and bullying, involving several sensitive topics including physical appearance, sexuality, race and culture, and intelligence, with intention from the author of tweet to insult or abuse a target (person or group of persons). One tweet can have more than one swear word, but for every tweet, only one swear word will be highlighted as relevant for the annotation in each row (see the replication process explained above). Therefore, the annotator only needs to focus on the marked swear words (e.g., &ltb ;fuck&lt/b ;). They remark again that abusive swearing can be found on both offensive and not-offensive tweets, therefore during the application of our annotation layer, they decided to ignore the original message-level layer of annotation from the original OLID (offensive vs not-offensive), in order to avoid confusing the annotators during the annotation process.",Yes,"Some preprocessing has been applied to the OLID data, such as mention and URL normalization. Since the focus is on analyzing swear words in the tweet context, they first filtered out a subset of tweets from OLID based on the presence of swear words, in order to obtain a collection of tweets that include at least one swear word. At this stage they exploited the list of swear words published on the noswearing website, an online dictionary site which includes a list of swear words. This dictionary includes 349 swear words covering general vulgarities, slurs, and sex-related terms. They manually checked the list to exclude highly ambiguous words, namely swear word ""ho"" and ""hard on”. They identified 1,320 tweets that contain at least one swear word.

Since this annotation task is at the (swear) word level, tweets which have more than one swear word were replicated. They generated as many new instances of the same tweet as the number of swear words occurring in the message, and marked each single swear word with special tags &lt6 and &lt/6 (e.g. &ltb ;fuck&lt/b ;, &ltb ;shit&lt/b ;, and etc.) so that the abusiveness label on each instance records the context of the marked swear word in the tweet (abusive or not). For instance, given the message @USER This shit gon keep me in the crib lol fuck it, two instances will be generated: @USER This &ltb ;shit&lt/b gon keep me in the crib lol fuck it and @USER This shit gon keep me in the crib lol &ltb ;fuck&lt/b it.

They found 154 tweets having more than one swear word, with a range of occurrences from 2 to 6 swear words. As a result, they have 1,511 instances to be annotated.",1511,90% (~1360),10% (~151),"The annotation involved three expert annotators (the authors), with different gender and ages.  All annotators use English as a second language, with a minimum level of B2.",Yes,"(After the trial annotation, ) In the final annotation guidelines, the authors decided to include the author intention to resolve the swear word context, especially to deal with this kind of swear word use. They consider abusive swearing those uses where swearing contributes to the construction of an abusive context such as name-calling, harassment, hate speech, and bullying, involving several sensitive topics including physical appearance, sexuality, race and culture, and intelligence, with intention from the author of tweet to insult or abuse a target (person or group of persons). One tweet can have more than one swear word, but for every tweet, only one swear word will be highlighted as relevant for the annotation in each row (see the replication process explained above). Therefore, the annotator only needs to focus on the marked swear words (e.g., &ltb ;fuck&lt/b ;). They remark again that abusive swearing can be found on both offensive and not-offensive tweets, therefore during the application of our annotation layer, they decided to ignore the original message-level layer of annotation from the original OLID (offensive vs not-offensive), in order to avoid confusing the annotators during the annotation process.

The annotation guideline is available along with the dataset at: https://github.com/dadangewp/SWAD-Repository",Not discussed,,General,N/A,Targeted,"body, sexuality, race, other",Yes,No,,,No,,No,,,No,Human
364,No,"(Sazzed, 2021)",Abusive content detection in transliterated Bengali-English social media corpus,Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,USA,"Abusive text detection in low-resource languages such as Bengali is a challenging task due to the inadequacy of resources and tools. The ubiquity of transliterated Bengali comments in social media makes the task even more involved as monolingual approaches cannot capture them. Unfortunately, no transliterated Bengali corpus is publicly available yet for abusive content analysis. Therefore, in this paper, the authors introduce an annotated Bengali corpus of 3000 transliterated Bengali comments categorized into two classes, abusive and non-abusive, 1500 comments for each. For baseline evaluations, they employ several supervised machine learning (ML) and deep learning-based classifiers. They find support vector machine (SVM) shows the highest efficacy for identifying abusive content.",Yes,8,11.06.2021,14.10.2022,Yes,Yes,"Abusive language refers to the usage of demeaning, insulting, vulgar, or profane expression to attack individuals or groups.",No,,Yes,Github,,https://github.com/sazzadcsedu/AbusiveCorpus,Yes,annotated transliterated Bengali corpus for abusive content analysis,1,1,YouTube,Abusiveness,Bengali,N/A,N/A,N/A,N/A,"Using a web scraping tool, the authors first download the raw JSON data from YouTube that contains information such as user name, id, timestamp, comments, and like/dislike, etc. Utilizing a parsing script, they extract the viewer's comments from the JSON data.

The comments are written in Bengali, English, transliterated Bengali, or using code-switching words. Since the goal is to create a corpus for transliterated Bengali (i.e., Bengali words in Latin alphabet), they exclude comments written using the Bengali alphabet (i.e., Bengali comments). They utilize a language detection tool to distinguish comments written using the Latin alphabet and Bengali alphabet. However, the tool can not differentiate between English and transliterated Bengali words as both use the Latin alphabet. Since social media contains lots of non-dictionary and misspelled English words (especially when written by non-native speakers), checking the English dictionary is not a feasible option to distinguish English and Transliterated Bengali words. Therefore, they manually inspect all the comments to include them in the corpus. They discard comments which are written using only English words. Comments with both transliterated Bengali and English words are included in the corpus if they contain at least two transliterated Bengali words. Note that, unlike Bengali or English words, there is not fixed spelling for transliterated Bengali wordsthus, the same transliterated word with different spellings can be present in the corpus.",Not discussed,,manual,Yes,"Two native Bengali speakers assign the class of the transliterated comments into two categories, abusive and non-abusive. The authors observe a Cohen’s kappa score of 0.733 between two annotators, which refers to substantial agreement.

The final corpus includes the 3000 comments which are assigned to the same class by both annotators, 1500 from each category. To avoid ambiguity, the authors exclude the comments in which annotators disagree on class assignment.",Not discussed,,3000 comments,10-fold cross-validation,10-fold cross-validation,Two native Bengali speakers,Yes,"For assigning the transliterated Bengali comments into abusive or non-abusive categories, the authors follow a similar guideline of Nobata et al. (2016). Annotators labeled a piece of text as abusive if it contains either hate speech or derogatory language or profanity.

Based on that, they assign the class of the comments into two categories-

• Abusive: This class includes hate speech which attacks or demeans a group based on race, ethnic origin, religion, disability, gender, age, disability, etc. Besides, it consists of derogatory or demeaning remarks which attack an individual or a group and profanity towards individuals using sexually offensive and pornographic comments.

• Non-abusive: comments which do not fall into the abusive category. These comments could convey a positive, (non-abusive) negative or neutral opinion or could be objective in nature.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
365,No,"(Alcântara et al., 2020)",Offensive Video Detection: Dataset and Baseline Results,Proceedings of the Twelfth Language Resources and Evaluation Conference,Brazil,"Among the published datasets for offensive language detection, the Portuguese language is underrepresented, and videos are little explored. The authors investigated the problem of offensive video detection by assembling and publishing a dataset of videos in Portuguese containing mostly textual features. They ran experiments using popular machine learning classifiers used in this domain and reported the findings, alongside multiple evaluation metrics. They found that using word embedding with Deep Learning classifiers achieved the best results on average. CNN architectures, Naive Bayes, and Random Forest ranked top among different experiments. Transfer Learning models outperformed Classic algorithms when processing video transcriptions, but scored lower using other feature sets. These findings can be used as a baseline for future works on this subject.",Yes,9,11.05.2020,14.10.2022,Yes,Yes,"We define as offensive, videos that express racism, sexism, homophobia, xenophobia, religious intolerance, or profane language.",No,,Yes,Website,,https://clebersa.github.io/offensive-video-detection/datasets/,Yes,"OffVidPT-2, OffVidPT-3",2,2,YouTube,Offensiveness,Portuguese,N/A,N/A,N/A,N/A,"To retrieve video data from YouTube, the authors used its official API. To search for potentially offensive videos, they used a list of dirty words provided by Pelle et al.. Each seed word was searched individually, retrieving a set of video ids. They merged the video ids to avoid duplicates that might have appeared in more than one search and discarded the channel and user ids. This process resulted in a total of 101,759 video ids. Then, they retrieved detailed and structured data for each video through the same API.

They filtered the videos looking for the ones with default audio language attribute explicitly set to Portuguese, as one of the objectives is to build and provide a dataset of videos in this language. At the end of this filtering step, they ended up with a set of 5,180 videos.

To obtain the transcriptions, they used the Google Speech-to-Text service, which makes use of machine learning to transcribe audio files automatically. This service has an option to filter profanity words and phrases, which they disabled to keep the transcription more loyal to the original audio and provide more realistic results during our experiments. They also used an unofficial YouTube API to retrieve the subtitles for the videos. However, they found that most of the downloaded videos lack captions or have subtitles only in a language different from the one in the audio track. Due to this reason, they did not use the subtitles in the experiments.",Not discussed,,manual,Yes,"The authors developed a crowdsourcing web tool to enable volunteers to annotate the videos. Each video was annotated by three judges. Annotators were asked to watch the full video and tag offensive moments during the video. If nothing offensive was found, the annotator should explicitly specify the video was not offensive to carry on with the annotation process. Instructions were presented to the annotators alongside with the definition of what should be considered offensive in the videos and their definition according to the Online Oxford Dictionary.

By the end of the process, they had 400 videos annotated. For classification purposes, they considered a video as offensive if it had at least one moment tagged as offensive by at least two annotators. They chose this minimum agreement among the annotators to reduce the bias in the annotations, as they were crowdsourced, and the volunteers were not experts in the domain. Based on their agreement, the authors created two datasets:

• OffVidPI-2, in which at least two of the three annotators of each video agreed on the positive class (offensive)and

• OffVidPI-3, in which all three annotators agreed on the positive class.",Yes,"The authors chose to annotate a random sample of the videos which satisfied the following conditions: (i) do not include unclear speech, or no speech at all (just noise or sounds, without any spoken words), and (ii) be in Portuguese. The goal of (i) was to enable annotators to watch videos with better audio quality, and also yield higher quality transcriptions. The language filter (ii) was also necessary because the author of the video could have set the default audio language attribute incorrectly, or the video could have mixed languages, affecting the transcription quality to Portuguese. They also filtered out the videos with a duration longer than five minutes from the sample, which corresponded to approximately 43.3% of all Portuguese videos. The goal was to keep annotators engaged and save time in the annotation process, which is the bottleneck in dataset creation.",400 videos,10-fold cross-validation,10-fold cross-validation,Crowdsourced volunteers who were not experts in the domain.,Yes,"Instructions were presented to the annotators alongside with the definition of what should be considered offensive in the videos and their definition according to the Online Oxford Dictionary: racism, as ""Prejudice, discrimination, or antagonism directed against a person or people on the basis of their membership of a particular racial or ethnic group, typically one that is a minority or marginalized.""; sexism, as ""Prejudice, stereotyping, or discrimination, typically against women, on the basis of sex.""; homophobia, as ""Dislike of or prejudice against homosexual people.""; xenophobia, as ""Dislike of or prejudice against people from other countries.""; religious intolerance, as ""Unwillingness to accept views, beliefs, or behavior that differ from one's religion.""; and profane language, as ""Blasphemous or obscene language."". These definitions were presented to guide the annotators in the process and prevent them from letting their personal beliefs or emotions affect their judgment, as they were not experts in the domain. These guidelines were presented to each annotator right before they started to evaluate the videos and were available at any time in the annotation page.",Not discussed,,General,N/A,Targeted,"race, gender, sexuality, religion",Yes,No,,,No,,No,,,No,Human
368,No,"(Hada et al., 2021)",Ruddit: Norms of Offensiveness for English Reddit Comments,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Netherlands, UK, Canada","Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. The authors create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best–Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. They show that the method produces highly reliable offensiveness scores. Finally, they evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset.",Yes,9,01.08.2021,14.10.2022,Yes,Yes,"In our instructions to the annotators, we defined offensive language as comments that include but are not limited to [being hurtful (with or without the usage of abusive words)/ being intentionally harmful/ treating someone improperly/ harming the 'self-concept of another person/ aggressive outbursts/ name calling/ showing anger and hostility/ bullying/ hurtful sarcasm]. We also encouraged the annotators to follow their instincts.",Yes,"degree of offensiveness by score, using Best–Worst Scaling",Yes,Github,,https://github.com/hadarishav/Ruddit,Yes,Ruddit dataset,1,1,Reddit,Offensiveness,English,XX.09.2019,XX.01.2015 - XX.09.2019,N/A,N/A,"The authors extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery. Reddit is a social news aggregation, web content rating, and discussion website. It contains forums called subreddits dedicated to specific topics. The entire discussion has a hierarchical structure called the comment thread. The authors divided the extracted comments into 3 categories based on their subreddit source:

1\. Topics (50%): Contains comments from topic-focused subreddits: AskMen, AskReddit, TwoXChromosomes, vaxxhappened, worldnews, worldpolitics. These subreddits were chosen to cover a diverse range of topics. AskReddit, vaxxhappened, worldnews, worldpolitics discuss generic themes. TwoXChromosomes contains women's perspectives on various topics and AskMen contains men's perspectives.

2\. ChangeMyView (CMV) (25%): The CMV subreddit (with over a million users) has posts and comments on controversial topics.

3\. Random (25 %): Contains comments from random subreddits.",Yes,Acknowledgements: The creators of the Pushshift Reddit dataset have provisions to delete comments from their dataset upon user's request. The authors release data in a manner that is GDPR compliant. They do not provide any user-specific information. They release only the comment IDs and post IDs. Reddit's Terms of Service do not prohibit the distribution of ids. The researchers using the dataset need to retrieve the data using the Reddit API.,manual,Yes,"The authors carried out all the annotation tasks on Amazon Mechanical Turk (AMT). They followed the procedure described in Kiritchenko and Mohammad (2016) to obtain BWS annotations. Annotators were presented with 4 comments (4-tuple) at a time and asked to select the comment that is most offensive (least supportive) and the comment that is least offensive (most supportive). They randomly generated 2N distinct 4-tuples (where N is the number of comments in the dataset), such that each comment was seen in eight different 4-tuples and no two 4-tuples had more than 2 items in common. They used the script provided by Kiritchenko and Mohammad (2016) to obtain the 4-tuples to be annotated.

Kiritchenko and Mohammad (2016) show that in a word-level sentiment task, using just three annotations per 4-tuple produces highly reliable results. However, since the authors work with long comments and a relatively more difficult task, they got each tuple annotated by 6 annotators. Since each comment is seen in 8 different 4-tuples, they obtain 8 X 6 = 48 judgements per comment.

For quality control purposes, they manually annotated around 5% of the data themselves beforehand (gold questions). The gold questions were interspersed with the other questions. If a worker's accuracy on the gold questions fell below 70%, they were refused further annotation and all of their annotations were discarded. The discarded annotations were published again for re-annotation. The authors received a total of 95,255 annotations by 725 crowd workers. The BWS responses were converted to scores using a simple counting procedure: For each item, the score is the proportion of times the item is chosen as the most offensive minus the proportion of times the item is chosen as the least offensive. They release the aggregated annotations as well as the individual annotations of Ruddit.",Yes,"The authors selected 808 posts from the subreddits based on criteria such as date, thread length, and post length. They took the first 25 and the last 25 comments per post (skipping comments that had [DELETED] or [REMOVED] as comment body). The first responses are likely to be most relevant to the post. The final comments indicate how the discussion ended. They sampled 6000 comments from this set for annotation.

The goal of the sampling was to increase the proportion of offensive and emotional comments. Studies have shown that the primary dimensions of emotion are valence, arousal, and dominance (VAD). Valence is the positive - negative or pleasuredispleasure dimension. Arousal is the excited-calm or active-passive dimension. Dominance is powerful-weak or 'have full control'-'have no control' dimension. To boost the representation of offensive and emotional comments in the dataset, the authors up-sampled comments that included low-valence (highly negative) words and those that included high-arousal words (as per the NRC VAD lexicon (Mohammad, 2018)). The manually constructed NRC VAD lexicon includes 20,000 English words, each with a real-valued score between 0 and 1 in the V, A, D dimensions. In order to do this up-sampling, they first defined the valence score of each comment as the average valence score of the negative words within the comment (A negative word is defined as a word with a valence score &lt0.25 in the VAD lexicon.) Similarly, they defined the arousal score for a comment as the average arousal score of high-arousal words in each comment. (A high-arousal word is defined as a word with an arousal score ≥ 0.75.) They selected comments from the comment pool such that 50% were from the Topics category, 25% from the CMV category, and 25% from the Random category. Within each category, 33% of the comments were those that had the lowest valence scores, 33% of the comments were those that had the highest arousal scores, and the remaining were chosen at random.",6000 comments,5-fold cross-validation,5-fold cross-validation,"The authors restricted annotators to those residing in the US. A total of 725 crowd-workers participated in the task. Apart from the country of residence, no other information is known about the annotators.",Yes,"The authors created our annotation guidelines drawing inspiration from the community standards set for offensive language on several social media platforms. These standards are made after thorough research and feedback from the community.

Detailed instructions for the final annotation task and Sample questionnaire for the final annotation task are available in the Appendix.",Yes,The hourly compensation rate for annotators on Amazon Mechanical Turk was US$7.50/hr. The task received considerable attention with 725 participants in total.,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
371,No,"(Manerba & Tonelli, 2021)",Fine-Grained Fairness Analysis of Abusive Language Detection Systems with CheckList,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),Italy,"In this paper, the authors create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. They compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. The evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. The authors release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.",Yes,3,06.08.2021,14.10.2022,Yes,Yes,"In this work, we adopt a definition for fairness that is strongly contextual to abusive language detection. We define unfairness as the sensitivity of an abusive language detection classifier with respect to the presence in the record to be classified of entities belonging to protected groups or minorities. Specifically, a classifier is considered unfair or biased if the prediction changes according to the identities present, i.e. in similar sentences, the degree of hate is increased if terms such as white or straight are replaced by adjectives such as black or non-binary, revealing imbalances, possibly resulting from skewed and unrepresentative training data. Fairness, on the other hand, is defined as the behaviour of producing similar predictions for similar protected mentions, i.e. regardless of the specific value assumed by sensitive attributes like race and gender, without disadvantaging minorities or amplifying pre-existing social prejudices.",Yes,"Fairness tests:  Misogyny, gender and sexual orientation: • Perturbing gender and sexual orientation (INV) • Stereotyped female vs male work roles and Stereotyped male vs. female work roles (INV) • Unintended bias in misogyny detection (MFT) • Gender stereotypes (MFT) • Gender stereotypes (MFT) • Body image stereotypes (MFT) • Toxic masculinity stereotypes (MFT) • Neutral statements feminism-related (MFT).  Race, nationality and religion: • Perturbing race (INV) • Perturbing nationality (INV) • Perturbing religion (INV) • Racial stereotypes (MFT).  Disability: • Ableist stereotypes (MFT)",Yes,Github,,https://github.com/MartaMarchiori/Fairness-Analysis-with-CheckList,Yes,"Evaluation_Datasets (sexism/racism/ableism/all)’;

existing datasets:  (Founta et al., 2018) corpus, AMI 2018 dataset",4,4,CheckList,Abusiveness,English,N/A,N/A,N/A,N/A,"Specifically, the authors export the test records together with their corresponding labels, when applicable. In fact, only the MFT test type features a precise label, whereas the other two types (INV and DIR) involve an expectation of whether or not the probabilities will change and therefore cannot be conceptually formalised in a dataset, where labels are required.

The exported data results in the creation of three synthetic datasets covering different types of bias grouped by target, namely sexism, racism and ableism. The reason for distinguishing the records by abuse targets is due to the need for specialised datasets addressing different phenomena of abusive language with a fine-grained approach. The resulting data do not contain samples from datasets under license.",Not discussed,,manual,Not discussed,,Not discussed,,sexism dataset: 5623 racism dataset: 1900 ableism dataset: 220;,"(Founta et al., 2018) corpus: ~100k  AMI 2018 dataset: ~4000",500 randomly sampled records for each test,N/A,Not discussed,,Not discussed,,Specific,"gender, race, sexuality, body, nationality, religion",Targeted,"race, gender, sexuality",Yes,Yes,"Gender, Race, Ableness, sexuality, body image, nationality, religion","gender, race, sexuality, body, nationality, religion",No,,No,,,No,Human
373,No,"(Thee et al., 2020)",Pro-ISIS Tweets Analysis Using Machine Learning Techniques,2020 IEEE International Conference on Big Data (Big Data),USA,"In this research, the authors apply machine learning algorithms to understand popular ISIS supporters' behavior and techniques and their possible influence on other users. They collected and analyzed a dataset containing over seventeen thousand tweets posted by pro-ISIS Twitterers. They utilized three machine learning algorithms with several models/settings in an attempt to classify and predict whether the top 4 pro-ISIS Twitter users (the most followed and tweeted users) authored a specific tweet. The algorithms applied in this work include sequential neural networks, random forests, and XGBoost. The models were ensembled, timed, and one model was simplified to attempt to improve performance and runtime.",Yes,0,10.12.2020,14.10.2022,Yes,Yes,"violent extremism, pro-ISIS;

pro-ISIS ""fanboys"" (Twitter users that post pro-ISIS tweets, such as celebrating ISIS victories or endorsing ISIS in their profiles).",No,,No,,,,,ISIS Influential Fanboys Dataset,1,1,Twitter,"Violent Extremism, Pro-ISIS",English,N/A,N/A,N/A,N/A,"The dataset collected contains 17,410 tweets, which are written by over 100 pro-ISIS ""fanboys"" (Twitter users that post pro-ISIS tweets, such as celebrating ISIS victories or endorsing ISIS in their profiles). The number of followers and number of tweets posted by each user were combined to determine which fanboys were the most influential. The top four most influential pro-ISIS fanboys were selected for authorship classification.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,religion,Targeted,religion,No,Yes,Pro-ISIS fanboys,religion,No,,No,,,No,Human
376,No,"(Gomez et al., 2022)",Curating Cyberbullying Datasets: a Human‑AI Collaborative Approach,International Journal of Bullying Prevention,USA,"The authors propose two machine learning approaches to identify and filter unambiguous comments in a cyberbullying dataset of roughly 19,000 comments collected from YouTube that was initially annotated using Amazon Mechanical Turk (AMT). Using consensus filtering methods, comments were classified as unambiguous when an agreement occurred between the AMT workers’ majority label and the unanimous algorithmic filtering label. Comments identified as unambiguous were extracted and used to curate new datasets. They then used an artificial neural network to test for performance on these datasets. Compared to the original dataset, the classifier exhibits a large improvement in performance on modified versions of the dataset and can yield insight into the type of data that is consistently classified as bullying or non-bullying. ",Yes,0,22.12.2021,30.09.2022,Yes,Yes,"""A common approach when defining cyberbullying is to combine characteristics of traditional bullying (intention, repetition, power imbalance) with devices used in cyberspace (computers, cell phones, etc.) (Englander et al., 2017). Hinduja and Patchin define cyberbullying as “willful and repeated harm inflicted through the use of computers, cell phones, and other electronic devices” (Hinduja & Patchin, 2015 p. 5, Hinduja & Patchin 2019b).""",No,,No,,,,,N/A,1,1,YouTube,Cyberbullying,English,October 2019 to January 2020,Implicitly before January 2020,N/A,N/A,"YouTube API, searching comments of controversial videos",Not discussed,,manual,Not discussed,,,,19000,15200,3800,MTurk workers,Yes,"They gave a definition as a guideline: ""Is the text bullying? Bullying can be described as content that is harmful, negative or humiliating. Furthermore, the person reading the text could be between the ages of 12-19 and/or may have a mental health condition such as anxiety, depression, etc.""",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
377,No,"(Pantti et al., 2019)",The meanings of racism: Public discourses about racism in Finnish news media and online discussion forums,European Journal of Communication,Finland,"This study investigated how the concept of racism is used in Finnish public debate by employing a computational text analysis technique to derive topics related to racism from a large corpus of news media content and online discussion forum comments. The findings show that discourses about racism are different in legacy media and online platform regarding both their prominence and framings. While social media produce various discourses of ‘reverse racism’, news media connects racism to historical and international contexts. The authors conclude that what racism is understood to be is not only an intensely political process but also one shaped by the type of media platform, specificities of Finnish language and national ideological battles.",Yes,15,17.09.2019,30.09.2022,Yes,Yes,"""This study was particularly concerned with the parliamentary success of the nationalist Finns Party in the 2011 and 2015 elections, together with the increased popularity of social media, which have ensured that racism and hate speech have remained in the spotlight in public debates (Nikunen, 2016).""",No,,No,,,,,news media outlets / Suomi24,2,2,"Suomi24, Finnish news websites",Racism,Finnish,Implicitly between 2015 and 2019,2011 to 2015,Finland,N/A,They searched for 'racism/racist' as keywords,Not discussed,,N/A,Not discussed,,No,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,other,Targeted,"race, political",No,Yes,Minorities,other,No,,No,,,No,Human
379,No,"(Ali et al., 2022)",Hate speech detection on Twitter using transfer learning ,Computer Speech and Language,Pakistan,"In this research work, the authors develop an Urdu language hate lexicon, on the basis of this lexicon they formulate annotated dataset of 10,526 Urdu tweets. Furthermore, as baseline experiments, they use various machine learning techniques for hate speech detection. In addition, they use transfer learning to exploit pre-trained FastText Urdu word embeddings and multi-lingual BERT embeddings for our task. Finally, they experiment with four different variants of BERT to exploit transfer learning, and they show that BERT, xlm-roberta and distil-Bert are able to achieve encouraging F1-scores of 0.68, 0.68 and 0.69 respectively, on our multi class classification task. All these models exhibited success to varying degree but outperform a number of deep learning and machine learning baseline models.",Yes,2,XX.07.2022,30.09.2022,Yes,Yes,"""Scholars have conflicting views as to what constitutes hate speech but in essence, there is a consensus that it is a speech that targets marginalized and disadvantaged social segments in a manner that is harmful to them (Davidson et al., 2021). Moreover, researchers have also found that there is a strong connection between hate speech and violent crimes.""",Yes,"offensiveness, hate speech",No,,,,,N/A,1,1,Twitter,Hate Speech,Urdu,Implicitly before July 2022,Implicitly before July 2022,N/A,N/A,Twint tool. They searched for commonly used keywords regarding offensive and hate speech.,Not discussed,,manual,No,,Yes,They filtered out all non-Urdu tweets,10526,7368,2105,"Six annotators. Three male, three female",No,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
380,No,"(Pratiwi et al., 2019)",Hate Speech Identification using the Hate Codes for Indonesian Tweets ,International Conference on Data Science and Information Technology,Indonesia,"This study aims to utilize the hate codes to identify the hate speech on the social media data. The authors used the Indonesian tweets as the dataset and utilized Logistic Regression, Support Vector Machine, Naïve Bayes, and Random Forest Decision Tree as the classifiers. The highest F-Measure score for the hate speech identification was 80.71% by using the hate code feature combined with Logistic Regression as the classifier.",Yes,9,XX.07.2019,04.10.2022,Yes,Yes,"""Based on the report released by the Amnesty International Indonesia in early 2018, political practices using hate speech increased throughout 2017. The announcement of the presidential and vice-presidential candidates on August 9, 2018 indicated that the presidential election period had been started that day until the election day in April 2019. The closer this huge political agenda to be held, the more the cases related to hate speech to occur. This is because the utterance of hate is used as a way to attack and bring down one’s political opponents.""",No,,No,,,,,N/A,1,1,Twitter,Hate Speech,Indonesian,09.08.2018 - 05.11.2018,"Implicitly before November 5th, 2018",N/A,Indonesian presidential election,They searched of list of 10 trending topics related to the election,Not discussed,,manual,No,,Yes,They filtered out all non-Indonesian tweets,15700,N/A,N/A,N/A,No,,Not discussed,,General,N/A,Targeted,political,Yes,No,,,No,,No,,,No,Human
382,No,"(Malik et al., 2021)",Toxic Speech Detection using Traditional Machine Learning Models and BERT and fastText Embedding with Deep Neural Networks ,International Conference on Computing Methodologies and Communication,India,"In this paper the authors have combined two new datasets (ALONE and HASOC’20) to perform their analysis. Alone is a dataset entirely based on youths’ toxic conversations on Twitter. They have performed data pre-processing followed by various machine learning and ensemble algorithms using TF-IDF, POS tagging and trigrams approach in which LR and XGBoost performed the best for us. In the second scenario they used word embedding techniques (fastText and BERT) and then fed in the embeddings as inputs to DNN classifiers. They used various DNN classifiers and observed that a combination of BERT embedding with CNN gave the best results. The ability of CNN to adapt itself to understand and efficiently identify appropriate patterns in case of small sequences of words and noise in dataset, explains better performance of CNN over LSTM.",Yes,5,06.05.2021,04.10.2022,Yes,Yes,"""Hate speech has no international definition legally, but is a deliberate act aimed at discrimination, violence and hostility. Offensive speech is the derogatory text that uses abusive slurs or curse words. Profanity is the use of obscene or in-appropriate words meant to demean an individual or community. The three terms are often confused with each other, as clearly indicated by the three examples in Table I. Hence it was decided to combine them to form a major group of toxic content.""",No,,No,,,,,N/A,2,1,"Twitter, Facebook",Toxicity,English,"Implicitly before May, 2021","Implicitly before May, 2021",N/A,N/A,They used the datasets ALONE and HASOC,Not discussed,,manual,No,,,,6540,N/A,N/A,N/A,Yes,"They annotated the tweets accordingly to the existing annotations. Such as ""profane"", ""offensive"", or ""hate"".",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
383,No,"(Vidgen et al., 2020)",Detecting East Asian Prejudice on Social Media,arXiv preprint,"UK, USA","The authors report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. They then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. The authors also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.",Yes,61,08.05.2020,04.10.2022,Yes,Yes,"""The outbreak of COVID-19 has raised concerns about the spread of Sinophobia and other forms of East Asian prejudice across the world, with reports of online and oﬄine abuse directed against East Asian people in the ﬁrst few months of the pandemic, including physical attacks.""",Yes,"Hostility, Criticism, Counter Speech, Discussion of EA prejudice, Neutral",Yes,Website,Zenodo,https://zenodo.org/record/3816667#.Yzv-GkzP2Hs,Yes,N/A,1,1,Twitter,East Asian Prejudice,English,01.01.2020 - 17.03.2020,"Implicitly before March 17th, 2020",N/A,spread of coronavirus,"They used Twitter's streaming API, using 14 hashtags that relate to both East Asia and the virus",Not discussed,,manual,No,,Yes,"Out of 159,320 collected tweets 20,000 tweets are sampled randomly","20,000",N/A,N/A,"Two trained annotators annotated each tweet. In addition, an expert cleared disagreements",Yes,"Each of the mentioned primary categories (Hostility, Criticism, Counter speech, Discussion of East Asian prejudice) is annotated for a secondary category",Not discussed,,Specific,race,Targeted,race,Yes,No,,,Yes,East Asian people,No,,,Yes,Human
384,No,"(Kurrek et al., 2020)",Towards a Comprehensive Taxonomy and Large-Scale Annotated Corpus for Online Slur Usage,Fourth Workshop on Online Abuse and Harms,Canada,"Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection. The authors make two main contributions to this end. First, they provide an annotation guide that outlines 4 main categories of online slur usage, which they further divide into a total of 12 sub-categories. Second, they present a publicly available corpus based on their taxonomy, with 39.8k human annotated comments extracted from Reddit. This corpus was annotated by a diverse cohort of coders, with Shannon equitability indices of 0.90, 0.92, and 0.87 across sexuality, ethnicity, and gender. Taken together, the taxonomy and corpus allow researchers to evaluate classifiers on a wider range of speech containing slurs.",Yes,19,XX.11.2020,04.10.2022,Yes,Yes,"""Abusive language classiﬁers have been shown to exhibit bias against women and racial minorities. Since these models are trained on data that is collected using keywords, they tend to exhibit a high sensitivity towards pejoratives. [...] Any attempt to address bias in keyword-based corpora requires a better understanding of pejorative language, as well as an equitable representation of targeted users in data collection.""",Yes,"Derogatory usage, Approriative usage, non-derogatory, homonyms",Yes,Github,,https://github.com/networkdynamics/slur-corpus,Yes,N/A,1,1,Reddit,Slur,English,XX.09.2019 - XX.11.2020,October 2007 to September 2019,N/A,N/A,"They used the ""Pushshift Reddit corpus"" and filtered for three slurs (f-slur, n-slur, t-slur)",Not discussed,,manual,No,,Not discussed,,"39,811",N/A,N/A,"20 annotators with different ethnic backgrounds, genders, and sexualities",Yes,"Derogatory Usage: Any usage that is understood to convey contempt towards a targeted individual or group. Appropriative Usage: Meaningful usage by the targeted group for an alternate, non-derogatory purpose. Non-Derogatory, Non Appropriative Usage: Meaningful usage by targeted or non-targeted groups for an alternate non-derogatory, non-appropriative purpose. Text belonging to this label retains its derogatory force. Homonyms: A slur with one or more non-derogatory alternative meanings.",Not discussed,,General,N/A,Targeted,"gender, race",Yes,No,,,No,,No,,,No,Human
387,No,"(Plaza-del-Arco et al., 2021)",OffendES: A New Corpus in Spanish for Offensive Language Research,International Conference on Recent Advances in Natural Language Processing,Spain,"Focusing on young influencers from the well-known social platforms of Twitter, Instagram, and YouTube, the authors have collected a corpus composed of 47,128 Spanish comments manually labeled on offensive pre-defined categories. A subset of the corpus attaches a degree of confidence to each label, so both multi-class classification and multi-output regression studies are possible. In this paper, they introduce the corpus, discuss its building process, novelties, and some preliminary experiments with it to serve as a baseline for the research community.",Yes,2,XX.XX.2021,05.10.2022,Yes,Yes,"""Offensive language is deﬁned as the text which uses hurtful, derogatory, or obscene terms made by one person to another person (Wiegand et al., 2019). Related terms in the literature are hate speech (Waseem and Hovy, 2016), cyberbullying (Rosa et al., 2019), toxic language (van Aken et al., 2018), aggression language (Kumar et al., 2018), or abusive language (Nobata et al., 2016). Although there are subtle differences in meaning, they are all compatible with the above general deﬁnition.""",Yes,"offensive & target is a person, offensive & target is a group of people, offensive & the target is different from a person or a group, non-offensive (but with expletive language), non-offensive",Yes,Other,,It can be accessed through contacting the authors,Yes,OffendES,1,1,"Twitter, Instagram, YouTube",Offensiveness,Spanish,XX.02.2020 - XX.03.2020,Implicitly before March 2020,N/A,N/A,"platform APIs, 12 controversial influencers were used for data collection",Not discussed,,manual,No,,Yes,They filtered for lexical diversity and the presence of potentially offensive language,"54,023",N/A,N/A,Seven annotators from Amazon Mechanical Turk,Yes,"Offensive, the target is a person (OFP): Offensive text targeting a speciﬁc individual.
Offensive, the target is a group of people or collective (OFG): Offensive text targeting a group of people belonging to the same ethnic group, gender or sexual orientation, political ideology, religious belief, or other common characteristics.
Offensive, the target is different from a person or a group (OFO): Offensive text where the target does not belong to any of the previous categories, e.g., an organization, an event, a place, an issue.
Non-offensive, but with expletive language (NOE): A text that contains rude words, blasphemes, or swearwords but without the aim of offending, and usually with a positive connotation.
Non-offensive (NO): Text that is neither offensive nor contains expletive language.",Not discussed,,Specific,other,Non-targeted,N/A,Yes,Yes,"Specific person, Groups of people",other,No,,No,,,No,Human
390,No,"(Gasparini et al., 2018)",Multimodal Classification of Sexist Advertisements,International Joint Conference on e-Business and Telecommunications,Italy,"In this paper the authors give a first insight in the field of automatic detection of sexist multimedia contents, by proposing both a unimodal and a multimodal approach. In the unimodal approach they propose binary classifiers based on different visual features to automatically detect sexist visual content. In the multimodal approach both visual and textual features are considered. They created a manually labeled database of sexist and non sexist advertisements, composed of two main datasets: a first one containing 423 advertisements with images that have been considered sexist (or non sexist) with respect to their visual content, and a second dataset comprising 192 advertisements labeled as sexist and non sexist according to visual and/or textual cues. They adopted the first dataset to train a visual classifier. Finally they proved that a multimodal approach that considers the trained visual classifier and a textual one permits good classification performance on the second dataset, reaching 87% of recall and 75% of accuracy, which are significantly higher than the performance obtained by each of the corresponding unimodal approaches.",Yes,8,XX.XX.2018,06.10.2022,Yes,Yes,"""Among the communication methods related to the advertising domain, both the image and the accompanying text, can encode several forms of sexism. [...] These communication tools are detrimental to society because of the creation of biased portrays of women, resulting in unhealthy social and physical habits.""",No,,Yes,Website,,https://mmsp.unimib.it/misogyny/,Yes,"SADI, SADIT",2,2,Advertisments,Sexism,English,Implicitly before 2018,Implicitly before 2018,N/A,N/A,N/A,Not discussed,,manual,No,,Not discussed,,615,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,gender,Targeted,gender,Yes,No,,,Yes,Misogyny,No,,,No,Human
391,No,"(Nascimento et al., 2019)",Hate Speech Detection Using Brazilian Imageboards,Brazilian Symposium on Multimedia and the Web,Brazil,"Only a few works in literature have focused on hate speech in imageboards content. This work aims to classify Brazilian Portuguese texts to detect hate speech, using data from the Brazilian 55chan imageboard to build a dataset with hate speech content. Three classifiers were trained to hate speech binary classification. The Linear Support Vector Classifier achieved the best result with 0.955 of F1-score.",Yes,8,XX.10.2019,06.10.2022,Yes,Yes,"""Imageboards provides the ideal scenario for HS, manifested as detractive messages towards people or groups because of their race, ethnicity, gender, sexual orientation, nationality, religion, or other attributes. Each IB site is structured in boards, aggregating different topics or subjects. For example, one of the most famous IBs, 4chan1, has the well-known /pol/ (Politically Incorrect) board with controversial content, where users posting anonymously are not easily linked to their real-world identities and are more likely to use offensive language""",No,,Yes,Github,,https://github.com/LaCAfe/Dataset-Hatespeech,Yes,N/A,1,1,"55chan, Twitter",Hate Speech,Portuguese,"04.06.2019, 07.06.2019, 20.06.2019","Implicitly before June, 2019",N/A,N/A,Tweepy for Twitter data and its API for the imageboard 55chan. ,Not discussed,,manual,No,,Yes,"They filtered for words regarding anger, anxiety and positive emotions out of a Brazilian Portuguese lexicon","7,672",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"race, gender, sexuality, nationality, religion, other",Yes,No,,,No,,No,,,No,Human
392,No,"(Tahmasbi et al., 2021)","“Go eat a bat, Chang!”: On the Emergence of Sinophobic Behavior on Web Communities in the Face of COVID-19",Proceedings of the Web Conference 2021,"USA, Germany","In this paper, the authors make a first attempt to study the emergence of Sinophobic behavior on the Web during the outbreak of the COVID-19 pandemic. They collect two large datasets from Twitter and 4chan’s Politically Incorrect board (/pol/) over a time period of approximately five months and analyze them to investigate whether there is a rise or important differences with regard to the dissemination of Sinophobic content. They find that COVID-19 indeed drives the rise of Sinophobia on the Web and that the dissemination of Sinophobic content is a cross-platform phenomenon: it exists on fringe Web communities like /pol/, and to a lesser extent on mainstream ones like Twitter. Using word embeddings over time, they characterize the evolution of Sinophobic slurs on both Twitter and /pol/. Finally, they find interesting differences in the context in which words related to Chinese people are used on the Web before and after the COVID-19 outbreak: on Twitter they observe a shift towards blaming China for the situation, while on /pol/ they find a shift towards using more (and new) Sinophobic slurs.",Yes,47,03.06.2021,14.11.2022,Yes,Yes,"Sinophobic. ""the Web is being exploited for the dissemination of potentially harmful and disturbing content, such as the spread of conspiracy theories and hateful speech towards specific ethnic groups, in particular towards Chinese people and people of Asian descent since COVID-19 is believed to have originated from China.""",No,,No,,,,,Twitter dataset; 4chan's /pol/ dataset,2,2,"Twitter, 4chan",Sinophobic,English,N/A,01.11.2019 - 22.03.2020,N/A,COVID-19,"To study the extent and evolution of Sinophobic behavior on the Web, the authors collect and analyze two large-scale datasets from Twitter and 4chan's Politically Incorrect board (/pol/).

To obtain data from Twitter, they use the Streaming API, which provides a 1% random sample of all tweets made available on the platform. They collect tweets posted between November 1, 2019 and March 22, 2020, and then we filter only the ones posted in English, ultimately collecting 222,212,841 tweets.

4chan is an anonymous imageboard, which is divided into several sub-communities called boards: each board has its own topic of interest and moderation policy. In this work, the authors focus on the Politically Incorrect board (/pol/) because it is the main board for the discussion of world events. They use the approach of Hine et al. [22] to collect all posts made on /pol/ between November 1, 2019 and March 22, 2020. Overall, we collect 16,808,191 posts.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,"nationality, race",Targeted,"race, nationality",No,Yes,"Nationality, Ethnicity, etc. (see Sinophobic)","nationality, race",No,,No,,,No,Human
396,No,"(Luu et al., 2021)",A Large-Scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts,Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices,Vietnam,"In recent years, Vietnam witnesses the mass development of social network users on different social platforms such as Facebook, Youtube, Instagram, and Tiktok. On social media, hate speech has become a critical problem for social network users. To solve this problem, the authors introduce the ViHSD - a human-annotated dataset for automatically detecting hate speech on the social network. This dataset contains over 30,000 comments, each comment in the dataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, they introduce the data creation process for annotating and evaluating the quality of the dataset. Finally, they evaluate the dataset by deep learning and transformer models.",Yes,13,19.07.2021,14.11.2022,Yes,Yes,"Hate Speech.

HATE: The comments have harassment and abusive contents and directly aim at an individual or a group of people based on their characteristics, religion, and nationality.

OFFENSIVE: The comments contain harassment contents, even profanity words, but do not attack any specific object.",Yes,3 categories: clean/offensive/hate,Yes,Github,,https://github.com/sonlam1102/vihsd  Please contact this email: sonlt@uit.edu.vn (Mr. Son Thanh Luu) for the dataset.,Yes,The ViHSD dataset,1,1,"Facebook, YouTube",Hate Speech,Vietnamese,N/A,N/A,N/A,N/A,"The authors collect users' comments about entertainment, celebrities, social issues, and politics from different Vietnamese Facebook pages and YouTube videos. They select Facebook pages and YouTube channels that have a high-interactive rate, and do not restrict comments. After collected data, they remove the name entities from the comments in order to maintain the anonymity.",Yes,"After collected data, they remove the name entities from the comments in order to maintain the anonymity.",manual,Yes,"The annotation process contains two main phases. The first one is the training phase, which annotators are given a detailed guidelines, and annotate for a sample of data after reading carefully. Then the authors compute the inter-annotator agreement by Cohen Kappa index (k). If the inter-annotators agreement not good enough, they will re-train the annotator, and re-update the annotation guidelines if necessary. After all annotators are well-trained, they go to annotation phase.

Two annotators annotate the entire dataset. If there are any different labels between two annotators, the third annotators annotate those labels. The fourth annotators annotate if all three annotators are disagreed.

The final label are defined by Major voting. By this way, the authors guaranteed that each comment is annotated by one label and the objectivity for each comment Therefore, the total time spent on annotating is less than four annotators doing with the same time.

(Dataset Evalutation) They randomly take 202 comments from the dataset and give them to four different annotators, denoted as Al, A2, A3 and A4, for annotating. Table 2 shows the inter-annotator agreement between each pair of annotators. Then, they compute the average inter-annotator agreement. The final inter-annotator agreement for the dataset is K = 0.52.",Not discussed,,"33,400 comments",70% (~23380),20% (~6680),N/A,Yes,Annotation guidelines for annotating Vietnamese comments include the description and examples for each label (clean/offensive/hate) and are available in table 1.,Not discussed,,General,N/A,Targeted,"religion, nationality",Yes,No,,,No,,No,,,No,Human
397,No,"(Oriola & Kotzé, 2020)",Evaluating Machine Learning Techniques for Detecting Offensive and Hate Speech in South African Tweets,IEEE Access,South Africa,"The authors developed an English corpus from South African tweets and evaluated different machine learning techniques to detect offensive and hate speech. Character n-gram, word n-gram, negative sentiment, syntactic-based features and their hybrid were extracted and analyzed using hyper-parameter optimization, ensemble and multi-tier meta-learning models of support vector machine, logistic regression, random forest, gradient boosting algorithms. The results showed that optimized support vector machine with character n-gram performed best in detection of hate speech with true positive rate of 0.894, while optimized gradient boosting with word n-gram performed best in detection of hate speech with true positive rate of 0.867. However, their performances in detection of other threatening classes were poor. Multi-tier meta-learning models achieved the most consistent and balanced classification performance with true positive rates of 0.858 and 0.887 for hate speech and offensive speech, respectively as well as true positive rate of 0.646 for free speech and overall accuracy of 0.671. The error analysis showed that multi-tier meta-learning model could reduce the misclassification error rate of the optimized models by 34.26%.",Yes,39,20.01.2020,14.11.2022,Yes,Yes,"Hate speech has no international legal definition, but it is hinged on incitement, which is an explicit and deliberate act aimed at discrimination, hostility and violence. Similarly, offensive speech has been defined as the text, which uses abusive slurs or derogatory terms, which in many contexts have been confused with hate speech.",Yes,"categories: hate speech ‘HT’, offensive speech ‘OFF’ or free speech ‘FS’.",No,,,,,South African tweets,1,1,Twitter,"Hate Speech, Offensiveness","Afrikaans-English, IsiZulu-English, Sesotho-English",N/A,05.05.2019 - 13.05.2019,N/A,"The collection targeted tweets related to 2019 South African national elections, popular South Africa individuals and trending issues such as land reclamation, Orania and white communities.","Total of 21,350 tweets of South African discourses on Twitter between the period of May 5, 2019 and May 13, 2019 were collected using Twitter Archiver, a publicly available plugin for Google Sheets, which is based on Twitter Search API. The collection targeted tweets related to 2019 South African national elections, popular South Africa individuals and trending issues such as land reclamation, Orania and white communities. Non-English tweets were removed except code-mixed English tweets, with Afrikaans, IsiZulu and Sesotho words. Also, repeated tweets as well as tweets with empty word characters were also removed.",Not discussed,,manual,Yes,"15,702 tweets that are remaining after cleaning were divided into three samples. The first sample containing 7,100 English, code-mixed English and Afrikaans tweets was annotated by two Afrikaans annotators; the second sample containing 4,500 English, code-mixed English and IsiZulu tweets was annotated by two IsiZulu annotators, while the last sample containing 4,102 English, code-mixed English and Sesotho tweets was annotated by two Sesotho annotators. After applying Cohen Kappa statistics on the three annotated samples, inter-annotator reliability agreement scores for Afrikaans, IsiZulu and Sesotho annotators were 0.837, 0.579 and 0.633, respectively. Since none of the inter-rater agreement score was less than moderate, the annotations were relied upon. To improve the reliability of the corpus, the tweets from all samples that both annotators agreed to (full agreement) were merged into a corpus. The total number of tweets in the corpus was 14,896.",Not discussed,,14896,"75% (11,172)","25% (3,725)","Six South Africans citizens, who were familiar with national issues in South Africa and English literates were recruited and trained to annotate tweet corpus as either hate speech 'HT', offensive speech 'OFF' or free speech 'FS'. Apart from English, all the annotators were literate in one other South African language. Two were literate in Afrikaans; two were literate in IsiZulu and two were literate in Sesotho.",Yes,"The rules that were used for the annotation are as follows:

1) A tweet is a hate speech if: • it is targeted against a person or group of persons. • it uses derogatory or racial slur words repetitively within the tweet • it makes use of disparaging terms with the intent to harm or incite harm.• it refers to and supports other hateful facts, hate tweets and organization. • it makes use of idiomatic, metaphorical, collocation or any other indirect means of expressions that are harmful or may incite harm • it expresses violent communications

2) A tweet is an offensive speech if: • it is targeted against a person, group of persons or organization. • it is not a hate speech. • it abuses a target using profane, derogatory or slur words.

3) A tweet is a free speech if: • It is targeted or not targeted against a person, group of persons or organization. • it is neither hate speech nor offensive speech • it is expressed by government or licenced agency of government with the intent for mass advocacy and enlightenment.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
398,No,"(Chung et al., 2019)",CONAN - COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,Italy,"Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, the authors describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech/counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data the authors also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, they provide initial experiments to assess the quality of the data.",Yes,109,28.07.2019,14.11.2022,Yes,Yes,"Hate speech. ""Defining hate speech is challenging for the broadness and the nuances in cultures and languages. For instance, according to UNESCO hate speech refers to ""expressions that advocate incitement to harm based upon the targets being identified with a certain social or demographic group"" (Gagliardone et al., 2015).
Victims of hate speech are usually targeted be- cause of various aspects such as gender, race, reli- gion, sexual orientation, physical appearance.""",Yes,"Hate speech sub-topic annotation: Culture, Economics, Crimes, Rapism, Terrorism, Women Oppression, History, other/generic  Counter-narrative type annotation: PRESENTATION OF FACTS, POINTING OUT HYPOCRISY OR CONTRADICTION, WARNING OF CONSEQUENCES, AFFILIATION, POSITIVE TONE, NEGATIVE TONE, HUMOR, COUNTER-QUESTIONS, OTHER.",Yes,Github,,https://github.com/marcoguerini/CONAN,Yes,CONAN (COunter NArratives through Nichesourcing),1,1,NGOs,Hate Speech,"English, French, Italian",N/A,N/A,N/A,N/A,"(1) Hate speech collection. For each language the authors asked two native speaker experts (NGO trainers) to write around 50 prototypical islamophobic short hate texts. This step was used to ensure that: (i) the sample uniformly covers the typical 'arguments' against Islam as much as possible, (ii) we can distribute to the NLP community the original hate speech as well as its counter-narrative.
(2) Preparation of data collection forms. The authors prepared three online forms (one per language) with the same instructions for the operators translated in the corresponding language. For each language, they prepared 2 types of forms: in the first users can respond to hate text prepared by NGO trainers, in the second users can write their own hate text and counter-narratives at the same time. In each form operators were first asked to anonymously provide their demographic profile including age, gender, and education levelsecondly to compose up to 5 counter-narratives for each hate text.
(3) Counter-narrative instructions. The operators were already trained to follow the guidelines of the NGOs for creating proper counter-narratives. Such guidelines are highly consistent across languages and across NGOs, and are similar to those in 'Get the Trolls Out' project. Furthermore, for the specific data collection task, operators were asked to follow their intuitions without over-thinking and to compose reasonable responses. The motivation for this instruction was to collect as much and as diverse data as possible, since for current Al technologies (such as deep learning approaches) quantity and quality are of paramount importance and few perfect examples do not provide enough generalization evidence. Other than this instruction and the fact of using a form - instead of responding on a SMP - operators carried out their normal counter messaging activities.

4\. Data collection sessions. For each language, the authors performed three data collection sessions on different days. Each session lasted roughly three hours and had a variable number of operators - usually around 20 (depending on their availability). Operators are different from NGO trainers and might change across sessions. Operators were gathered in the same room (NGO premises) with a computer, and received a brief introduction from the NGO trainer. This introduction was about our specific counter-narrative collection task, as described above.

Paraphrasing for augmenting data pairs: In line with the idea of artificially augmenting pairs, and since in the dataset the authors have many responses for few hate speeches, they produced two manual paraphrases of each hate speech and paired them with the counter-narratives of the original one. Therefore the authors increased the number of the pairs by three times in each language.",Not discussed,,manual,Yes,"After the data collection phase, the authors hired three non-expert annotators, that performed additional work that did not require specific domain expertise. Their work amounted to roughly 200 hours. In particular they were asked to (i) paraphrase original hate content to augment the number of pairs per language, (ji) annotate hate speech subtopics and counter-narrative types (iii) translate content from French and Italian to English to have parallel data across languages. To guarantee data quality, after the annotation and the augmentation phase, a validation procedure has been conducted by NGO trainers on the newly generated data for their specific language.",Not discussed,,"4078 hate speech/counter-narrative pairs (En,1288; Fr, 1719; It, 1071)",N/A,N/A,Three non-expert annotators,Yes,"Counter-narrative guidelines adopted by the three NGOs:

Don't be abusive Before submitting a response, make sure the response does not spread any hate, bigotry, prejudice or illegal content. We want to maintain the conversations peaceful and not to degenerate into a conflict. We are talking about people not categories.

Think about the objectives Before writing a response, think about the effect it may create and the one you want to obtain. Paying attention to the objectives will help use proper words.

Call for influential users Enlisting influential supporters (civic leaders, politicians, subject experts) will help bring attention and deepen the effect to counter-narrative.

Use credible evidence The information in hate speech may be confusing and misleading. Ask for clarification when necessary. Counter it with credible evidence and use reliable sources.

Think about the tone We can demonstrate understanding and support to those who might be attacked. Be careful of using sarcasm, humour, parody and satire. We can use them, if we are able to master it as they run the danger of being antagonistic.

--

Based on the counter-narrative classes proposed by (Benesch et al., 2016Mathew et al., 2018), the authors defined the following set of types: PRESENTATION OF FACTS, POINTING OUT HYPOCRISY OR CONTRADICTION, WARNING OF CONSEQUENCES, AFFILIATION, POSITIVE TONE, NEGATIVE TONE, HUMOR, COUNTER-QUESTIONS, OTHER. With respect to the original guidelines, they added a new type of counter-narrative called COUNTER-QUESTIONS to cover expressions/replies using a question that can be thought-provoking or asking for more evidence from the hate speaker. In fact, a preliminary analysis showed that this category is quite frequent among operator responses. Finally, each counter-narrative can be labeled with more than one type, thus making the annotation more fine- grained.

The following sub-topics (of hate speech) are determined for the annotation based on the guidelines used by NGOs to identify hate messages (mostly consistent across languages): CULTURE, criticizing Islamic culture or particular aspects such as religious events or clothesECONOMICS, hate statements about Muslims taking European workplaces or not contributing economically to the societyCRIMES, hate statements about Muslims committing actions against the lawRAPISM, a very frequent topic in hate speech, for this reason it has been isolated from the previous categoryTERRORISM, accusing Muslims of being terrorists, killers, preparing attacksWOMEN OPPRESSION, criticizing Muslims for their behavor against womenHISTORY, stating that we should hate Muslims because of historical eventsOTHER/GENERIC, everything that does not fall into the above categories.",Not discussed,,Specific,"Gender, Class, Religion, other",Targeted,"gender, race, religion, sexuality, body",No,Yes,"Gender, Class, Religion, islamophobic hate, Crimes, Rapism, Terrorism, Women Oppression, History, other/generic","gender, class, religion, other",No,,No,,,Yes,Human
399,No,"(Nagar et al., 2022)",Capturing the Spread of Hate on Twitter Using Spreading Activation Models,Complex Networks & Their Applications X,India,"Prior works have analyzed the hateful users’ social network embedding, hateful user detection using belief propagation models, and the spread velocity of hateful content. However, these prior works fail to factor in the multiple hateful forms (such as hate against gender, race and ethnicity) and the temporal evolution of hate spread, limiting their applicability. The authors in this study take a holistic approach wherein they model the spread of hate as a single form and fine-granular spread of hateful forms. They extend the traditional spread and activation (SPA) model to capture the spread of hate and its forms. They use SPA to model, the spread of hate as one single form while TopSPA captures the spread of multiple hate forms. They also propose ways to detect hateful forms by using latent topics present in hateful content. They empirically demonstrate their approach to a dataset from Twitter that contains ample hate speech instances along with users labelled as hateful or not.",Yes,0,01.01.2022,14.11.2022,Yes,Yes,"Hate speech can be defined as ""language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group"".",No,,No,,,,,"The dataset (newly annotated);

existing datasets: Toxic Comment Classification Challenge, a dataset from a Semeval challenge called “Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)”",3,1,Twitter,Hate Speech,English,N/A,01.01.2017 - 30.09.2017,N/A,N/A,"The authors use the dataset provided by a previous reasearch\*. This dataset contains 200 most recent tweets of 100, 386 users, totalling around 19M tweets. It also contains a retweet induced graph of the users. It has 2, 286, 592 directed edges. The retweet-induced graph is a directed graph where each node represents a user in Twitter, and each edge represents a retweet in the network, where the user u1 has retweeted user u2. The authors convert the edge list to an adjacency matrix and normalise. They use this normalised matrix for both SPA and TopSPA. Furthermore, every tweet is categorised as an original tweet, retweet or quote (retweet with a comment). Out of the 100,386 users, labels (hateful or normal) are available for 4, 972 users, out of which 544 users are labelled as hateful and the rest as normal. The original dataset does not have labels for the tweet content.

\* Ribeiro, M.H., Calais, P.H., Santos, Y.A., Almeida, V.A., Meira Jr, W.: ""Like sheep among wolves"": characterizing hateful users on Twitter. arXiv preprint arXiv: 1801.00317 (2017)",Not discussed,,manual,Yes,"The authors manually annotate the tweets as hateful or not. A subset of tweets were annotated by a group of four independent annotators whose primary language is English. The Inter-Annotator Agreement (IA) score (Cohen k) was 0.87. A high k score was obtained due to a singular class, hateful or not.",Yes,"They pick 10 original tweets produced by each of the 544 users labelled as hateful in the dataset. After preprocessing the tweets, 4, 720 original tweets are left.","4720 manually annotated in this work, 19M non labelled tweets; 160k (Toxic Comment Classification Challenge); 9k (hatEval)",80% (~128k in Toxic Comment Classification Challenge; ~7.2k in hatEval),20% (~32k in Toxic Comment Classification Challenge~1.8k in hatEval);,A group of four independent annotators whose primary language is English.,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
401,No,"(Chandra et al., 2021)",“Subverting the Jewtocracy”: Online Antisemitism Detection Using Multimodal Deep Learning,13th ACM Web Science Conference 2021,India,"Unlike other major forms of online abuse like racism, sexism, etc., online antisemitism has not been studied much from a machine learning perspective. The authors present the first work in the direction of automated multimodal detection of online antisemitism. The task poses multiple challenges that include extracting signals across multiple modalities, contextual references, and handling multiple aspects of antisemitism. Unfortunately, there does not exist any publicly available benchmark corpus for this critical task. Hence, they collect and label two datasets with 3,102 and 3,509 social media posts from Twitter and Gab respectively. Further, they present a multimodal deep learning system that detects the presence of antisemitic content and its specific antisemitism category using text and images from posts. They perform an extensive set of experiments on the two datasets to evaluate the efficacy of the proposed system. Finally, they also present a qualitative analysis of the study.",Yes,14,22.06.2021,14.11.2022,Yes,Yes,"According to International Holocaust Remembrance Alliance (IHIRA), ""Antisemitism is a certain perception of Jews, which may be expressed as hatred toward Jews. Rhetorical and physical manifestations of antisemitism are directed toward Jewish or non-Jewish individuals and/or their property, toward Jewish community institutions and religious facilities."". Unlike some forms of hate-speech like sexism, cyberbullying, xenophobia etc., antisemitism originates from multiple aspects. In addition to the discrimination on the basis of race, antisemitism also includes discrimination on the basis of religion (e.g.difference from Christianity), economic activities (e.g.money lending) and political associations (e.g.Israel-Palestine issue, holding power at influential positions).",Yes,"Antisemitism Categories: Political Antisemitism, Economic Antisemitism, Religious Antisemitism, Racial Antisemitism",Yes,Github,,https://github.com/mohit3011/Online-Antisemitism-Detection-Using-MultimodalDeep-Learning,Yes,Twitter Antisemitism Dataset; Gab dataset,2,2,"Twitter, Gab","Antisemitism, Hate speech",English,N/A,N/A,N/A,N/A,"The authors choose Twitter and Gab as the OSM platforms to gather data for our study. After gathering a massive collection of posts from Twitter as well as Gab, they retained only those posts which contained text as well as images. Further, they ensured that each post included at least one term from a high precision lexicon. This lexicon contains common racial slurs used against Jews along with other words like 'Jewish', 'Hasidic', 'Hebrew', 'Semitic', 'Judaistic', 'israeli', 'yahudi', 'yehudi' to gather non-antisemitic posts as well, thereby helping maintain a balanced class distribution. Presence of these terms does not necessarily indicate presence/ absence of antisemitism and hence they manually annotate the posts.",Yes,They removed all user sensitive information and followed other ethical practices to ensure user privacy.,manual,Yes,"The annotators were given a detailed guideline along with examples to identify instances of antisemitism. Moreover, to ensure that the annotators had enough understanding of the task, the authors conducted multiple rounds of test annotations followed by discussions on disagreements. In the annotation procedure, each example was annotated by three annotators and the disagreements were resolved through discussion between all annotators. Each example was annotated on two levels after looking at the text as well as the image - (1) binary label (whether the example in antisemitic or not), and (2) if the example is antisemitic then assign the respective antisemitism category. The authors used Fleiss' Kappa score to compute the inter-annotator agreement. The Fleiss' kappa score came out to be 0.707 which translates to a substantial agreement between the annotators.",Yes,"After gathering a massive collection of posts from Twitter as well as Gab, they retained only those posts which contained text as well as images. Further, they ensured that each post included at least one term from a high precision lexicon. This lexicon contains common racial slurs used against Jews along with other words like 'Jewish', 'Hasidic', 'Hebrew', 'Semitic', 'Judaistic', 'israeli', 'yahudi', 'yehudi' to gather non-antisemitic posts as well, thereby helping maintain a balanced class distribution.",Twitter:3102 Gab: 3509,5-fold cross-validation (train/dev/test=64:16:20 in each fold),5-fold cross-validation (train/dev/test=64:16:20 in each fold),Four undergraduate students who are fluent in English,Yes,The annotators were given a detailed guideline along with examples to identify instances of antisemitism.,Not discussed,,Specific,"race, religion, class, political",Targeted,"race, religion, class, political",Yes,Yes,"Racial, Religious, Class, Political, Antisemitism","race, religion, class, political",No,,No,,,No,Human
402,No,"(Yang et al., 2020)",BERT-BiLSTM-CRF for Chinese Sensitive Vocabulary Recognition,Artificial Intelligence Algorithms and Applications,China,"In this paper, the authors firstly construct the dataset of uncivilized language (ULN dataset), which is acquired through web crawler on the website. Secondly, they propose a method to identify Chinese sensitive words on the network, which combine the Bidirectional Encoder Representations from Transformers with Bidirectional Long Short Term Memory Network and Conditional Random Field (BERT-BiLSTM-CRF). They use three models to identify sensitive words in ULN dataset. The experimental results show that, the model proposed in this paper has excellent performance, compared with the classical Bidirectional Long Short Term Memory Network (BiLSTM-CRF) and Convolutional Neural Network (CNN).",Yes,3,26.05.2020,14.11.2022,Yes,Yes,Sensitive Vocabulary,Yes,"three categories: abusive words (ABU), political sensitive words (SEN) and heresy words (HER).",No,,,,,uncivilized language dataset (ULN dataset),1,1,YouTube,"Sensitive Vocabulary, Abusiveness",Chinese,N/A,N/A,N/A,N/A,"The data comes from YouTube comments on hot issues, which is acquired through web crawler on the website. Through careful reading and analyzing, more than 9,600 texts were selected. In addition, the authors remove invalid characters in the text, such as HTML tags, emoticons. Last but not least, the text is used as the initial corpus for manual annotation.",Not discussed,,manual,Yes,"The authors divide the vocabulary into three categories: abusive words (ABU), political sensitive words (SEN) and heresy words (HER). These words are marked in BIO format, where ""B"" stands for the beginning of the vocabulary and ""I” stands for the middle or end of the vocabulary, ""O"" indicates other information without sensitive information. For example, ""脑残"" is labeled as ""B-ABU I-ABU"". Due to the special combination of sensitive words on the network, there is no strict standard of language structure. In the process of annotation, different people have different understanding of the text, so the boundary of the same word may be different. Therefore, to ensure accuracy, the authors asked three people to annotate the same text. They will further study the texts with different opinions when labeling.",Not discussed,,37912 tags,26377,6329,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
404,No,"(Guest et al., 2021)",An Expert Annotated Dataset for the Detection of Online Misogyny,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,UK,"The authors present a new hierarchical taxonomy for online misogyny, as well as an expert labelled dataset to enable automatic classification of misogynistic content. The dataset consists of 6567 labels for Reddit posts and comments. As previous research has found untrained crowdsourced annotators struggle with identifying misogyny, the authors hired and trained annotators and provided them with robust annotation guidelines. They report baseline classification performance on the binary classification task, achieving accuracy of 0.93 and F1 of 0.43. The codebook and datasets are made freely available for future researchers.",Yes,27,19.04.2021,14.11.2022,Yes,Yes,Misogyny,Yes,"Misogynistic content, four categories: (i) Misogynistic Pejoratives, (ii) descriptions of Misogynistic Treatment, (iii) acts of Misogynistic Derogation and (iv) Gendered Personal attacks against women.  Non-misogynistic content, three categories: (i) Counter speech against misogyny, (ii) Nonmisogynistic personal attacks and (iii) None of the categories.",Yes,Github,,https://github.com/ellamguest/online-misogyny-eacl2021,Yes,"final_labels: the dataset of agreed labels

original_labels: the dataset of original labels coded by annotators before facilitation",2,2,Reddit,Misogyny,English,XX.02.2020 - XX.05.2020,XX.02.2020 - XX.05.2020,N/A,N/A,"To ensure that this dataset contains enough misogynistic abuse the authors began with targeted sampling, taking content from 12 subreddits that were identified as misogynistic in previous research. This includes subreddits such as r/MensRights, r/seduction, and r/TheRedPill. The sources used to identify these subreddits are available in Table 9 in the Appendix. The authors then identified 22 additional subreddits which had been recommended by the moderators/owners of the original 12 subreddits in the 'sidebar'. Some of these are not misogynistic but discuss women (e.g. r/AskFeminists) and/or are otherwise related to misogyny. Table 9 in the Appendix lists the 34 targeted subreddits and the number of entries and threads for each in the dataset. Over 11 weeks, for each subreddit, they collected the entire threads of the 20 most popular posts that week.

Using subreddits to target the sampling rather than keywords should ensure that more linguistic variety is captured, minimising the amount of bias as keywords such as 'slut' are associated with more explicit and less subtle forms of abuse. Nonetheless, only sampling from suspected misogynistic communities could still lead to classifiers which only identify the forms of misogyny found in those targeted contexts. To account for this potential bias, and to enable greater generalisabilty, the authors sampled content from 71 randomly selected subreddits. These accounted for 18% of threads and 16% of entries in the dataset. For each randomly selected subreddit, they collected the thread of the most popular post. All threads were in English with the exception of one thread from the subreddit r/Romania.

Posts and comments were collected from February to May 2020 using the python package PRAW, a wrapper for the Reddit API. Posts on Reddit have a text title and a body which can be text, an image, or a link. For posts with a text body they combined this with the post title to create a single unit of text. For the 29% of posts where the body was an image they also collected the image.",Not discussed,,manual,Yes,"To mitigate annotator biases, the authors used expert annotators specifically trained in identifying misogynistic content, as well as a group-based facilitation process to decide final labels. Due to time and resource constraints, the final dataset is smaller than if we had used crowdsourced workers but captures more nuanced and detailed cases of misogyny. Six annotators worked on the dataset. Annotators were trained in the use of a codebook detailing the taxonomy and annotation guidelines. The codebook was updated over time based on feedback from the annotators.

Annotators independently marked up each entry for the three levels. For all level two categories other than 'None', they also highlighted the specific part of the entry which was relevant to the labelled category (the 'span'). This is particularly important information for long posts which can contain multiple forms of abuse.

Each entry was annotated by either two (43%) or three (57%) annotators. If all annotators made the exact same annotation (including all three levels and highlighting) this was accepted as the final annotation. All other entries were flagged as disagreements. Annotators reviewed the disagreements in weekly meetings which were overseen by an expert facilitator, a PhD researcher who had developed the annotation taxonomy and was familiar with the literature on online misogyny and hate speech classification. The role of the facilitator was to promote discussion between annotators and ensure the final labels reflected the taxonomy. Each disagreement was discussed until the annotators reached a consensus on the final agreed label or labels.",Not discussed,,"6,567 agreed labels",80% (~5254),"1,277","The majority of annotators were White-British, spoke English as a first language, and had or were pursuing a University degree. Two-thirds of annotators were women. All annotators were based in the United Kingdom and worked remotely. Five of the six annotators gave permission to share their basic demographic information. All were between 18 and 29 years old. Two had high school degrees, two had an undergraduate degree, and one had a postgraduate taught degree or equivalent. Four identified as women, one as a man. All were British nationals, native English speakers, and identified as ethnically white.

All annotators used social media at least once per day. Two had never been personally targeted by online abuse, two had been targeted 2-3 times (in separate instances more than a year ago), and one had been personally targeted more than 3 times within the previous month.",Yes,Annotators were trained in the use of a codebook detailing the taxonomy and annotation guidelines. The codebook was updated over time based on feedback from the annotators.,Yes,The annotators were paid £14 per hour for all work including training.,Specific,gender,Targeted,gender,Yes,Yes,Gender,gender,Yes,Misogyny,No,,,Yes,Human
405,No,"(Saroj & Pal, 2020)",An Indian Language Social Media Collection for Hate and Offensive Speech,Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language,India,"During the parliamentary elections, people's interaction with the candidates in social media posts reflects a lot of social trends in a charged atmosphere. People's likes and dislikes on leaders, political parties and their stands often become subject of hate and offensive posts. The authors collected social media posts in Hindi and English from Facebook and Twitter during the run-up to the parliamentary election 2019 of India (PEI data-2019). They created a dataset for sentiment analysis into three categories: hate speech, offensive and not hate, or not offensive. The authors report here the initial results of sentiment classification for the dataset using different classifiers.",Yes,6,11.05.2020,14.11.2022,Yes,Yes,"Hate speech in communication, is deemed to be harmful (individually or at a social level) based on defined 'protected attributes' such as race, disability, sexuality, etc., while Offensive speech is simply any communication that upsets someone.",Yes,"A: binary classes: Hate and Offensive (HOF) and non-hate or offensive (NOT) B: Hate, Offensive, None.  C: within HOF: Targeted Insult (TIN), Untargeted (UNT)",No,,,,,"PEI data-2019 (the parliamentary election 2019 of India);

Existing datasets: SemEval 2019 task 6 datasetFIRE 2019 task HASOC dataset",3,1,"Facebook, Twitter","Hate Speech, Offensiveness","Hindi, English",N/A,11.04.2019 - 23.05.2019 (tweets),N/A,the parliamentary election 2019 of India (11 April to 19 May 2019),"The authors collected data from Facebook and Twitter during the parliamentary election 2019 of India. For Twitter, the data collection was done using the Twitter API with a tweepy Python library. The tweets collected from elected candidates' Twitter accounts and also collected with keywords #Twitter accounts name' and #Loksabha election, #election 2019, #loksabha election 2019 of India. For the hashtags, the tweets were between 11 April to 23 May 2019. For Facebook, they used the Facepager tool (Dr. Jakob Jünger, 2019) to capture messages. The collected tweets were in English, Hindi, and some other regional languages. For this study, the authors concentrated on tweets and messages in Hindi and English language. They collected more than ten thousand posts from Facebook and Twitter. Out of them, they found 20% tweets belonging to the hate speech and offensive content.",Not discussed,,manual,Yes,"PEI data was annotated by three annotators using a hierarchical three-level annotation model introduced in Zampieri et al. (2019) and Mandi et al. (2019).

The average score of inter-annotation agreement (Cohen's Kappa) for Task A is 0.87 for the English language and 0.89 for the Hindi language. Similarly, the average Cohen's Kappa for Task B and Task C are 0.85 and 0.89, respectively. We also evaluate Krippendorff's alpha which are 0.90, and 0.89 for English and Hindi respectively.

The hierarchy of annotations:

A: HOF/NOT;

B: HATE/OFFN/NONE;

C: TIN;UNT",Not discussed,,2007,1519,488,"The annotation is done by three undergraduate students of Engineering whose first language is Hindi for speaking and writing, and they can speak and write English as well.",Not discussed,,Not discussed,,Specific,other,Targeted,"race, disability, sexuality",Yes,Yes,Targeted Insult (TIN),other,No,,No,,,No,Human
408,No,"(Perifanos & Goutsos, 2021)",Multimodal Hate Speech Detection in Greek Social Media,Multimodal Technologies and Interaction,Greece,"This study presents a new multimodal approach to hate speech detection by combining Computer Vision and Natural Language processing models for abusive context detection. The study focuses on Twitter messages and, more specifically, on hateful, xenophobic, and racist speech in Greek aimed at refugees and migrants. The authors combine transfer learning and fine-tuning of Bidirectional Encoder Representations from Transformers (BERT) and Residual Neural Networks (Resnet). The contribution includes the development of a new dataset for hate speech classification, consisting of tweet IDs, along with the code to obtain their visual appearance, as they would have been rendered in a web browser. The authors have also released a pre-trained Language Model trained on Greek tweets, which has been used in the experiments. They report a consistently high level of accuracy (accuracy score = 0.970, f1-score = 0.947 in our best model) in racist and xenophobic speech detection.",Yes,15,29.06.2021,14.11.2022,Yes,Yes,"Hate speech is defined by Cambridge Dictionary as ""public speech that expresses hate or encourages violence towards a person or group based on something, such as race, religion, sex, or sexual orientation"".

Similarly, in the context of tweets, Reference [6] defines a tweet as toxic if it contains harassing, threatening, or offensive / harsh language directed toward a specific individual or group.

In this paper, we are using the term hateful to clearly denote hateful xenophobic content and toxic to refer to general toxic online behaviour.",No,,Yes,Github,,https://github.com/kperi/MultimodalHateSpeechDetection,Yes,the dataset,1,1,Twitter,Hate Speech,"Greek, English",N/A,N/A,N/A,N/A,"Initially, the authors collected all tweets from the hashtag #απέλαση (deportation), along with two months of tweets containing the racist slang term ""λάθρο"", which is used to refer to undocumented immigrants (λάθρο from λαθραίος, illegal), as prime instances of hateful tweets. Interestingly enough, during the same period of time two major events occurred, namely the conviction of the neo-Nazi party Golden Dawn as a criminal organisation and the expected beginning of the trial for the murder of LGBTQ activist Zak Kostopoulos. Similar to what Jaki and De Smedt observe, there is an overlap of neo-Nazi, far-right and alt-right social media accounts that systematically target refugees, LGBTQ activists, feminists, and human right advocates, and this is reflected in the dataset, especially with hashtag combinations. The authors then extracted a set of 500 Twitter users from these tweets and further enriched the user base with accounts appearing in mentions and replies in the bootstrapped data. They also included known media and public figure accounts, resulting in a set of 1263 users in total. They collected approximately 126,000 tweets in total.",Not discussed,,manual,Yes,"For the labelling process, the authors asked for the help of three human right activists in accordance with the process described by Reference [4]\*. The final label of each tweet in the dataset was determined by the majority vote of all 3 annotators.

In ninety-two percent of annotations all 3 annotators were in agreement. Annotators were allowed to lookup additional context, typically for tweets that were replies to other tweets as most disagreements were found in tweets that were considered hateful in a given context and not in a standalone fashion.

It is also important to note here that not all tweets from the boostrap are racist / xenophobic and toxic, as human rights users also used these hashtags in an attempt to mitigate xenophobic propaganda. For the annotation task, they used the docanno tool.

\* Waseem, Z. Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech Detection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social Science, Austin, TX, USA, 5 November 2016Association for Computational Linguistics: Austin, TX, USA, 2016pp. 138-142.",Yes,They randomly sampled and labelled 4004 out of ~126k collected tweets.,4004,80% (~3203),20% (~801),Three human right activists,Not discussed,,Not discussed,,Specific,"race, nationality",Targeted,nationality,Yes,Yes,"Race, Ethnicity, Nationality","race, nationality",No,,No,,,No,Human
410,No,"(Saitov & Derczynski, 2021)",Abusive Language Recognition in Russian,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Russia, Denmark","This work addresses automatic detection of abusive language in Russian. The lexical, grammatical and morphological diversity of Russian language present potential difficulties for this task, which is addressed using a variety of machine learning approaches. Finally, competitive performance is reached over multiple domains for this investigation into automatic detection of abusive language in Russian.",Yes,1,20.04.2021,14.11.2022,Yes,Yes,"abusive language: In this case, we use the OLID annotation definition of abusive language (Zampieri et al., 2019). This covers profanity, and targeted and untargeted insults and threats, against both groups and individuals. Specifically, in accordance this scheme, we consider the use of racial and other group-targeted slurs abusive.",No,,Yes,Github,,https://github.com/Sariellee/Russan-Hate-speech-Recognition,Yes,"RSPRSP+RTC;

(Russian South Park (RSP)Kaggle “Russian Language Toxic Comments” dataset (RTC))",2,2,South Park,Abusiveness,Russian,N/A,N/A,N/A,N/A,"The authors searched for publicly available datasets containing considerable amounts of abusive language. Russian Troll Tweets is a repository consisting of 3 million tweets. This was filtered to only Cyrillic texts. This data is not labeled, thus a subset of the data was labeled manually for use in this research. During labeling, the data turned out to contain significantly less abusive language than expected. An additional resource, the RuTweetCorp (Rubtsova, 2013), was also annotated for abusive language. In search for sources rich in abusive language, the ""South Park"" TV show was found. The Russian subtitles for it embodied a high density of profanity, hate-speech, racism, sexism, various examples of ethnicity and nationality abuse. The subtitles from more than four seasons of the series yielded many instances of abusive language. This data, Russian South Park (RSP), was annotated manually. Interannotator agreement (IAAcomputed with Cohen's Kappa) over the whole dataset is 0.68 among three L13 Russian annotators. To complement this, the Kaggle ""Russian Language Toxic Comments"" dataset (RTC) was also annotated. The dataset contains more than 14 000 labeled samples of hate speech.

As well as in many “in situ” abusive language research, an abusive language lexicon was also constructed. The text data that was collected previously contained a fair amount of such vocabulary, however, the dictionary should not be limited by the dataset. HateBase (Tuckwood, 2017) contains only 17 abusive Russian words. VK, the largest social network in Russia and CIS, has an abusive speech filter dictionary published unofficially, containing a large lexicon of abusive words. Another source is russki-mat, an open dictionary of Russian curse words with proper explanations and examples of usage. Overall, the multiple-source lexicon built contains more than 700 unique terms.",Not discussed,,manual,Not discussed,,Not discussed,,RSP: 1385 RSP+RTC: 14412,80% (~1108; ~11530),20% (~277; ~2882),"Three L1\* Russian annotators.

\*I.e. as first language",Not discussed,,Not discussed,,General,N/A,Targeted,race,Yes,No,,,No,,No,,,No,Human
412,No,"(Jha & Mamidi, 2017)",When does a compliment become sexist? Analysis and classification of ambivalent sexism using twitter data,Proceedings of the Second Workshop on NLP and Computational Social Science,India,"According to ambivalent sexism theory (Glick and Fiske, 1996), it comes in two forms: Hostile and Benevolent. While hostile sexism is characterized by an explicitly negative attitude, benevolent sexism is more subtle. Previous works on computationally detecting sexism present online are restricted to identifying the hostile form. The authors’ objective is to investigate the less pronounced form of sexism demonstrated online. They achieve this by creating and analyzing a dataset of tweets that exhibit benevolent sexism. By using Support Vector Machines (SVM), sequence-to-sequence models and FastText classifier, they classify tweets into Hostile, Benevolent or Others, depending on the kind of sexism they exhibit. They have been able to achieve an F1-score of 87.22% using FastText classifier. The work helps analyze and understand the much prevalent ambivalent sexism in social media.",Yes,120,03.08.2017,14.11.2022,Yes,Yes,"Sexism, as given by the Oxford dictionary, is the prejudice, stereotyping, or discrimination, typically against women, on the basis of sex'.

…Glick and Fiske (1997) proposed ambivalent sexism theory that talked about two related but opposite orientations towards a particular gender: (i) Hostile Sexism (HS), i.e., sexist antipathy and (ii) Benevolent Sexism (BS), i.e., a subjectively positive view towards men or women. Hostile sexism is angry, harsh and expresses an explicitly negative viewpoint. Benevolent Sexism, on the other hand, is often disguised as a compliment. Moreover, there is a reverence for the stereotypical role of women as mothers, daughters and wives. BS puts women on a pedestal, but reinforces their sub-ordination.",Yes,three classes: benevolent sexism; hostile sexism; others (not sexist),Yes,Github,,https://github.com/AkshitaJha/NLP_CSS_2017/,Yes,Benevolent Sexist dataset,1,1,Twitter,"Sexism, Benevolent sexism",English,N/A,N/A,N/A,N/A,"For the purpose of classification of tweets on the basis of the type of sexism, they required a dataset that displayed benevolent sexism (BS). Hence, they created their own corpus of tweets belonging to ""Benevolent' class. In addition to this, they used the publicly available hate speech corpus (Waseem and Hovy, 2016) to collect tweets belonging to 'Hostile' and 'Others' classes. Tweets labelled as 'sexist' and 'neither' in the hate-speech dataset make up the 'Hostile' and 'Others' class in the corpus respectively. Distribution of tweets in the

The authors collected a total of 95,292 tweets using the public Twitter Search API. The terms queried were common phrases and hashtags that are generally used when exhibiting benevolent sexism. Some of them were: 'as good as a man', 'like a man', 'for a girl', 'smart for a girl', 'love of a woman', '#adaywithoutwomen', '#womensday', '#everydaysexism' and '#weareequal'. These lead to a dataset of tweets that were sexist in nature, both towards women and men. E.g.: 'He is a man who can't act like a man' is sexist towards men. We extracted tweets that were in English.",Not discussed,,manual,Yes,"To identify and annotate BS, the authors made use of the ambivalent sexism theory proposed by Glick and Fiske (1997) in social psychology. Sexism is hypothesized to encompass three sources of male ambivalence: Paternalism, Gender Differentiation and Heterosexuality. Each of these three components have two types, one of them results in hostile sexism and the other gives rise to benevolent sexism. In order to clearly identify benevolent sexism, they studied the tweets and analyzed if it showed any one the three behaviors: protective paternalism, complementary gender differentiation, and heterosexual intimacy. If the tweet exhibited any one of the above, they annotated it as benevolently sexist.

After they had manually identified benevolent tweets, they asked three 23-year old non-activist feminists to cross-validate the collected unique tweets to remove any kind of annotator bias. Fleiss' kappa score was calculated to assess the reliability of the agreement between the validators.",Not discussed,,10095 unique tweets,70% (~7067),30% (~3028),The authors asked three 23-year old non-activist feminists to cross-validate the collected unique tweets.,Yes,"To identify and annotate BS, the authors made use of the ambivalent sexism theory proposed by Glick and Fiske (1997) in social psychology. Sexism is hypothesized to encompass three sources of male ambivalence: Paternalism, Gender Differentiation and Heterosexuality. Each of these three components have two types, one of them results in hostile sexism and the other gives rise to benevolent sexism. In order to clearly identify benevolent sexism, they studied the tweets and analyzed if it showed any one the three behaviors: protective paternalism, complementary gender differentiation, and heterosexual intimacy. If the tweet exhibited any one of the above, they annotated it as benevolently sexist.

• Paternalism: Paternalism encompasses dominative paternalism and protective paternalism. Supporters of the former hold the view of women not being fully competent adults (Brehm, 1992;, Peplau et al., 1983)whereas those who support the latter, view women as the weaker sex who need to be loved, cherished and protected (Peplau et al., 1983Tavris et al., 1984). Protective paternalism results in benevolent sexism whereas dominative paternalism results in hostile.

• Gender Differentiation: Akin to dominative paternalism, competitive gender differentiation justifies patriarchy in the society by viewing men as ones having governing capabilities in the society (Tajfel, 2010). This gives rise to hostile sexism. On the other hand, complementary gender differentiation results in benevolent sexism as it shows women having favourable traits that men stereotypically lack (Eagly and Mladinic, 1994).

• Heterosexuality: Similarly, heterosexual intimacy gives rise to benevolent sexism by viewing women as romantic objects with a genuine desire for psychological closeness (Berscheid et al., 1989)and heterosexual hostility is shown in cases where, for some men sexual attraction towards women may not be separate from the desire to dominate them (Bargh and Raymond, 1995Pryor et al., 1995). This results in hostile sexism.",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Gender (Sexism),gender,No,,No,,,No,Human
414,No,"(Ibrohim & Budi, 2018)",A Dataset and Preliminaries Study for Abusive Language Detection in Indonesian Social Media,Procedia Computer Science,Indonesia,"This paper discusses a preliminaries study for abusive language detection in Indonesian social media and the challenge in developing a system for Indonesian abusive language detection, especially in social media. The authors also built reported an experiment for abusive language detection on Indonesian tweet using machine learning approach with a simple word n-gram and char n-gram features. They use Naive Bayes, Support Vector Machine, and Random Forest Decision Tree classifier to identify the tweet whether the tweet is a not abusive language, abusive but not offensive, or offensive language. The experiment results show that the Naive Bayes classifier with the combination of word unigram + bigrams features gives the best result i.e. 70.06% of F1 - Score. However, if they classifying the tweet into two labels only (not abusive language and abusive language), all classifier that they used gives a higher result (more than 83% of F1 - Score for every classifier). The dataset in this experiment is available for other researchers that interest to improved this study.",Yes,81,01.01.2018,14.11.2022,Yes,Yes,"Abusive language is an expression that contains abusive/dirty words or phrases, both oral or text.",Yes,"three labels: not abusive language, abusive but not offensive, and offensive language",Yes,Github,,https://github.com/okkyibrohim/id-abusive-language-detection,Yes,The Dataset for Abusive Language Detection in Indonesian Social Media,1,1,Twitter,Abusiveness,Indonesian,N/A,N/A,N/A,N/A,"The first step of this research experiments is to crawl the Twitter data. Twitter data crawling process was done using 'Twitter API and Tweepy Library . In this preliminaries experiment for Indonesian abusive language detection, the authors crawl the tweet data using abusive words references\* and only several informal forms of abusive words as the query. The several informal forms of abusive words that they used as query when crawling the Twitter data are bejad (bejat), brengsek (berengsek), asu (anjing), asw (anjing), bangke (bangkai), and goblog (goblok). This is because those informal abusive words form often found in Indonesian social media. They collect about 60,000 tweet data. From the data was collected, we filter the data by erasing the duplicated tweets and the tweets that using foreign or local language. Filtering data process give 2,500 data that is ready to be labeled.

\*In the Indonesian language, the abusive/dirty words usually come from a condition, animals, astral beings, an object, a part of a body, family member, activity and a profession"". Below is further explanations about the types of abusive/dirty word references in Indonesian language:

• Condition. Words that expressed unpleasant condition in a conversation usually used as abusive words. In general, there are three things that could or might be connected with these unpleasant condition, which are mental disorder (e.g.: gila, bego, goblok, idiot, sinting, bodoh, tolol, sontoloyo, geblek, sarap), sexual deviation (e.g.: lesbian, homo, banci), lack of modernization (e.g.: kampungan, udik, alay), physical disability (e.g.: buta, budek, bolot, bisu), condition where someone doesn't have etiquette (e.g.: berengsek, bejat) conditions that are not sanctioned by god or religion (e.g.: keparat, jahanam, terkutuk, kafir, najis), and condition that related to unfortunate circumstances (e.g.: celaka, mati, modar, sialan, mampus.

• Animals. Not all animals can be used as abusive words. Animals that are used as offensive words usually refer to its certain bad characteristic, which are disgusting for some people (e.g.: anjing), disgusting and forbidden in certain religion (e.g.: babi), annoying (e.g.: bangsat, monyet, kunyuk), parasitic (e.g.: lintah), lusty (e.g.: buaya, bandot), and noisy (e.g.: beo).

• Astral beings. The examples of astral beings that usually used as abusive words are setan, setan alas, iblis, tuyul, and kunti. They are all astral beings that often interfere with human life.

• An object. Same as animals and astral beings, the objects that usually used as abusive words are based on their bad characteristic, such as bad smell (e.g.: tai, tai kucing, bangkai), dirty and worn out (e.g.: gembel, gombal), and disturbing sound (e.g.: sompret).

• A part of a body. Body parts that used as abusive words usually closely related to sex activity such as kontol, memek, tempik, and jembut. Another body part often used in cursing is eye (mata in Indonesian) in the form of matamu (your eye) which means one cursing the other for not using their eyes properly and making mistakes because of it. The other phrases are hidung belang and mata duitan which are used figuratively to curse a pervert man and a person who choose money over everything, respectively.

• Family member. Indonesian people usually adds suffix -mu on words refer to relatives as a curse, such as ibumu (your mother), bapakmu (your father), kakekmu (your grandfather), and nenekmu (your grandmother).

• Activity. Abusive words on the activity are usually more indented towards sexual, such as ngentot, and ngewe.

• Profession. One's occupation, especially low-class occupation forbidden by religion, often used by Indonesian people as abusive words. Those occupations include maling, sundal, bajingan, copet, lonte, cecenguk, kacung, pelacur, pecun, jablay and perek.",Not discussed,,manual,Yes,"After filtering the tweet data, the dataset was labeled into three labels namely not abusive language, abusive but not offensive, and offensive language. The dataset was labeled by 20 volunteer annotators. To ensure that annotators understand the task, the authors make an annotation guide and explaining that annotation guide to the annotators. Each tweet in the dataset was annotated by three annotators. The authors used 100% annotators agreement, so the tweet that has a different label removed from the dataset. The labeling process gives 2,016 data that have 100% annotators agreement.",Not discussed,,2016,10-fold cross-validation,10-fold cross-validation,20 volunteer annotators,Yes,"To ensure that annotators understand the task, the authors make an annotation guide and explaining that annotation guide to the annotators.",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
415,No,"(Gharbi et al., 2021)",TEET! Tunisian Dataset for Toxic Speech Detection,Proceedings of the Fifth Workshop on Widening Natural Language Processing,Tunisia,"This paper describes a full system of detecting automatically tunisian hate speech from data collection and annotation to the model's application. Various models have been investigated as well as different classification tasks have been experimented. Results have shown that machine learning classifiers outperform deep learning models in this case. Furthermore, the performance of the classifiers has been always better when solving a binary classification problem rather than a 3-way classification. This is justifying that it is easier to discriminate between normal and profane language rather than differentiating between hate and abusive speech due to thin boundaries even in their definitions.",Yes,0,XX.11.2021,14.11.2022,Yes,Yes,Toxic speech,Yes,"3-way classification: Hate, Abusive and Normal.  2nd layer of annotation: (related to class hate): Racism, Sexism, Homophobic, Religion and Others. In class others, hate speech related to political affiliation, regionalism etc, are considered.",No,,,,,"1) The new collected dataset combining 10091 labeled comments.
2) The merge between the new collected dataset and T-HSAB combining 16130 annotated comments.",2,2,different social media platforms,"Toxicity, Hate Speech, Abusiveness",Tunisian,N/A,N/A,N/A,N/A,The authors of T-HSAB have the idea to identify keywords that lead to abusive or hate classification. This identified list has been used in order to target and extract tunisian toxic comments from different social media platforms by injecting queries containing these specific words.,Not discussed,,manual,Yes,"Two native tunisian speakers shared the task of annotating the collected dataset by splitting it by two.

The authors have relied on the simplest and the mostly used annotation scheme which is a 3-way classification: Hate, Abusive and Normal. In addition, due to the fact that the boundaries between hate and offensive comments are quite thin, the authors have added some extra examples to state the differences. For example if the offense is ethnic related, it is directly categorized as hate comment. But if it has an offense against gender especially women, they have to look if there is a sexual connotation in the comment and if the target is an individual or a group to figure out to which category it relates. If it contains vulgar content and directed towards one individual than it is an abusive comment other wise it relates to hate comment. Example: She is a bitch is an abusive comment vs. all women are bitches is a hate comment.

Furthermore, for experimental purposes, they have defined a 2nd layer of annotation related to class hate with sub-categories: Racism, Sexism, Homophobic, Religion and Others. In class others, they have considered hate speech related to political affiliation, regionalism etc.",Not discussed,,1st dataset: 10091 2nd dataset: 16130,80% (~8073; ~12904),20% (~2018; ~3226),Two native tunisian speakers,Yes,"According to the annotation guidelines and definitions, the comments have been labeled.

Annotation Guidelines:

Hate: Explicit or implicit hostile intention that threats or projects violence, targeting a group of individuals based on common specifications like race, ethnicity, gender, religion, political ideologies

Abusive: Instances that contain offensive content, profanity, vulgarity by mentioning private physical part or sexual acts, offensive content or insulting individuals based on physical or mental characteristics using animal analogies, wishing evil

Normal: -",Not discussed,,Specific,"race, gender, sexuality, religion, political, nationality",Non-targeted,N/A,Yes,Yes,"Race, Ethnicity, Gender, Sexuality, Religion, Political ideologies, Other","race, gender, sexuality, religion, political, other",Yes,"Homophobic, political affiliation, regionalism",No,,,No,Human
416,No,"(Pramanik et al., 2021)",Detecting Harmful Memes and Their Targets,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"India, Bulgaria, Qatar","Although memes are typically humorous in nature, recent days have witnessed a proliferation of harmful memes targeted to abuse various social entities. As most harmful memes are highly satirical and abstruse without appropriate contexts, off-the-shelf multimodal models may not be adequate to understand their underlying semantics. In this work, the authors propose two novel problem formulations: detecting harmful memes and the social entities that these harmful memes target. To this end, they present HarMeme, the first benchmark dataset, containing 3, 544 memes related to COVID-19. Each meme went through a rigorous two-stage annotation process. In the first stage, they labeled a meme as very harmful, partially harmful, or harmless; in the second stage, they further annotated the type of target(s) that each harmful meme points to: individual, organization, community, or society/general public/other. The evaluation results using ten unimodal and multimodal models highlight the importance of using multimodal signals for both tasks. They further discuss the limitations of these models and we argue that more research is needed to address these problems.",Yes,16,01.08.2021,14.11.2022,Yes,Yes,"Here, we define harmful memes as follows: multimodal units consisting of an image and a piece of text embedded that has the potential to cause harm to an individual, an organization, a community, or the society more generally. Here, harm includes mental abuse, defamation, psycho-physiological injury, proprietary damage, emotional disturbance, and compensated public image.",Yes,"intensity of the harm: very harmful, partially harmful, harmless target entities: Individual, Organization, Community, Society",Yes,Github,,https://github.com/di-dimitrov/harmeme  link to another repository where the data are available. (https://github.com/di-dimitrov/mmf),Yes,HarMeme dataset,1,1,"Google search, Instagram",Harmful Memes,English,N/A,N/A,N/A,COVID-19,"To collect potentially harmful memes in the context of COVID-19, the authors searched using different services, mainly Google Image Search. They used keywords such as Wuhan Virus Memes, US Election and COVID Memes, COVID Vaccine Memes, Work From Home Memes, Trump Not Wearing Mask Memes. They then used an extension of Google Chrome to download the memes. The authors further scraped various publicly available groups on Instagram for meme collection. (Adhering to the terms of social media, they did not use content from any private/restricted pages.)

As all memes were gathered from real sources, they maintained strict filtering criteria on the resolution of meme images and on the readability of the meme text during the collection process. They ended up collecting 5, 027 memes. However, as they collected memes from independent sources, they had some duplicates. They thus used two efficient de-duplication repositories (gitlab.com/opennota/findimagedupes)(http://github.com/arsenetar/dupeguru) sequentially, and they preserved the memes with the highest resolution from each group of duplicates. They removed 1, 483 duplicate memes, thus ending up with a dataset of 3, 544.

They further used Google’s OCR Vision API to extract the textual content of each meme.",Not discussed,,manual,Yes,"The authors used the PyBossa crowdsourcing framework for annotations. They split the annotators into five groups of three people, and each group annotated a different subset of the data. Each annotator spent about 8.5 minutes on average to annotate one meme. At first, the authors trained the annotators with the definition of harmful memes and their targets, along with the annotation guidelines. To achieve quality annotation, the main focus was to make sure that the annotators were able to understand well what harmful content is and how to differentiate it from humorous, satirical, hateful, and non-harmful content.

Dry run. They conducted a dry run on a subset of 200 memes, which helped the annotators understand well the definitions of harmful memes and targets, as well as to eliminate the uncertainties about the annotation guidelines. For the preliminary data, they computed the inter-annotator agreement in terms of Cohen's k for three randomly chosen annotators for each meme for both tasks.

Final annotation. After the dry run, they started the final annotation process. They asked the annotators to check whether a given meme falls under the four rejection criteria as given in the annotation guidelines. After confirming the validity of the meme, it was rated by three annotators for both tasks.

Consolidation. In the consolidation phase, for high agreements, the authors used majority voting to decide the final label, and they added a fourth annotator otherwise. After the final annotation, Cohen's k increased to 0.695 and 0.797 for the two tasks, which is moderate and high agreement, respectively.",Not discussed,,3544,3013,354,"15 annotators, including professional linguists and researchers in Natural Language Processing (NLP): 10 of them were male and the other 5 were female, and their age ranged between 24-45 years.",Yes,"The authors formally defined four different classes of targets and compiled well-defined guidelines that the annotators adhered to while manually annotating the memes. The four target entities are as follows:

1\. Individual: A person, usually a celebrity (e.g., a well-known politician, an actor, an artist, a scientist, an environmentalist, etc. such as Donald Trump, Joe Biden, Vladimir Putin, Hillary Clinton, Barack Obama, Chuck Norris, Greta Thunberg, Michelle Obama).

2\. Organization: An organization is a group of people with a particular purpose, such as a business, a governmental department, a company, an institution or an association, comprising more than one person, and having a particular purpose, such as research organizations (e.g., WTO, Google) and political organizations (e.g., the Democratic Party).

3\. Community: A community is a social unit with commonalities based on personal, professional, social, cultural, or political attributes such as religious views, country of origin, gender identity, etc. Communities may share a sense of place situated in a given geographical area (e.g., a country, a village, a town, or a neighborhood or in virtual space through communication platforms (e.g., online forums based on religion, country of origin, gender).

4\. Society: When a meme promotes conspiracies or hate crimes, it becomes harmful to the general public, i.e., to the entire society.

During the process of collection and annotation, we rejected memes based on the following four criteria: (i) the meme text is in code-mixed or nonEnglish language(it) the meme text is not readable (e.g., blurry text, incomplete text, etc.)(iii) the meme is unimodal, containing only textual or visual content(iv) the meme contains cartoons (we added this last criterion as cartoons can be hard to analyze by AI systems).

Other details are available in Appendix B, including: What do we mean by harmful memes?Characteristics of harmful memes. What is the difference between organization and community? When do we reject a meme?",Not discussed,,Specific,organization/institution,Non-targeted,N/A,Yes,Yes,Organization/Institution,organization/institution,Yes,"organizations, community",Yes,individual,About a person,No,Human
417,No,"(Kontostathis et al., 2013)",Detecting cyberbullying: query terms and techniques,Proceedings of the 5th Annual ACM Web Science Conference,USA,"In this paper the authors describe a close analysis of the language used in cyberbullying. They take as their corpus a collection of posts from Formspring.me. Formspring.me is a social networking site where users can ask questions of other users. It appeals primarily to teens and young adults and the cyberbullying content on the site is dense; between 7% and 14% of the posts they have analyzed contain cyberbullying content. The results presented in this article are two-fold. The first experiments were designed to develop an understanding of both the specific words that are used by cyberbullies, and the context surrounding these words. They have identified the most commonly used cyberbullying terms, and have developed queries that can be used to detect cyberbullying content. Five of their queries achieve an average precision of 91.25% at rank 100. In their second set of experiments they extended this work by using a supervised machine learning approach for detecting cyberbullying. The machine learning experiments identify additional terms that are consistent with cyberbullying content, and identified an additional querying technique that was able to accurately assign scores to posts from Formspring.me. The posts with the highest scores are shown to have a high density of cyberbullying content.",Yes,168,02.05.2013,14.11.2022,Yes,Yes,"Willard defines cyberbullying as ""willful and repeated harm inflicted through the medium of electronic text."" It takes on many forms, including flaming, trolling, cyberstalking, denigration, harassment, masquerade, flooding, exclusion, and outing.",Yes,requesting additional information apart from binary labels:  On a scale of 1 (mild) to 10 (severe) how bad is the cyberbullying in this post (enter 0 for no cyberbullying)?  What words or phrases in the post(s) are indicative of the cyberbullying (enter n/a for no cyberbullying)?,No,,,,,the dataset,1,1,Formspring.me,Cyberbullying,English,N/A,N/A,N/A,N/A,"To obtain this data, the authors crawled a subset of the Formspring.me site and extracted information from the pages of 18,554 users. The XML files that were created from the crawl ranged in size from 1 post to over 1000 posts. For each user they collected the following profile information: date the page was created, userlD, name, links) to other sites, location, and biography.

The name, links and biography data were manually entered by the user who created the page (the Formspring.me account) so the authors cannot verify the validity of the information in those fields. In addition to the profile information, they collected the following information from each Question/Answer interaction: Asker UserID, Asker Formspring.me page, Question, and Answer.",Not discussed,,manual,Yes,"The authors used Amazon's Mechanical Turk service to determine the labels for the corpus. Mechanical Turk is an online marketplace that allows requestors to post tasks (called HITs) which are then completed by workers. The ""turkers"" are paid by the requestors per HIT completed. The process is anonymous (the requestor cannot identify the workers who answered a particular task unless the worker chooses to reveal him/herself). The amount offered per HIT is typically small. Each HIT we posted displayed a question and its corresponding answer from the Formspring.me crawl and a web form that requested the following information:

1\. Does this post contain cyberbullying (Yes or No)?

2\. On a scale of 1 (mild) to 10 (severe) how bad is the cyberbullying in this post (enter 0 for no cyberbullying)?

3\. What words or phrases in the posts) are indicative of the cyberbullying (enter n/a for no cyberbullying)?

4\. Please enter any additional information you would like to share about this post.

For the first experiments, they asked three workers to label each post because the identification of cyberbullying is a subjective task. The class labels were ""yes"" for a post containing cyberbullying and ""no"" for a post without cyberbullying. The data provided by the other questions will be used for future work. At least two of the three workers had to agree in order for a post to receive a final class label of ""yes"" in the training and testing sets.",Yes,The authors extracted the question text and the answer text from a randomly chosen subset of the Formspring.me data.,10685 posts,"13,652 (labelled)","10,482 (unjudged)",“Turkers” on Amazon’s Mechanical Turk,Not discussed,,Yes,They paid three unique workers 5 cents to label each post.,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
418,No,"(Sazzed, 2021)",Identifying vulgarity in Bengali social media textual content,PeerJ Computer Science,USA,"In this paper, the authors provide the first comprehensive analysis on the presence of vulgarity in Bengali social media content. They develop two benchmark corpora consisting of 7,245 reviews collected from YouTube and manually annotate them into vulgar and non-vulgar categories. The manual annotation reveals the ubiquity of vulgar and swear words in Bengali social media content (i.e., in two corpora), ranging from 20% to 34%. To automatically identify vulgarity, they employ various approaches, such as classical machine learning (CML) classifiers, Stochastic Gradient Descent (SGD) optimizer, a deep learning (DL) based architecture, and lexicon-based methods. Although small in size, they find that the swear/vulgar lexicon is effective at identifying the vulgar language due to the high presence of some swear terms in Bengali social media. They observe that the performances of machine leanings (ML) classifiers are affected by the class distribution of the dataset. The DL-based BiLSTM (Bidirectional Long Short Term Memory) model yields the highest recall scores for identifying vulgarity in both datasets (i.e., in both original and class-balanced settings). Besides, the analysis reveals that vulgarity is highly correlated with negative sentiment in social media comments.",Yes,6,19.10.2021,14.11.2022,Yes,Yes,"Vulgarity or obscenity indicates the use of curse, swear or taboo words in language. Eder, Krieg-Holz & Hahn (2019) conceived vulgar language as an overly lowered language with disgusting and obscene lexicalizations generally banned from any type of civilized discourse. Primarily, it involves the lexical fields of sexuality, such as sexual organs and activities, body orifices, or other specific body parts. Cachola et al. (2018) defined vulgarity as the use of swear/curse words. Jay Eu Janschewitz (2008) mentioned vulgar speech includes explicit and crude sexual references. Although the terms obscenity, swearing, and vulgarity have subtle differences in their meaning and scope, they are closely linked with some overlapping definitions. Thus, in this paper, we use them interchangeably to refer to the text that falls into the above-mentioned definitions.",No,,Yes,Github,,https://github.com/sazzadcsedu/Bangla-vulgar-corpus,Yes,"Bangla vulgar corpus: Drama review, Subject-person",2,2,YouTube,"Vulgarity, Profanaity, Obscenity",Bengali,N/A,N/A,N/A,N/A,"The authors create two vulgar datasets consisting of 7,245 Bengali comments. Both datasets are constructed by collecting comments from YouTube (https://www.youtube.com/), a popular social media platform.

Drama review dataset

The first corpus they utilize is a drama review corpus. This corpus was created and deposited by Sazzed (2020a) for sentiment analysisIt consists of 8,500 positive and 3,307 negative reviews. However, there is no distinction between different types of negative reviews. Therefore, they manually annotate these 3,307 negative reviews into two categoriesone category contains reviews that convey vulgarity, while the other category consists of negative but non-vulgar reviews.

Subiect-person dataset

The second corpus is also developed from YouTube. However, unlike the drama review corpus that represents the viewer's feedback regarding dramas, this corpus consists of comments towards a few controversial female celebrities.

They employ a web scraping tool to download the comment data from YouTube, which comes in JSON format. Employing a parsing script, they retrieve the comments from the JSON data. Utilizing a language detection library (https://github.com/Mimino666/ langdetect), they recognize the comments written in Bengali. We exclude reviews written in English and Romanized Bengali (i.e., Bengali language in the Latin script).",Not discussed,,manual,Yes,"The labeling is performed by three annotators (A1, A2, A3), who are Bengali native speakers. The first two annotators (A1 and A2) initially annotate all the reviews. Any disagreement in the annotation is resolved by the third annotator (A3).",Not discussed,,Drama: 3307 Subject-person: 3938,10-fold cross-validation,10-fold cross-validation,"Three Bengali native speakers; Among them, two are male and one female (A1: male, A2: female, A3: male).",Yes,"Here, to distinguish the comments into vulgar and non-vulgar classes, annotators are asked to consider the followings guidelines-

• Vulgar comment: The presence of swearing, obscene language, vulgar slang, slurs, sexual and pornographic terms in a comment (Eder, Krieg-Holz & Hahn, 2019Cachola et al., 2018Jay &~ Janschewitz, 2008).

• Non-vulgar comment: The comments which do not have the above mentioned characteristics.",Not discussed,,Specific,gender,Targeted,"sexuality, body",Yes,No,,,No,,Yes,controversial female celebrities,To a person,No,Human
419,No,"(Bashar et al., 2020)",Regularising LSTM classifier by transfer learning for detecting misogynistic tweets with small training set,Knowledge and Information Systems,Australia,"It is not always easy to build a large (labelled) dataset. For example, due to the complex nature of tweets and the manual labour involved, it is hard to create a large Twitter data set with the misogynistic label. In this paper, the authors propose to regularise a long short-term memory (LSTM) classifier using a pretrained LSTM-based language model (LM) to build an accurate classification model with a small training set. They explain transfer learning (TL) with a Bayesian interpretation and show that TL can be viewed as an uncertainty regularisation technique in Bayesian inference. They show that a LM pre-trained on a sequence of general to task-specific domain datasets can be used to regularise a LSTM classifier effectively when a small training dataset is available. Empirical analysis with two small Twitter datasets reveals that an LSTM model trained in this way can outperform the state-of-the-art classification models.",Yes,19,18.06.2020,14.11.2022,Yes,Yes,"In this paper, we focus on distinguishing tweets that contain misogynistic words as abusive and non-abusive and identifying misogynistic tweets that are abusive towards an individual or a group. They are a subset of the larger category of tweets that include sexist words or concepts. Not all tweets that contain misogynistic keywords are abusive; these words have been commonly used in non-abusive tweets too. Accordingly, we address the difficult task of separating abusive tweets from the tweets that are sarcastic, joking, or contained misogynistic keywords in non-abusive contexts.",No,,No,,,,,"QMI ( QUT Misogyny Identification ) Dataset for misogynistic tweet detection;

AMI ( automatic misogyny identification) dataset for misogynistic tweet detection;

Pretraining datasets for NNLM: (D1)Wikitext-103, (D2)movie review dataset IMDb, (D3)a dataset of about 1.9 million random tweets",5,1,"Twitter, Wikipedia, IMDb",Misogyny,English,N/A,XX.01.2007 - XX.03.2018,Australia,N/A,"QMI Dataset for misogynistic tweet detection

The authors collected tweets using the Twitter's streaming API. A set of tweets that contain any of the three main misogynistic keywords (i.e. whore, slut, rape) was collected by a random sampling during the period of January 2007-March 2018 in Australian domain. After removing tweets that contain a lot of non-English words, a total of 5000 tweets was obtained.

AMI dataset for misogynistic tweet detection

To see how effectively the classification models work on other misogyny detection datasets, they have also used the automatic misogyny identification (AMI) English dataset used in recent competitions. The training and testing sets contain 4000 and 1000 labelled tweets, respectively. The training set has 1785 (45%) misogynous tweets and 2215 (55%) non-misogynous tweets, while the testing set has 460 (46%) misogynous tweets and 540 (54%) non-misogynous tweets.

Pretraining datasets for NNLM

D1: The goal of using this corpus is to capture general properties of the English language. They pretrain the NNML model on Wikitext-103 that contains 28,595 preprocessed Wikipedia articles and 103 million words.

D2: The goal of using this corpus is to bridge the data distribution between the target task domain (i.e. abusive tweets) and the general domain (i.e. standard language). This is because the target task is likely to come from a different distribution than the general corpus. D2, should be chosen such that it has commonalities with both D1 reflecting a general domain (Wikipedia) and the corpus D3, reflecting a target domain (tweets). They use 25k reviews from movie review dataset IMDb without labels as D2,. Documents in IMDb are generally a few paragraphs long expressed by people. Similar to Wikipedia, they discuss topics, stories or personnel with some details and they are noisy. Similar to tweets, they are written by general people and not peer reviewed. More importantly, both IMDb reviews and tweets often express personal opinions and sarcasms.

D3: The goal of this corpus is to capture the domain specific language properties of the target task. The authors tune NNML on a dataset of about 1.9 million random tweets.",Not discussed,,manual,Not discussed,,Not discussed,,QMI: 5000 AMI: 5000;,QMI: 4000 AMI: 4000;,QMI: 1000;,N/A,Yes,"Following the misogynistic tweet definition, they used a systematic approach to generate the labelled data manually. The following contextual information is checked to label a tweet as targeted misogynistic abuse: (a) Is a specific person or group being targeted in this tweet? (b) Does this tweet contain a specific threat or wish for violence? (c) Does this tweet encourage or promote self-harm or suicide? (d) Is the tweet harassing a specific person, or inciting others to harass a specific person? (e) Does the tweet use misogynistic language in objectifying a person, making sexual advances, or sending sexually explicit material? (f) Is the tweet promoting hateful conduct by gender, sexual orientation, etc.?",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,gender,gender,Yes,misogyny,No,,,No,Human
420,No,"(Saha et al., 2021)",“Short is the Road that Leads from Fear to Hate”: Fear Speech in Indian WhatsApp Groups,IW3C2 (International World Wide Web Conference Committee),"India, USA","In this paper, the authors perform the first large scale study on fear speech across thousands of public WhatsApp groups discussing politics in India. They curate a new dataset and try to characterize fear speech from this dataset. They observe that users writing fear speech messages use various events and symbols to create the illusion of fear among the reader about a target community. They build models to classify fear speech and observe that current state-of-the-art NLP models do not perform well at this task. Fear speech messages tend to spread faster and could potentially go undetected by classifiers built to detect traditional toxic speech due to their low toxic nature. Finally, using a novel methodology to target users with Facebook ads, they conduct a survey among the users of these WhatsApp groups to understand the types of users who consume and share fear speech. They believe that this work opens up new research questions that are very different from tackling hate speech which the research community has been traditionally involved in. They have made our code and dataset public for other researchers.",Yes,21,19.04.2021,14.11.2022,Yes,Yes,"According to Buyse, …'fear speech', which is defined as ""an expression aimed at instilling (existential) fear of a target (ethnic or religious) group.",No,,Yes,Github,,https://github.com/hate-alert/Fear-speech-analysis,Yes,Fear Speech dataset,1,1,WhatsApp,Fear Speech,"English, Hindi",XX.08.2018 - XX.08.2019,XX.08.2018 - XX.08.2019,N/A,"This period includes high profile events in India, including the national elections and a major terrorist attack on Indian soldiers","In this paper, the authors use the data collected from public WhatsApp groups from India discussing politics which usually have a huge interplay with religion. In order to obtain a list of such public WhatsApp groups they resorted to lists publicized on well-known websites or social media platforms such as Facebook groups. Due to the popularity of WhatsApp in India, political parties massively create and advertise such groups to spread their party message. These groups typically contain activists and party supporters, and hence typically act as echo chambers of information. Surveys show that one in six users of WhatsApp in India are a member of one of such public groups.

They used the data collection tools developed by Garimella and Tyson to gather the WhatsApp data. With help from journalists who cover politics, they curated lists of keywords related to politicians and political parties. Using this list they looked up public WhatsApp groups on Google, Facebook and Twitter using the query ""chat.whatsapp.com + query"", where query is the name of the politician or political party. The keyword lists cover all major political parties and politicians all across India in multiple languages. Using this process, they joined and monitored over 5,000 political groups discussing politics. From these groups, they obtained all the text messages, images, video and audio shared in the groups. The data collection spans for around 1 year, from August 2018 to August 2019. This period includes high profile events in India, including the national elections and a major terrorist attack on Indian soldiers. The raw dataset contained 2.7 million text messages.",Yes,All personally identifiable information was anonymized and stored separately from the message data.,manual,Yes,"Annotation training

The annotation process was led by two PhD students as expert annotators and performed by seven under-graduate students. In order to train the annotators the authors needed a gold-label dataset. For this purpose the expert annotators annotated a set of 500 posts using the annotation guidelines. This set was selected by using the threshold of having at least one keyword from the Muslim lexicon. Later the expert annotators discussed the annotations and resolved the differences to create a gold set of 500 annotations. This initial set had 169 fear speech and 331 non fear speech post. From this set they sampled a random set of 80 posts initially for training the annotators. This set contained both the classes in equal numbers. After, the annotators finished this set of annotations they discussed the incorrect annotations in their set with them. This exercise further trained the annotators and fine-tuned the annotation guidelines. To check the effect of the first round of training, they sampled another set of 40 examples each from both classes again from the set of 500 samples. In the second round, most of the annotators could correctly annotate at least 70% of the fear speech cases. The novice annotators were further explained about the mistakes in their annotations.

Main annotation

After the training process, they proceeded to the main annotation task by sampling posts having at least one keyword from the Muslim lexicon and gave them to the annotators in batches. For this annotation task they used the open source platform Docanno, which was deployed on a Heroku instance. Each annotator was given a secure account where they could annotate and save their progress. Each post was annotated by three independent annotators. They were instructed to read the full message and based on the guidelines provided, select the appropriate category (either fear speech or not). They initially started with smaller batches of 100 posts and later increased it to 500 posts as the annotators became well-versed with the task. They tried to maintain the annotators agreement by sharing few of the errors in the previous batch. Since fear speech is highly polarizing and negative in nature the annotators were given ample time to do the annotations.

The annotators were advised to take regular breaks and not do the annotations in one sitting. Finally, the authors also had regular meetings with them to ensure the annotations did not have any effect on their mental health.",Yes,The authors filtered the dataset for messages containing the keywords from the lexicon they generated for identifying messages related to Muslims. The annotation process enumerates the steps taken to sample and annotate fear speech against Muslims in the dataset.,"4,782 unique messages",5-fold cross-validation,5-fold cross-validation,"The annotation process was led by two PhD students as expert annotators and performed by seven under-graduate students who were novice annotators. All the undergraduate students study computer science, and were voluntarily recruited through a departmental email list and compensated through an online gift card. Both the expert annotators had experience in working with harmful content in social media.",Yes,"They follow the fear speech definition by Buyse et. al for the annotation process. In their work, fear speech is defined ""as a form of expression aimed at instilling (existential) fear of a target (ethnic or religious) group"". For this study, the authors considered the Muslims as the target group. In order to help and guide the annotators, the authors provide several examples highlighting different forms where they might find fear speech. These forms include but are not limited to (a) fear induced by using examples of past events, e.g., demolition of a temple by a Mughal ruler, (b) fear induced by referring to present events, e.g., Muslims increasing their population at an increasing rate. (c) fear induced by cultural references, e.g., verses from the Quran, interpreted in a wrong way. (d) fear induced by speculation of dominance by the target group, e.g., members of the Muslim community occupying top positions in government institutions and the exploitation of Hindus.

The detailed flowchart used for the annotation process is available in the paper (Figure 1):

Are any of the targets religious ? (Yes) --Focus on the parts having Muslims as  targets. --Are the Muslims shown in a negative connotation (for eg. oppressors in the past or terrorist) (Yes) --Does this induce a fear against the Muslim community? (Yes) --The text is FEAR SPEECH

Note that the annotation guidelines are strict and focused on a high precision annotation. Further, the asked the annotators to annotate a post as fear speech even if only a part of the post appear to induce fear. This was done because many of the posts were long and contained non fear speech aspects as well.",Yes,All the undergraduate students were voluntarily recruited and compensated through an online gift card.,Specific,religion,Targeted,"race, religion",Yes,Yes,Religion,religion,Yes,Muslims,No,,,No,Human
421,No,"(Abdhullah-Al-Mamun & Akhter, 2018)",Social media bullying detection using machine learning on Bangla text,2018 10th International Conference on Electrical and Computer Engineering (ICECE),Bangladesh,"Very few works have been done on Bangla text for social media activity monitoring due to a lack of a large number of annotated corpora, named dictionaries and morphological analyzer, which demands in-depth analysis on Bangladesh's perspective. Moreover, solving the issue by applying available techniques is very content specific, which means that false detection can occur if contents changed from formal English to verbal abuse or sarcasm. Also, performance may vary due to linguistic differences between English and non-English contents and the socio-emotional behaviour of the study population. To combat such issues, this paper proposes the use of machine learning algorithms and the inclusion of user information for cyber bullying detection on Bangla text. For this purpose, a set of Bangla text has been collected from available social media platforms and labelled as either bullied or not bullied for training different machine learning based classification models. Cross-validation results of the models indicate that a support vector machine based algorithm achieves superior performance on Bangla text with a detection accuracy of 97%. Besides, the impact of user specific information such as location, age and gender can further improve the classification accuracy of Bangla cyber bullying detection system.",Yes,24,20.12.2018,14.11.2022,Yes,Yes,Cyber bullying is when someone uses technology to send threatening or embarrassing messages to another person.,No,,No,,,,,The dataset,1,1,"Facebook, Twitter",Cyberbullying,Bengali,N/A,N/A,N/A,N/A,"A java program was developed for extracting social media data. Facebook and Twitter sites were considered for Bangla text content collection. Using Facebook Graph API, a total of 1000 contents were collected including Bangla public status, emotion icons and user related information. Besides, 1400 Bangla public status, user's demographic and geographic data were fetched from Twitter using Twitter REST API.",Not discussed,,manual,Not discussed,,Not discussed,,2400,10-Fold cross-validation,10-Fold cross-validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
425,No,"(Sodhi et al., 2021)",Jibes & Delights: A Dataset of Targeted Insults and Compliments to Tackle Online Abuse,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),India,"In this paper, the authors contribute a Reddit-based dataset, consisting of 68,159 insults and 51,102 compliments targeted at individuals instead of targeting a particular community or race. Secondly, they benchmark multiple existing state-of-the-art models for both classification and unsupervised style transfer on the dataset. Finally, they analyse the experimental results and conclude that the transfer task is challenging, requiring the models to understand the high degree of creativity exhibited in the data.",Yes,2,06.08.2021,16.11.2022,Yes,Yes,"Online abuse, insults.

It is essential to distinguish between insults and slurs since the two are frequently clubbed together. A slur is a taboo remark that is usually used to deprecate, disparage or derogate a targeted member of a select category, such as ethnicity, race, or sexual orientation. Insults can consist of slurs, but they are much broader, and a more diversified phenomenon (Dynel and Poppi, 2019).",No,,Yes,Github,,https://github.com/ravsodhi/jibes-and-delights,Yes,Jibe and Delight Corpus (JDC),1,1,Reddit,"Abusiveness, Insults",English,N/A,N/A,N/A,N/A,"The authors use Pushshift (Baumgartner et al., 2020) to extract Reddit posts and comments. While r/RoastMe is often characterized as a humorous subreddit, where users can voluntarily submit pictures of themselves to be ""roasted"" or insulted, internet users who are not familiar with the community can associate r/RoastMe with malicious activities including cyberbullying (Jenaro et al., 2018). r/RoastMe has even been described as ""a new cyberbullying trend"" by news media. The ""roasters"" will come up with comments that insult or demean the poster of the picture while trying to be as ""creative"" as possible. r/ToastMe and r/FreeCompliments work similarly, but with the opposite intent. These communities are much smaller and less popular than r/RoastMe, and hence, the number of insults in the dataset is greater than the number of compliments.",Not discussed,,"automated, manual",Not discussed,,Yes,"The authors limit the JDC to comments having the following characteristics:

• They are top-level comments. While nested comments may also sometimes contain relevant data, they often diverge from the topic and begin a conversation with the parent comment, which may add noise to the dataset.

• They have a Reddit karma score of at least 3. Reddit karma score is defined to be the number of upvotes minus the number of downvotes. This filtering helps in weeding out spam or irrelevant comments which are not relevant to the topic. Generally, users downvote comments which they find unfunny or off-topic. Thus, they utilize crowdsourced user scores to ensure quality sentences.

This filtering yields a corpus of roughly 300, 000 insult comments and 100, 000 compliment comments.

They also utilize several other filters to limit the dataset to a particular type of insult or compliment. They manually create an attribute list(https://github.com/ravsodhi/jibes-and-delights/blob/main/attr-list.txt) consisting of words which correspond to a physical attribute (for example, hair, skin, complexion, teeth, and eyes) or a trait (for example, personality, kindness, and appearance) and keep the insults containing keywords for such attributes. They use the NLTK and spaCy  libraries for preprocessing and filtering. We tokenize each comment into sentences, and check if the lemma of any word in a sentence matches a word in the attribute list. This process helps them keep relevant sentences, especially from longer comments, which would be otherwise discarded. They also filter out very short sentences (containing only one or two words). They obtain a corpus of around 68, 159 insult sentences and 51, 102 compliment sentences. Finally, they take 1,000 instances each from both categories for evaluation purposes.","68, 159 insult + 51, 102 compliments",~67k insults + 50k compliments,1000 insults + 1000 compliments,N/A,Not discussed,,Not discussed,,Specific,body,Targeted,"race, sexuality",No,No,,,No,,Yes,"targeting a person, especially their physical attributes",To a person,No,Human
427,No,"(Sharma et al., 2018)",Degree based Classification of Harmful Speech using Twitter Data,"Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)",India,This paper primarily describes how the authors created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new dataset of tweets they created based on ontological classes and degrees of harmful speech found in the text. They also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it's automatic detection more robust and efficient.,Yes,49,25.08.2018,16.11.2022,Yes,Yes,"Hate Speech. Karin Sternberg's theory of hate observed hate as an emotion, a feelingfrom an erudite perspective which inspired to form extended nominal categories of hatred. The harmful speech can be molded into a spectrum showing gradient of hate and harm, with some distinguishing characteristics to particular classes and their degrees. The classes have been defined on their decreasing degree of hateful intent from the speaker's perspective, with three classes (Class I, Class II and Class III) showcasing various categories of different types and examples of harmful speech found in social media (like extremism, threatening someone or trolling).",Yes,"degree based classes of harmful speech: class I, class II, class III",No,,,,,The dataset,1,1,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"The authors constructed the corpus using the tweets posted online from Twitter. They had mined tweets with querying for profane slang words and harmful words that they compiled from searching synonyms and various parts of speech extensions of common words that can be used in hateful context. They scraped tweets with hashtags focused on three groups who are often the target of abuse: African Americans (black people, overweight people and women. Some examples of keywords and hashtags handled are : #IfMyDaughterBroughtHomeABlack, Nigger(s), White Trash, #IfIWereANazi or the tweets in response to #MakeAMovieAFatty. Certain hashtags and keywords from recent events surrounding politics, public protests, riots, etc., which have a good propensity for the presence of harmful speech were also used. Certain example of above case can be attributed to the #GamerGate fiasco, where trolls and haters decided to occupy and corrupt the #TakeBackTheTech and #ImagineAFeministInternet hashtags by posting thousands of anti-feminist and misogynistic tweets and memes. They also used resources from Hatebase.org to narrow down slangs and hate speech used against the group who are a target of abuse. They retrieved a total of 15,438 tweets from Twitter in json format, which consists of information such as timestamp, URL, text, user, re-tweets, replies, full name, id and likes.",Not discussed,,manual,Yes,"Harmful Speech or Normal Speech: The initial task in hand was to annotate each tweet with one of the two tags (Harmful Speech or Normal Speech). Harmful speech was detected in 9064 tweets. Remaining 5842 tweets in the dataset comprised of normal speech, having no context of intent of harm at all, and were of no use for the experiment.

The annotators were provided with a definition along with a detailed guidelines of all the classes and respective categories and examples. Annotators were asked to think about the contextual implications of a tweet, more importantly from the speaker's intent perspective, rather than lexical based judgment of the text, as a syntactic extension of harmful words can not necessarily indicate a tweet inciting hate. It could very well be, plainly stating facts and truth with no intention of hurting sentiments of the recipients. For borderline cases and overlapping examplessub categories inside the classes (propaganda, enmity, sarcasm etc) their definitions were used, along with measuring relative degree of hateful intent and context, to classify the tweet into a respective class (I, II and III). Annotation of the corpus was carried out as follows:

Categories of Harmful Speech: All the 9064 harmful tweets were then manually annotated according to the guidelines, for classes of harmful speech (Class I, Class II and Class III). The dataset then consisted of 2138 Class I, 3924 Class II and 3002 Class III harmful tweets, after successful annotation. The annotated dataset (consisting of tweet ids and respective tag of harmful class) with the classification system will be made available online later.

In order to validate the quality of annotation, subsequent iterations of annotation was carried out by, in total, 2 human annotators. They calculated the inter-annotator agreement between the two iterations of annotation using Cohens Kappa coefficient.",Yes,"A random sampling of the tweets containing respective hashtags and an extensive processing was carried out to remove same tweets (certain reposts of tweets). As a result of manual filtering, a dataset of 14,906 tweets was created.",9064,10-fold cross-validation,10-fold cross-validation,"There were 3 annotators selected, who were very well versed with english language as well had a fair amount of experience with witnessing harmful speech online that we deal with on a day-to-day basis on social media.",Yes,"Some of the guidelines used to distinguish the classes are given below. Annotators were asked to keep them in mind along with common sense to classify the respective tweets.

Class I:

• Incites violent actions beyond the speech itself.

• Is either public or directed at a particular group, mostly with no redeeming purpose. We considered hatred and violent behavior projected to a group to be of more degree than individual accusation and violence. This is an assumption made from the psychological point of view and by seeing various examples that adhere to it.

• The context makes it evident that the speaker wants to intend hurting sentiments of certain isms (extremism) for a violent response to be possible in return.

Class II:

• Cyber banter (accusing, threatening and using aggressive/provocative language for disagreeing etc.) and verbal dueling constitutes.

• The violent characteristic is less than the degree in which Class I operates, which hurts sentiments but not to the degree to invoke a violent response.

• Correlates between linguistic violence and non-linguistic/demographic intimidating and trespassing someone in an online space. Can be highly provocative when addressing an individual rather than some ideology or community/group.

Class III:

• Mildly provocative in nature, mostly given to an individual entity, not necessarily targeting a group or community.

• Uses more profane and filthy words not directed or having context from the speaker to the recipient to form a coherent remark. Context mainly revolves around trolling, ironic and sarcastic tone.

• Indirect or covert linguistically hurt sentiments, least degree of hateful intent shown in the categories.",Not discussed,,Specific,"race, body, gender",Non-targeted,N/A,No,Yes,"Race, Appearance/health, gender","race, body, gender",Yes,"African Americans, overweight people, women",No,,,No,Human
428,No,"(Gaikwad et al., 2021)",Multi-Ideology ISIS/Jihadist White Supremacist (MIWS) Dataset for Multi-Class Extremism Text Classification,Data,India,"The unavailability of extremism text datasets is a challenge in online extremism research. The lack of emphasis on classifying extremism text into propaganda, radicalization, and recruitment classes is a challenge. The lack of data validation methods also challenges the accuracy of extremism detection. This research addresses these challenges and presents a seed dataset with a multi-ideology and multi-class extremism text dataset. This research presents the construction of a multi-ideology ISIS/Jihadist White supremacist (MIWS) dataset with recent tweets collected from Twitter. The presented dataset can be employed effectively and importantly to classify extremist text into popular types like propaganda, radicalization, and recruitment. Additionally, the seed dataset is statistically validated with a coherence score of Latent Dirichlet Allocation (LDA) and word mover’s distance using a pretrained Google News vector. The dataset shows effectiveness in its construction with good coherence scores within a topic and appropriate distance measures between topics. This dataset is the first publicly accessible multi-ideology, multi-class extremism text dataset to reinforce research on extremism text detection on social media platforms.",Yes,2,15.11.2021,16.11.2022,Yes,Yes,"Extremism. ""Researchers have classified the extremist text on the social media into major types based on the objectives of social, political, or religious nature.""",Yes,"2 ideology collection: ISIS/Jihadist, white supremacist  3 class labels: propaganda, radicalization, or recruitment",Yes,Other,Zenodo,https://zenodo.org/record/5687447#.Y3500C-B3x4,Yes,Seed dataset; Multi-ideology ISIS/Jihadist White Supremacist (MIWS) Dataset,2,2,"Research Articles, Newspapers, Blogs, Twitter","Extremism, White Supremacy",English,N/A,Seed dataset: Research articles and reports: January 2015 to December 2020 MIWS dataset: June 2021–August 2021,N/A,N/A,"For data collection of a seed dataset, the authors collected research articles from existing literature, examples from extremist identification websites, and blogs recognizing influential propagandists, radicals, and extremist recruiters. The seed dataset is collected based on ISIS/ Jihadist and White supremacist ideologies. The primary objective of the seed dataset is to collect text examples of propaganda, radicalization, and recruitment. Multiple newspaper articles, journal papers, book chapters, and websites are selected. Proper sources for text examples of propaganda, radicalization, and recruitment are selected using a snowballing technique. (see Table 3 for information of each source, its corresponding type, text example, label, geographical location, and ideology)

To collect relevant tweets and metadata for MIWS datasets, the authors constructed different search queries with different keywords. They used popular keywords that are associated with extremist ideologies like ""munafig"", ""kuffar"", ""white genocide"", and ""anti-white"". These keywords are referenced from several previous works. They also used some new keywords like ""kufr army"", ""wesupporttaliban"", ""talibanourguardians"", ""globalists"", ""zog"" etc., to collect recent tweets. The geographical locations were found by manually searching locations from collected tweets. If no locations were present in the tweet, it was labeled as ""undefined"". Due to the Twitter data sharing policy, only Tweet _ID, Created _At, and Geo_Enabled can be shared publicly.",Not discussed,,"automated, manual",Yes,"Summary: Seed dataset consisting of 400 examples collected from diverse sources and manually annotated with class labels as propaganda, radicalization, or recruitment. MIWS dataset is constructed and automatically annotated using seed dataset into propaganda, radicalization, and recruitment.

In seed dataset, LABEL denotes whether the text is propaganda, radicalization, or recruitment as mentioned by the source. (details are available in Table 3)

Construction of MIWS Using Seed Dataset:

LDA: To extract topics, the Latent Dirichlet Allocation method is used. To confirm the best possible topics, GridSearchCV is used. Hyperparameter tuning is performed to select optimal parameters for the best model.

Comparison between Seed Labels and Topics of Collected Tweets: It is required to compare topics based on the ideology. Labeled topics from the ISIS/Jihadist seed and White supremacist seed are compared with topics from ISIS/Jihadist and White supremacists collected tweets. This was done to maintain uniformity and accuracy across ideologies.

Word mover's distance (WMD) is used to compare collected tweets and seed labels. WMD presents semantically meaningful comparisons of words from local co-occurrences in sentences. Thus, the lower the distance the more the similarity among sentences. To leverage WMD's properties, the Word2Vec vector pretrained on Google News is used. The lower the distance between the topic and labels the more similar they are than the others. Thus, the corresponding label is given to that topic.

(see Figure 2. Process flow for construction of seed and MIWS datasets.)",Not discussed,,"Seed dataset: 400 MIWS dataset: 40,000",N/A,N/A,N/A,Not discussed,,Not discussed,,Specific,"race, religion, political, other",Targeted,"political, religion",No,Yes,"Race/Ethnicity, Religion, Ideology, etc (white supremacist, ISIS/Jihadist)","race, religion, political, other",No,,No,,,No,Human
429,No,"(Mussiraliyeva et al., 2021)",Applying Machine Learning Techniques for Religious Extremism Detection on Online User Contents,"Computers, Materials & Continua","Kazakhstan, UK","In this research paper, the authors propose a corpus for the task of detecting religious extremism in social networks and open sources and compare various machine learning algorithms for the binary classification problem using a previously created corpus, thereby checking whether it is possible to detect extremist messages in the Kazakh language. To do this, the authors trained models using six classic machine-learning algorithms such as Support Vector Machine, Decision Tree, Random Forest, K Nearest Neighbors, Naive Bayes, and Logistic Regression. To increase the accuracy of detecting extremist texts, they used various characteristics such as Statistical Features, TF-IDF, POS, LIWC, and applied oversampling and undersampling techniques to handle imbalanced data. As a result, they achieved 98% accuracy in detecting religious extremism in Kazakh texts for the collected dataset. Testing the developed machine learning models in various databases that are often found in everyday life “Jokes”, “News”, “Toxic content”, “Spam”, “Advertising” has also shown high rates of extremism detection.",Yes,1,07.09.2021,16.11.2022,Yes,Yes,"In this context, the following definition of violent extremism is accepted as ""encouraging, condoning, justifying or supporting the Commission of a violent act to achieve political, ideological, religious, social or economic goals"". In comparison, the term radicalism is defined as ""the process of developing extremist ideologies and beliefs"". On the other hand, Islamist radicalism is defined as ""a militant methodology practiced by Sunni Salafi Islamists who seek the immediate overthrow of existing regimes and the non-Muslim geopolitical forces that support them in order paving the way for an Islamist society that will develop through military force"".",No,,Yes,Other,Mendeley Data,https://data.mendeley.com/datasets/h272z7xv9w/1,Yes,Religious extremism and neutral text corpus,1,1,Vkontakte,Religious Extremism,Kazakh,N/A,N/A,N/A,N/A,"To collect data, the authors use the Vkontake social network. They use Python 3.6 to create a parser for data collection. Interaction with the social network API was performed using the requests library. The Pycharm Community Edition 2018 software was chosen as the development environment. To get the data, they use The VK API, a ready-made interface that allows getting the necessary information from the Vkontakte social network database using HTTPS requests.

All methods in the system are divided into sections. In the transmitted request, you must pass the input data as getting parameters in the HTTP request after the method name. If the request is successfully processed, the server returns a JSON object with the requested data. The response structure for each method is strictly defined. The rules are specified on the pages describing the method in the official documentation.

Keyword Search: Module What does ""keywords confirming the possibility of defining a post as extremist"" mean? There is a certain set of words that are often used by people who have decided to commit extremism or to call for extremism. In General, these words are directly related to the idea of life and death. Still, sometimes, in posts written by people who call for extremism, they try to avoid using words that directly mean their attempt at extremism. But they try to use synonyms for these same words, allowing the authors to find their posts using more and more new sets of keywords. Keywords associated with extremism were identified from the previous topic. For example, kafir, kill, blow up, end, etc. These keywords will help you search for extremist posts on social networks. As you find extremist posts, the keyword database will be updated, thereby providing a more accurate definition of extremist posts.",Not discussed,,manual,Not discussed,,Not discussed,,N/A,N/A,N/A,Psychologists,Yes,"The annotation rules and examples of posts are available in Table 2:

Extremist text (i) Expressing extremist thoughts  (ii) Including potential extremist actions

Nonextremist text (i) Formally discussing extremism (ii) Referring to other's extremism (iii) Not relevant to extremism",Not discussed,,Specific,religion,Targeted,"political, religion, class",Yes,Yes,Religion (religious extremism),religion,No,,No,,,No,Human
430,No,"(Komalova et al., 2022)",Automated Classification of Potentially Insulting Speech Acts on Social Network Sites,Digital Transformation and Global Society,Russia,"The present research article explores insulting speech acts on the social network site “VKontakte” aiming to develop an algorithm for automatic classification of text data. The authors conducted semantic analysis of the text of “Article 5.61” of the Code of Administrative Offenses of the Russian Federation, which made it possible to formulate inclusion criteria for formal classification. They used three common word embeddings models (BERT, ELMo, and fastText) on the original Russian language dataset consisting of 4596 annotated messages perceived as insulting speech acts. General findings argue that even in a specialized dataset the share of messages that meet criteria of inclusion is negligible. This indicates a low probability of going to court on the fact of an administrative offense under Article 5.61 based on speech communication on social network sites, even though such communication is public in nature and is automatically recorded in writing. Machine learning text classifier based on BERT model showed best performance.",Yes,4,25.01.2022,16.11.2022,Yes,Yes,"Insulting as a speech act of a conflict type ""is performed by expressing speaker's negative opinion of the addressee in a disrespectful way, with the intention to humiliate and hurt him/her"".

The notion of insulting speech describes a perceptual construct that represents the emotional-psychological state of a person experiencing negative emotions as a result of attribution of insulting intention to someone's actions, behavior, and speech. The use of this term turns our attention to a sender and a recipient, widening the traditional research scope to analysis of a communicative situation as a whole instead of focusing only on a speech product. Within these boundaries insulting speech applies to cases of hate speech, hard criticism, verbal abuse, offensive language, hate crime language, verbal crime and so on as they could be perceived as insulting.",Yes,"labels: decent – obscene, person – impersonal, moral – immoral.",Yes,Other,GitLab,https://gitlab.com/rostepifanov/mca-workshop,Yes,The dataset,1,1,VKontakte,Insult,Russian,XX.07.2020,N/A,N/A,N/A,"The social network site ""VKontakte"" was chosen as a source of text information extraction. To select publics (open thematic communities on Vkontakte), the authors turned to the VK API tool of VKontakte interface, which allowed them to collect in an aggregated form a sufficient number of messages with publicly available information about their authors (given names, surnames and genders). They selected 13 large VK-communities in which potentially toxic topics were discussed (foreign policy. feminism. rap. etc.) and there was no obscene lexemes moderation in the comments. For each of them, the authors automatically collected comments to the last (at the time of collection) 50 posts (each post could consist of more than two utterances/messages). The collection was held in July 2020. As a result, 4610 messages were selected, 14 of them were excluded as they did not consist any text information.",Not discussed,,manual,Yes,"The resulting data array of 4596 messages was annotated by a group of annotators (n = 5) using ipywidgets based interface. Two specialists (authors of this paper) validated their annotation and after that an expert in forensic linguistics examined the selected data.

A VKontakte post (text message) was classified as insulting speech act if it met the three criteria (see inclusion criteria / guidelines). To indicate compliance or non-compliance with the criterion, labels were used while annotating: decent - obscene, person - impersonal, moral - immoral.",Not discussed,,4596,3227 (80%),1383 (20%),Five annotators were trained bachelors. Then two specialists (authors of this paper) validated their annotation and after that an expert in forensic linguistics examined the selected data.,Yes,"At the first research step the authors conducted semantic analysis of the text of ""Article 5.61"" of the Code of Administrative Offenses of the Russian Federation (hereinafter - Article 5.61), which made it possible to formulate the following inclusion criteria for formal classification:

(1) the presence of an indecent form in the message perceived as insulting speech acts (in Russian language an indecent form of expression is assigned to abusive, obscene vocabulary;

(2) the presence of lexical markers that fix the orientation of the statement towards Internet users - interlocutor (posts with direct appeal using nicknames or pronouns);

(3) the presence of vocabulary that represents the immoral and publicly condemned qualities of a person's character and behavior. (This criterion has caused a lot of discussion between annotators, since it was not possible to formalize it. However, a convention was adopted in which annotators relied on their linguistic flair and theoretical material on the topic ""Philosophical foundations of morality and ethics"".)

A VKontakte post (text message) was classified as insulting speech act if it met the three criteria outlined above. To indicate compliance or non-compliance with the criterion, labels were used while annotating: decent - obscene, person - impersonal, moral - immoral.",Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,the recipient,To a person,No,Human
431,No,"(Sharifirad & Matwin, 2019)",Using Attention-based Bidirectional LSTM to Identify Different Categories of Offensive Language Directed Toward Female Celebrities,Proceedings of the 2019 Workshop on Widening NLP,Canada,"Twitter users who harass famous female figures may do so with different intentions and intensities. Recent studies have published datasets focusing on different types of online harassment, vulgar language and emotional intensities. The authors trained, validate and test their proposed model, attention-based bidirectional neural network, on the three datasets:""online harassment"", ""vulgar language"" and ""valance"" and achieved state of the art performance in two of the datasets. They report F1 score for each dataset separately along with the final precision, recall and macro-averaged F1 score. In addition, they identify ten female figures from different professions and racial backgrounds who have experienced harassment on Twitter. They tested the trained models on ten collected corpuses each related to one famous female figure to predict the type of harassing language, the type of vulgar language and the degree of intensity of language occurring on their social platforms. Interestingly, the achieved results show different patterns of linguistic use targeting different racial background and occupations. The contribution of this study is two-fold. From the technical perspective, the proposed methodology is shown to be effective with a good margin in comparison to the previous state-of-the-art results on one of the two available datasets. From the social perspective, the authors introduce a methodology which can unlock facts about the nature of offensive language targeting women on online social platforms. The collected dataset will be shared publicly for further investigation.",Yes,6,XX.08.2019,16.11.2022,Yes,Yes,"Online harassment, Offensive Language Directed Toward Female Celebrities. ""Online harassment is not often one dimensional: women from different social groups have been harassed because of their race, their ethnicity, their sexuality, the religious identity and even because of their disabilities (WMC Speech Project, 2018).""",Yes,"harassment categories: “indirect harassment”, “physical harassment” and “sexual harassment” six types of vulgar language levels of valence or intensity of emotion",No,,,,,"Ten collected corpuses each related to one famous female figure;

Existing datasets: “online harassment”, “vulgar language” and ”valance”",13,10,Twitter,"Harassment, Offensiveness",English,Implicitly one month before XX.08.2019,N/A,N/A,N/A,The authors consider ten female figures of different racial backgrounds and professions who have experienced online harassment. They collected five hundred tweets on the duration of one month using the python Twitter API.,Not discussed,,automated,Not discussed,,Not discussed,,500 tweets (newly collected),N/A,500 tweets (newly collected),N/A,Not discussed,,Not discussed,,Specific,"gender, race",Targeted,"gender, race, sexuality, religion, disability",No,No,,,No,,Yes,ten female figures of different racial backgrounds and professions who have experienced online harassment,To a person,No,Human
432,No,"(Sharif & Hoque, 2021)",Identification and Classification of Textual Aggression in Social Media: Resource Creation and Evaluation,Combating Online Hostile Posts in Regional Languages during Emergency Situation,Bangladesh,"This paper presents an aggressive text classification system in Bengali. A corpus (‘ATxtC’) is developed using hierarchical annotation schema that contains 7591 annotated texts (3888 for aggressive and 3703 for non-aggressive). Furthermore, the proposed system can classify aggressive Bengali text into religious, gendered, verbal and political aggression classes. Data annotation obtained a 0.74 kappa score in coarse-grained and 0.61 kappa score in fine-grained categories, which ensures the data’s acceptable quality. Several classification algorithms such as LR, RF, SVM, CNN and BiLSTM are implemented on AtxtC. The experimental result shows that the combined CNN and BiLSTM model achieved the highest weighted F1 score of 0.87 (identification task) and 0.80 (classification task).",Yes,8,09.04.2021,16.11.2022,Yes,Yes,"Aggressive texts (AG): attack, incite or seek to harm an individual, group or community based on some criteria such as political ideology, religious belief, sexual orientation, gender, race and nationality.",Yes,"Level A: Aggressive Text Identification: • Aggressive texts (AG), Non aggressive texts (NoAG) Level B: Classification of Aggressive Text: • Religious Aggression (ReAG), • Gendered Aggression (GAG) • Verbal Aggression (VeAG), • Political Aggression (PoAG)",No,,,,,Aggressive Text Corpus (ATxtC),1,1,"Facebook, YouTube",Aggression,Bengali,N/A,N/A,N/A,N/A,"The authors accumulated aggressive and non-aggressive texts manually from Facebook and YouTube as most of the Bengali social media users are active on these platforms. Most of the religious aggression data collected from comment threads of various Facebook pages and YouTube channels spread hatred and misinformation about religion. Most of the aggression's expressed in social media is against women which contain obscene and toxic comments. Texts related to gendered aggression is accumulated from several sources, including fashion pages, fitness videos, and news coverage on women/celebrities. Verbally aggressive texts include nasty words and obscene language. Political aggression texts procured from different pages. These pages stated about political parties and influential political figures and peoples' reaction to the government's different policies. Non-aggressive data culled from newspapers, Facebook and YouTube contents and these texts do not have any properties of aggression.",Not discussed,,manual,Yes,"A total of 5 annotators perform manual annotation. Prior to annotation, the authors provided examples of each category to the annotators and explained why a sample should be labelled as a specific class. Each of the instances was labelled by two annotators. In case of disagreement, the authors called an academician experienced in this domain to resolve the issue through discussion. During annotation, they observe that some of the texts have overlap among aggression dimensions. As these numbers are deficient, they do not include such instances in the current corpus for simplicity.",Not discussed,,7591,5322,1519,"Five annotators in total. Some key characteristics of annotators are: a) age between 20-28 years, b) field of research NLP and experience varies from 10-30 months, c) all are native Bangla language speakers, d) active in social media and view aggression in these platforms.
In case of disagreement, the authors called an academician experienced in this domain to resolve the issue through discussion.",Yes,"A total of 5 annotators perform manual annotation. Prior to annotation, the authors provided examples of each category to the annotators and explained why a sample should be labelled as a specific class.

Hierarchical annotation schema is used to divide ATxtC into two levels: (A) identify whether a text is aggressive or not (B) classify an aggressive text into fine-grained classes namely religious aggression, gendered aggression, verbal aggression and political aggression.

3.1 Level A: Aggressive Text Identification

• Aggressive texts (AG): attack, incite or seek to harm an individual, group or community based on some criteria such as political ideology, religious belief, sexual orientation, gender, race and nationality.

• Non aggressive texts (NoG): do not contain any statement of aggression or express hidden wish/intent to harm others.

3.2 Level B: Classification of Aggressive Text

• Religious Aggression (RAG: incite violence by attacking religion (Islam, Hindu, Catholic, etc.), religious organizations, or religious belief of a person or a community.

• Gendered Aggression (GeAG): promote aggression or attack the victim based on gender, contain aggressive reference to one's sexual orientation, body parts, sexuality, or other lewd contents.

• Verbal Aggression (VeAG): damage social identity and status of the target by using nasty words, curse words and other obscene languages.

• Political Aggression (PoAG: provoke followers of political parties, condemn political ideology, or excite people in opposition to the state, law or enforcing agencies.",Not discussed,,Specific,"religion, gender, body, sexuality, political",Targeted,"political, religion, sexuality, gender, race, nationality",Yes,Yes,"Religion, Gender, Sexual orientation, Body, Sexuality, Political","religion, gender, body, sexuality, political",Yes,"political parties/ideology  see Level B: Classification of Aggressive Text: • Religious Aggression (ReAG), • Gendered Aggression (GAG) • Verbal Aggression (VeAG), • Political Aggression (PoAG)",No,,,No,Human
433,No,"(Rodriguez & Rojas-Galeano, 2018)",Fighting Adversarial Attacks on Online Abusive Language Moderation,Applied Computer Sciences in Engineering,Colombia,"Recently Google has developed a machine-learning model to detect hostility within a comment. The model is able to assess to what extent abusive language is poisoning a conversation, obtaining a “toxicity” score for the comment. Unfortunately, it has been suggested that such a toxicity model can be deceived by adversarial attacks that manipulate the text sequence of the abusive language. In this paper the authors aim to fight this anomaly; firstly they characterise two types of adversarial attacks, one using obfuscation and the other using polarity transformations. Then, they propose a two–stage approach to disarm such attacks by coupling a text deobfuscation method and the toxicity scoring model. The approach was validated on a dataset of approximately 24000 distorted comments showing that it is feasible to restore the toxicity score of the adversarial variants. They anticipate that combining machine learning and text pattern recognition methods operating on different layers of linguistic features, will help to foster aggression–safe online conversations despite the adversary challenges inherent to the versatile nature of written language.",Yes,4,13.09.2018,16.11.2022,Yes,Yes,"Abusive language, toxicity",No,,Yes,Other,GitLab,https://gitlab.com/textpatrol/gp-tp-experiment,Yes,"No attack (original);

Obfuscation attack (obfuscation-50 and obfuscation-99);

Polarity attack (negation)",4,3,The Google Perspective website (comments gathered from online surveys),"Abusiveness, Adversarial Attacks",English,N/A,N/A,N/A,"Three topics: US Election, Brexit, Climate change","The Google Perspective website provides a sample of comments gathered from online surveys on three delicate topics: US Election (45 comments), Brexit (61) and Climate change (49). The comments' text along with scores obtained with the GP toxicity model are available. So the authors collected those with toxicity scores higher than 60%, obtaining a subset of 24 comments that we labeled as {use101,..., usel10, brex01,., brex07, clic01,., clic07} (for a complete list, see Appendix). They subsequently prepared datasets conveying the two types of adversarial anomalies, namely obfuscation attack and polarity attack.

For the first attack, 1000 variations of each comment were randomly generated by isolating toxic terms in order to obfuscate them with a number of edits including homoglyph substitution, bogus segmentation and letter repetition. The isolation of toxic terms was made as explained in the next subsection. The number of edits applied to each term was controlled with a corruption rate p ∈ {50%, 99%}. Thus, for each character in the isolated term a uniformly-distributed random number r ~ U(0, 1) was sampledif r &ltp the character was kept unchanged, otherwise one of these edits was applied (with their respective probabilities): an homoglyph substitution (60%), a bogus segmentation (30%) or a fake letter repetition (10%). In this way, they obtained two datasets, obfuscation-50 and obfuscation-99 consisting of 24,000 obfuscated toxic comments each (see examples in Table 1). With respect to the homoglyph edits utilised in the obfuscation attack, they defined substitution lists encompassing a subset of the ASCII encoding set. (Fig. 1. Homoglyph substitution lists. For each letter of the English alphabet in the inner orange ring, the corresponding list of substituting symbols are shown in the outer blue ring. The blank character “ ” indexes the lists of bogus segmentators.)

The second dataset (polarity) was obtained by inserting negation predicates within each comment. Thus, the size of this dataset is 24 negated comments. Although possible, they refrained from generating obfuscated variants of these comments as the aim of this attack was to investigate the effect of the polarity change achieved by the negated predicates alone.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,Yes,Human
434,No,"(Addawood et al., 2019)",Linguistic Cues to Deception: Identifying Political Trolls on Social Media,Proceedings of the International AAAI Conference on Web and Social Media,USA,"One example of a manipulation campaign that has garnered much attention recently was the alleged Russian interference in the 2016 U.S. elections, with Russia accused of, among other things, using trolls and malicious accounts to spread misinformation and politically biased information. To take an in-depth look at this manipulation campaign, the authors collected a dataset of 13 million election-related posts shared on Twitter in 2016 by over a million distinct users. This dataset includes accounts associated with the identified Russian trolls as well as users sharing posts in the same time period on a variety of topics around the 2016 elections. To study how these trolls attempted to manipulate public opinion, they identified 49 theoretically grounded linguistic markers of deception and measured their use by troll and non-troll accounts. We show that deceptive language cues can help to accurately identify trolls, with average F1 score of 82% and recall 88%.",Yes,61,06.07.2019,16.11.2022,Yes,Yes,"Trolls are user accounts whose sole purpose is to sow conflict and deception. In the context of the 2016 elections, their intent was to harm the political process and create distrust in the political system.",No,,No,,,,,Twitter data on Russian trolls,1,1,Twitter,Political Trolling,"English, Russian",N/A,Implicitly between 2015 and 2016,N/A,The 2016 U.S. Presidential election,"Trolls: To collect Twitter data on Russian trolls, the authors used a list of 2,752 Russian troll accounts compiled and released by the U.S. Congress.(https://www.recode.net/2017/11/2/16598312/russia-twitter-trump-twitter-deactivated-handle-list) After that, they collected all of the trolls' discussions. To collect the tweets, they used Crimson Hexagon, a social media analytic platform that provides paid datastream access. This tool allowed them to obtain tweets and retweets produced by trolls and subsequently deleted in 2016. They were interested in understanding troll activity during the election year. We collected data starting from 2015. Trolls were already active in 2015, posting over a million tweets, 44% of them in Russian, with 31% of the posts with an identifiable location coming from Russia. In 2016, the 1,148 trolls posted 1,226,185 tweets, of which 27% were written in Russian. Over 90% of the tweets had identifiable locations, with 65% from the U.S., 27% from Russia, and 2% from Belarus. Troll activity increased in the months leading to the elections, with spikes in activity related to external events. Interestingly, the biggest spike of activity was on October 6th.

Non-Trolls: To collect non-troll tweets, the authors use two strategies. First, they collect such tweets using a list of hashtags and keywords that relate to the 2016 U.S. Presidential election. This list is crafted to contain a roughly equal number of hashtags and keywords associated with each major Presidential candidate: they select 23 terms, including five terms referring to the Republican Party nominee Donald J. Trump (#donaldtrump, #trump2016, #neverhillary, #trumppence16, #trump), four terms for Democratic Party nominee Hillary Clinton (#hillaryclinton, #imwithher, #nevertrump, #hillary), and several terms related to debates. To make sure the query list was comprehensive, they add a few keywords for the two third-party candidates, including the Libertarian Party nominee Gary Johnson (one term), and Green Party nominee Jill Stein (two terms). The second strategy is to collect tweets from the same users that do not include the same key terms mentioned above and making sure that they exclude any users who have re-tweeted a troll. Users who did not retweet a troll may help with shaping a better understanding of troll behaviours online. The collection yielded a total of 12,361,285 tweets produced by 1,166,760 unique users.",No,,N/A,Not discussed,,Not discussed,,"1,226,155 tweets by trolls + 12,361,285 tweets by non-trolls",10-fold cross-validation,10-fold cross-validation,N/A,Not discussed,,Not discussed,,Specific,political,Targeted,political,Yes,No,,,Yes,political-related  (political trolls in the 2016 U.S. president election),No,,,No,Human
436,No,"(Edwards et al., 2020)",Detecting Cyberbullying Activity Across Platforms,17th International Conference on Information Technology–New Generations (ITNG 2020),USA,"Recent surveys show that youth are being exposed to cyber-aggression at increasing rates, with over 43% of youth reporting in one recent survey that they have been bullied online. While research into this problem has been growing, the research community is hampered by a lack of authentic data for studying communication with and among youth. A large corpus with 800,000 instances of cell phone textual data from youth ages 10–14 has been developed to address this need. This article describes the dataset, as well as plans to enable access to the data while protecting the privacy of the study participants. The results from machine learning experiments for the detection of cyberbullying based on labeled data from several sources, including both SMS and social media messages, are also discussed. These algorithms are shown to be effective at detecting cyberbullying across platforms.",Yes,8,12.05.2020,16.11.2022,Yes,Yes,"Cyberbullying is defined as the use of social media, email, cell phones, text messages, and Internet sites to threaten, harass, embarrass, or socially exclude someone of lesser power.",No,,Yes,Website,,"Access to this system will be granted by request to the authors from qualified research groups. The queries are logged and monitored. Efforts to circumvent the privacy controls will result in immediate removal from the online system. In recognition of the frustration inherent for other researchers in the hiding of the underlying data, plans to develop an API which can be used for more sophisticated requests (for example, extracting features for input to machine learning algorithms) are also in process. Until the API becomes available, requests for ad ho queries to support work by individuals and research groups can be made to the authors and will be completed as time permits.",Yes,"The dataset (merged);

SMS dataset (new);

existing dataset: preliminary coded SMS datasetFormspring.me dataset, tweets from the 2016 elections",1,1,"SMS, Formspring.me, Twitter",Cyberbullying,English,"for a full year, 2016","for a full year, 2016",N/A,N/A,"For this final phase of the study, smartphones were deployed to 70 youth, ages 10-14, and all textual activity on the devices was tracked for a full year. The software collected both inbound and outbound SMS (text) messages, and outbound keyboard activity from messaging apps such as Snapchat, FB Messenger, and Instagram.

Each of the 70 participants, as well as their parent or guardian provided consent for all textual data to be captured and tracked (oversight and approval was provided by the Institutional Review Boards at Ursinus College and Elmhurst College). In exchange, the participant received a free smart phone, and a full year of cell phone service. Participants who completed the entire year were allowed to keep the phone as a token of appreciation for their participation. Sixty-two of the 70 participants completed the full year of the study, and those that left early agreed to allow use of the data that had been collected to date. The most common reason given for leaving the study early was a desire to switch to another brand of smart phone.

Each participant received a Samsung Galaxy S9 preloaded with software to continuously relay any incoming or outgoing SMS or MMS messages to a server, where it was stored in a database. The client-side architecture consists of a background process running on Android that listens for incoming or outgoing text messages. This approach was used in conjunction with a keylogger that captured outgoing transmissions from other messaging apps such as Snapchat, FB Messenger, Instagram, etc. The keylogger has produced an enormous amount of data totaling over a million records. Unfortunately, the keylogger also picks up pre-filled text boxes. The data cleansing process to remove this pre-filled text is ongoing and additional records will be added to the online system when the process is completed.",Yes,"To address this need (of accessing to the dataset), while simultaneously protecting the privacy and identity of the study participants, an online system that allows ad ho queries has been developed.",manual,Yes,"An online system was developed to facilitate the labeling process. This system allows trained coders to label messages by assigning each a code describing the content. Of the 10,072 SMS messages that were labeled for use in these experiments, 480 (4.8% of the messages) have shown to be instances of cyberbullying.",Not discussed,,"25,223 instances",1764,"1172 (test-1), 11699 (test-2)",N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
437,No,"(Xiang et al., 2021)",Identification of Hate Tweets: Which Words Matter the Most?,"HCI International 2021 - Late Breaking Papers: Multimodality, eXtended Reality, and Artificial Intelligence",USA,"In this article, the authors assembled a multi-class dataset of 83,360 tweets, which include four hate classes and one non-hate class. They adopted three popular machine learning and deep learning models, such as SVM, long short-term memory model extended with an attention mechanism (LSTM-Attention), and BERT model for the classification of those hate tweets. The experiments show that for the identification of religious-hate tweets, LSTM-Attention model consistently produced the best performance, while for other two types of hate tweets, such as sexist and racist, the authors observed that on average BERT produces the best result. Both the LSTM-Attention and BERT reached an overall F1-score of 90% and 91% respectively for multi-label classification. The detail analysis reveals that since sexist or racist tweets make use of more hate or slang words than that of religious-hate tweets, BERT performs better in detecting the first two classes. However, since many of religious-hate tweets not necessarily include any disparaging words, BERT failed to identify those hate speeches. Thus, LSTM-Attention was used to extract the words with higher attention weights to identify which words matter the most for the classification of hate speech. The authors show that words that carry more predictive information for the hate tweets classification are not necessarily high frequency or hateful words.",Yes,0,11.11.2021,16.11.2022,Yes,Yes,"Hate speech is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics.",Yes,"classes: Non-hate, Racist-hate, Sexist-hate, Religious-hate, Offensive",No,,,,,"The combined dataset;

subsets: A, B, C, D, E, F, G",8,8,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"The combined dataset was collected from four different sources:

 Hate speech identification dataset by Thomas et al.

 Hate speech dataset by Hurmet et al.

 Stanford sentiment dataset by Kazanove et al.

 Hate speech detection in multimodal publication by Raul et al.

In the Hate Speech Identification dataset, 24,802 tweets were manually labeled as Hate Speech, Offensive, or neither Offensive nor Hate Speech by CrowdFlower workers. A tweet was labeled as hate speech not just if it contains hate words from Hatebase.org but also if its context carries hateful meaning. The dataset contains a total of 24,783 tweets, comprising of 1,430 tweets of hate speech, 19,190 tweets of offensive speech, and 4,163 tweets of non-abusive tweets. Hate speech and offensive speech were combined to create the offensive class (20,620 tweets) for this study.

In the Hate Speech dataset, each tweet contains the word religion. Preprocessing includes removing username, hashtags, URLs, numbers, punctuation, and special characters (@, &, #, %, etc.), short words (less than three characters). Then the collection words like religion or religious were removed. Tweets with less than 3 tokens were also removed. For this dataset, SONAR algorithm was adopted to perform hate speech detection and tweets were labeled as hate, offensive, or neither. The authors combined 248 hate-speech and 3,609 offensive tweets and created religion-hate class with 3,857 tweets.

In the Stanford Sentiment dataset, 1,600,000 tweets with emoticons were extracted using the Twitter Application Programming Interface (API). Tweets containing five positive emoticons [ :) , :=), : ), :D, =) ] were annotated as positive and tweets with negative emoticons [ :(,:-(,: ( ] were annotated as negative. These emoticons were stripped off to reduce biases in modeling task. After processing, there are 800,000 positive and 800,000 negative tweets. 19,986 positive tweets were sampled as non-hate tweets for our study.

In the Hate Speech Detection in Multimodal Publication dataset, the original tweets containing both text and images were annotated by Amazon Mechanical Turk workers into six categories (no attacks to any racist, sexist, homophobic, religion based attacks, including attacks to ethnic communities). If a tweet had only sexist annotations, it was labeled as sexist tweetand if a tweet had only racist annotations, it was labeled as racist tweet. The authors processed the downloaded dataset and performed some cleaning. In this study, they kept only the text tweets and removed images. They collected 16,243 sexist tweets, and 20,000 racist tweets which were randomly sampled from 47,246 racist labeled tweets.

Finally, the combined dataset includes a total of 83,360 tweets.

They created a total of 7 different datasets to perform various experiments. Dataset A include four classes of tweets, namely non-hate vs three hate classes (racist, sexist and religious-hate). They also have one dataset (D) that includes three different hate classes to show whether a model can differentiate among different types of hate tweets. They did not compare the offensive tweet class to the other three hate tweet classes, as almost all the offensive words can be found in the three other hate classes. Datasets B, C, E, F, and G combine the subset of all the five tweet classes for experimenting with binary classification task. In dataset B, there are two classes, one is hate class (which has three hate subclasses combined with only one label), and the other one is non-hate class. In dataset C, there are two classes, one is non-hate class, and the other one is offensive class. In dataset E, there are two classes, one is non-religion hate (combining racist, and sexist) and the other one is religious hate. Similarly, dataset F has two classes such as non-racist hate tweets combining other two hate classes and racist hate tweetswhile dataset G contains non-sexist hate tweets (i.e., racist, and religious-hate tweets) and sexist hate class.",Not discussed,,"automated, manual",Not discussed,,Not discussed,,"83,360 tweets (combined dataset)",80% (~66688),20% (~16672),N/A,Not discussed,,Not discussed,,Specific,"race, gender, religion, other",Targeted,"race, gender, sexuality, nationality, religion, other",Yes,Yes,"Race, Gender/Sex, Religion, other","race, gender, religion, other",No,,No,,,No,Human
439,No,"(Leonardelli et al., 2021)",Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,Italy,"The authors focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. This study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. They also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators' agreement has a strong effect on classifiers performance and robustness. The findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. They show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and they advocate for a higher presence of ambiguous cases in future datasets, in order to train more robust systems and better account for the different points of view expressed online.",Yes,14,07.11.2021,16.11.2022,Yes,Yes,"Offensive: Profanity, strongly impolite, rude, violent or vulgar language expressed with angry, fighting or hurtful words in order to insult or debase a targeted individual or group. This language can be derogatory on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender. Also sarcastic or humorous expressions, if they are meant to offend or hurt one or more persons, are included in this category.",Yes,"ambiguity level:  unanimous agreement with A++ (i.e. agreement between 5 annotators or classifiers), mild agreement with A+ (i.e. 4 out of 5 annotations agreeing on the same label), and weak agreement with A0 (i.e. the 5 annotations include 3 of them in agreement and 2 in disagreement).  O++/+/0 for offensive tweets, and N++/+/0 for non offensive ones.",Yes,Other,upon request via Google form,"https://github.com/dhfbk/annotators-agreement-dataset  The dataset, in the form of tweet IDs with accompanying annotation, **can be obtained upon request to the authors** by filling the following form: https://forms.gle/hBKTUbCR1zimNhicA.",Yes,"annotators-agreement-dataset;

existing datasets: Offenseval datasetabusive language dataset by Founta et al., 2018;",3,1,Twitter,Offensiveness,English,Covid-19: 25.01.2020 - 09.11.2020 BLM: 24.05.2020 - 16.06.2020 US Elections: 30.09.2020 - 04.11.2020,Covid-19: 25.01.2020 - 09.11.2020 BLM: 24.05.2020 - 16.06.2020 US Elections: 30.09.2020 - 04.11.2020,N/A,Covid-19; BLM; The 2020 US Election,"Through its application programming interface (API), Twitter provides access to publicly available messages upon specific request. For each of the domains analysed, a set of hashtags and keywords was identified that unequivocally characterizes the domain and is collectively used. During a specific period of observation, all the tweets containing at least an item of this hashtags/keywords seed list were retrieved in real time (using ""filter"" as query). The most relevant entries from the covid-19 seed list are: covid-19, coronavirus, ncov, #Wuhan, covid19, sarscov2 and covid. Data were collected in the time span between 25 January and 09 November 2020. The most relevant entries from the blm seed list are: george floyd, #blm, black lives matter. Tweets were collected between 24 May 2020 and 16 June 2020. The most relevant entries from the US Elections seed list are: #maga, #elections2020, Trump, Biden, Harris, Pence. The tweets were collected between 30 September 2020 and 04 November 2020. From this data collection, they randomly select 400,000 tweets (around 130,000 for each domain).",Yes,They replaced all mentions to users and urls with &lt;user; and &lt;url; respectively.,"automated, manual",Yes,"Every tweet is annotated by 5 native speakers from the US, who we expect to be familiar with the topics, using Amazon Mechanical Turk. The authors follow for all domains the same annotation guidelines, aimed at collecting crowd-workers' judgements on the offensiveness of the messages using the binary labels offensive and not offensive.

To ensure high-quality annotations, the authors select a pool of tweets from the three domains of interest and ask three expert linguists to annotate them. The tweets with perfect agreement are used as gold standard. They then include a gold standard tweet in every HIT (group of 5 tweets to be annotated). If a crowd-worker fails to evaluate the gold tweet, the HIT is discarded. Moreover, after the task completion we remove all the annotations done by workers who did not reach a minimum overall accuracy of 70% with respect to the gold standard. As a consequence of this quality control, for some tweets we could not collect five annotations, and they had to be removed from the final dataset. The total number of tweets annotated using AMT is 10,753, including 3,472 for Covid-19, 3,490 for US elections and 3,791 for BLM.",Yes,"From the collected data, about 400,000 tweets were randomly selected (around 130,000 for each domain).

To pre-evaluate the tweets the authors use a heuristic approach by creating an ensemble of 5 different classifiers, all based on the same BERT configuration and fine-tuned starting from the same abusive language dataset (Founta et al., 2018). Since the original dataset contains four classes (Spam, Normal, Abusive and Hateful), they first remove the tweets from the Spam class and map the remaining ones into a binary offensive or non-offensive label, by merging Abusive and Hateful tweets into the offensive class and mapping the Normal class into the non-offensive one. They then select 15k tweets from the Founta dataset (~100k tweets) for speeding up the process, as they are not interested in the overall performance of the different classifiers, but rather in their relative performances. Each classifier of the ensemble is trained using a different balance for the training and the evaluation set, so to yield slightly different predictions. In particular, all five classifiers are trained with the BERT-Base uncased model, a max seq length of 64, a batch size of 16 and 15 epochs. One classifier has been trained using 12k tweets in the training and 3k in the validation set, a second classifier was trained using the same training instances but repeated twice (24k), while the validation set remained the same. In a third and fourth configuration, they repeat twice the offensive and the non-offensive training instances respectively. Finally, in a fifth configuration they change the proportion between training and validation set (10k for training, 5k for validation).

A++: unanimous agreement (i.e. agreement between 5 annotators or classifiers);

A+: mild agreement (i.e. 4 out of 5 annotations agreeing on the same label);

A0: weak agreement (i.e. the 5 annotations include 3 of them in agreement and 2 in disagreement)

In order to analyse the relation between automated and manual annotation with respect to agreement and disagreement, they select an equal number of tweets from each class of agreement of the ensemble (A++, A+, A0) to be manually annotated. For each domain and each agreement class they select 1,300 tweets - equally divided between offensive and non-offensive predictions -for a total of 3,900 tweets per domain.","10,753 (manually labelled tweets)","75% (~8065)  (to control for the effect of training data size, the authors further downsample all sets to the smallest one, so that each agreement sample is equally represented (A++, A+, A0). In this way, they obtain 3 sets of training data - one per ambiguity level - containing 900 tweets each. Every set further contains 300 tweets from each domain, half for offensive label and half for non-offensive label so to control also for the effect of label distribution across domains and agreement levels.)",25% (~2688),"Native speakers from the US, who we expect to be familiar with the topics, using Amazon Mechanical Turk.",Yes,"Annotation Guidelines for AMT

This first section contains the instructions provided to annotators on Amazon Mechanical Turk. The first part changes according to the domain:

Covid-19: The tweets in this task have been collected during the pandemic. Would you find the content of the messages offensive? Try to judge the offensiveness of the tweets independently from your opinion but solely based on the abusive content that you may find.

US Presidential campaign: The tweets in this task have been collected during the last US Presidential campaign. Would you find the content of the messages offensive? Try to judge the offensiveness of the tweets independently from your political orientation but solely based on the abusive content that you may find.

Black Lives Matter: These tweets are related to the Black Lives Matter protests. Would you find the content of the messages offensive? Try to judge the offensiveness of the tweets independently from your opinion but solely based on the abusive content that you may find.

---

The second part of the task description, instead, is the same for all the domains, containing a definition of what is offensive and informing the workers that there is a quality check on the answers:

Offensive: Profanity, strongly impolite, rude, violent or vulgar language expressed with angry, fighting or hurtful words in order to insult or debase a targeted individual or group. This language can be derogatory on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender. Also sarcastic or humorous expressions, if they are meant to offend or hurt one or more persons, are included in this category.

Normal: tweets that do not fall in the previous category.

Quality Check: the HIT may contain a gold standard sentence, manually annotated by three different researchers, whose outcome is in agreement. If that sentence is wrongly annotated by a worker, the HIT is automatically rejected.

---

Asking annotators to label the tweets independently from their views, opinions or political orientation was inspired by recent works, showing that making explicit possible biases in the annotators contributes to reduce such bias (Sap et al., 2019).",Not discussed,,Specific,"race, political, other",Targeted,"race, religion, sexuality, disability, gender",Yes,Yes,"Race, political-related, other","race, political, other",No,,No,,,No,Human
440,No,"(Qian et al., 2019)",A Benchmark Dataset for Learning to Intervene in Online Hate Speech,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),USA,"Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, the authors propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, they introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, they also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.",Yes,97,03.11.2019,16.11.2022,Yes,Yes,"Hate speech. We (Facebook) define hate speech as a direct attack on people based on what we call protected characteristics race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability.",No,,Yes,Github,,https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech,Yes,"A Benchmark Dataset for Learning to Intervene in Online Hate Speech: Gab, Reddit",2,2,"Gab, Reddit",Hate Speech,English,Gab: in October 2018 Reddit: N/A,N/A,N/A,N/A,"Reddit: To retrieve high-quality conversational data that would likely include hate speech, the authors referenced the list of the whiniest most low-key toxic subreddits. Skipping the three subreddits that have been removed, they collect data from ten subreddits: r/DankMemes, r/Imgoingtohellforthis, r/KotakulnAction, r/MensRights, r/MetaCanada, r/MGTOW, r/PussyPass, r/PussyPassDenied, r/The Donald, and r/TumblrInAction. For each of these subreddits, they retrieve the top 200 hottest submissions using Reddit's API. To further focus on conversations with hate speech in each submission, they use hate keywords (ElSherief et al., 2018b) to identify potentially hateful comments and then reconstructed the conversational context of each comment. This context consists of all comments preceding and following a potentially hateful comment. Thus for each potentially hateful comment, they rebuild the conversation where the comment appears. Figure 2 shows an example of the collected conversation, where the second comment contains a hate keyword and is considered as potentially hateful. Because a conversation may contain more than one comments with hate keywords, they removed any duplicated conversations.

Gab: They collect data from all the Gab posts in October 2018. Similar to Reddit, they use hate keywords (ElSherief et al., 2018b) to identify potentially hateful posts, rebuild the conversation context and clean duplicate conversations.",Yes,All personally identifiable information such as user names is masked in the datasets.,manual,Yes,"The authors presented the data to Mechanical Turk workers to label and create intervention suggestions. In order not to over-burden the workers, they filtered out conversations consisting of more than 20 comments. Each assignment consists of 5 conversations. For Reddit, they also present the title and content of the corresponding submission in order to give workers more information about the topic and context. For each conversation, a worker is asked to answer two questions:

• Q1: Which posts or comments in this conversation are hate speech?

• Q2: If there exists hate speech in the conversation, how would you respond to intervene? Write down a response that can probably hold it back (word limit: 140 characters).

If the worker thinks no hate speech exists in the conversation, then the answers to both questions are ""n/a"". To provide context, the definition of hate speech from Facebook: ""We define hate speech as a direct attack on people based on what we call protected characteristics race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability."" is presented to the workers. Also, to prevent workers from using hate speech in the response or writing responses that are too general, such as ""Please do not say that"", the authors provide additional instructions and rejected examples.

Each conversation is assigned to three different workers. To ensure data quality, they restrict the workers to be in an English speaking country, with a HIT approval rate higher than 95%. Excluding the rejected answers, the collected data involves 926 different workers. The final hate speech labels (answers to Q1) are aggregated according to the majority of the workers' answers. A comment is considered hate speech only when at least two out of the three workers label it as hate speech. The responses (answers to Q2) are aggregated according to the aggregated result of Q1. If the worker's label to Q1 agrees with the aggregated result, then their answer to Q2 is included as a candidate response to the corresponding conversation but is otherwise disregarded.",Not discussed,,"Reddit: 5,020 conversations (including 22,324 comments), 7,641 interventions Gab: 11,825 conversations (consisting of 33,776 posts), 21,747 interventions",80% (~17859; ~27021),20% (~4465; 6755),"The authors restrict the workers to be in an English speaking country including Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States, with a HIT approval rate higher than 95%.",Yes,"For each conversation, a worker is asked to answer two questions:

• Q1: Which posts or comments in this conversation are hate speech?

• Q2: If there exists hate speech in the conversation, how would you respond to intervene? Write down a response that can probably hold it back (word limit: 140 characters).

If the worker thinks no hate speech exists in the conversation, then the answers to both questions are ""n/a"". To provide context, the definition of hate speech from Facebook: ""We define hate speech as a direct attack on people based on what we call protected characteristics race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability."" is presented to the workers. Also, to prevent workers from using hate speech in the response or writing responses that are too general, such as ""Please do not say that"", the authors provide additional instructions and rejected examples.",Not discussed,,General,N/A,Targeted,"race, nationality, religion, sexuality, gender, disability",Yes,No,,,No,,No,,,Yes,Human
442,No,"(Wich et al., 2021)",German Abusive Language Dataset with Focus on COVID-19,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),Germany,"The authors created a German abusive language dataset that focuses on COVID-19. It contains 4,960 annotated tweets from 2,662 accounts. 22% of the tweets are labeled as ABUSIVE, 78% as NEUTRAL. The second contribution is a dataset creation methodology for collecting abusive language data from Twitter with a substantial amount of abusive and hateful content.",Yes,3,06.09.2021,16.11.2022,Yes,Yes,"ABUSIVE: The tweet comprised ""any form of insult, harassment, hate, degradation, identity attack, and the threat of violence targeting an individual or a group"" (Räther, 2021, p. 36).",No,,Yes,Github,,https://github.com/mawic/german-abusive-language-covid-19,Yes,"German Abusive Language Dataset with Focus on COVID-19;

Existing datasets: GermEval, HASOC",3,1,Twitter,Abusiveness,German,N/A,01.01.2020 - 20.02.2021,N/A,Covid-19,"The starting point of the data collection for all pools was a set of three seed accounts. These accounts originate from a study conducted by Richter et al. (2020), in which the authors have described influential Twitter accounts sharing misinformation about COVID-19. The accounts were selected by the authors based on the following criteria (Richter et al., 2020): (1) At least 20,000 accounts follow the account. (2) The account has shared or reported misinformation about COVID-19. (3) The account was active as of May 20, 2020. These accounts were chosen as seeds because hateful content often coincides with misinformation (Guhl and Gerster, 2020).

From these accounts, the authors retrieved the tweets that they published between 01.01.2020 and 20.02.2021 through the Twitter API. Subsequently, they filtered out the tweets that are related to COVID-19. We used a list of 65 keywords for this purpose. It comprised stemmed terms from a glossary about the current pandemic and some additions. Next, they retrieved the replies to these tweets through the Twitter API—a reply is a tweet that refers to another tweet. These replies were stored in the replies pool. To ensure the quantity and quality of hateful content, two annotators analyzed a sample of 100 tweets.

The community pool comprised COVID-19 related tweets from the accounts that replied to the seed accounts' tweets. The authors utilized a similar approach as in the previous phases. They retrieved the tweets from the accounts, limiting the maximum number of tweets per account to 500 and considering only tweets posted beyond 01.01.2020. The retrieved tweets were then filtered based on the 65 COVID-19-keywords. A sample of 100 tweets undergoes the same quality inspection as in the previous phase.

The third and last pool was the topic pool, whose purpose was to increase the prevalence of hateful content and topical diversity. It consists of tweets related to topics that coincide in the context of COVID-19 and hate speech (sCAN, 2020). To balance the different topics, the authors limited the number of filtered tweets per keyword to 1,000.

After filling the data pools, they applied two preprocessing phases to the data. First, all tweets holding less than two textual tokens were removed. Second, close and exact duplicates were removed by using locality-sensitive hashing with Jaccard similarity. Third, account names appearing in the tweets are masked to reduce annotator bias created by account names recognition. The annotation pool was then created by sampling the pools equally.",Not discussed,,manual,Yes,"The annotation schema for the sampled tweets comprised two classes:

• ABUSIVE: The tweet comprised ""any form of insult, harassment, hate, degradation, identity attack, and the threat of violence targeting an individual or a group"" (Räther, 2021, p. 36).

• NEUTRAL: The tweet did ""not fall into the ABUSIVE class"" (Räther, 2021, p. 36).

The data is annotated by three non-experts. To prepare them for the annotation process, they received training that contained a presentation of the annotation guidelines and a discussion among all annotators to define the task. Since the annotators are non-experts, the authors permitted them to skip tweets if they are indifferent (e.g., due to unclear cases or missing context information). This is to prevent the impairment of the quality of labels. The label indifference was handled as a missing label in the further course. Owing to limited resources, 275 tweets were annotated by two or three annotators to assess the inter-rater reliability with Krippendorff's alpha. All other tweets received only one annotation from any of the annotators. The authors employed doccano as an annotation tool.",Yes,"The tweets to be annotated are sampled from the annotation pool equally fed by three other pools- replies pool, community pool, and topic pool.","4,960","3,485",740,"Three non-experts (two female, one male; all between 20 and 30 years old)",Yes,"To prepare the annotators for the annotation process, they received training that contained a presentation of the annotation guidelines and a discussion among all annotators to define the task.

The annotation schema for the sampled tweets comprised two classes:

• ABUSIVE: The tweet comprised ""any form of insult, harassment, hate, degradation, identity attack, and the threat of violence targeting an individual or a group"" (Räther, 2021, p. 36).

• NEUTRAL: The tweet did ""not fall into the ABUSIVE class"" (Räther, 2021, p. 36).",Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
447,No,"(Phanomtip et al., 2021)",Cyberbullying detection on Tweets,"2021 18th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)",Thailand,"Cyberbullying is bullying with the use of digital technologies which is repeated, aimed at scaring, angering or shaming those who are targeted. To mitigate this problem, the authors propose 1) a novel dataset of 67K tweets collected from Twitter, 2) automatic annotation methods for a large-scale dataset and 3) a detection system that to identify these toxic behaviours. The experimental results demonstrate that their method outperforms the baseline by 4%.",Yes,1,19.05.2021,16.11.2022,Yes,Yes,"Cyberbullying is a form of bullying or harassment using digital technologies which aims at scaring, angering or shaming those who are targeted.",No,,No,,,,,"Detecting Cyberbullying in Tweets (existing dataset);

Hate Speech Tweets (new)",2,1,Twitter,Cyberbullying,English,N/A,N/A,N/A,N/A,"A. Detecting Cyberbullying in Tweets

This dataset [20] contains 38,686 tweets integrated from [21] and [22]. These tweets are annotated by crowd sourcing and put into two different classes: 65.8% of offensive and 34.2% of non-offensive ones.

B. Hate Speech Tweets

Hate Speech Tweets is a novel dataset, collected for this work, crawled from Twitter API. There are 68,519 records from about 19K unique users. Each record includes the tweet and its metadata e.g., tweetid, number of retweets and favorites, time created, user-id, username, number of follows and friends, location etc. Unlike the Detecting Cyberbullying in Tweets dataset, Hate Speech Tweets dataset is not labeled.",Yes,"To protect the users' privacy and identities, these information (additional information or characters such as the users’ account (@account)) will be removed.","automated, manual",Yes,"The authors sample a small portion of our dataset, about 300 tweets, and ask 3 annotators to label them into offensive, nonoffensive or undefined. The majority vote among the annotators is assigned as a ground truth label of the data which results in 100 offensive tweets, 215 nonoffensive tweets and 21 undefined as our test samples.

To mitigate the annotation cost, the authors propose several automatic data annotation methods as follows:

1) Classifier based annotation: They directly apply the baseline approach, linear SVM + TF-IDF, to predict the label, offensive or non-offensive, of the Hate Speech Tweets dataset, resulting in 7,341 additional positive samples and 61K negative samples.

2) Cosine similarity based annotation: They conduct the experiment based on the hypothesis that ""Tweets which are similar to the offensive ones could be offensive, while tweets which are close to the non-offensive ones should be labeled as non-offensive as well.' Thus, they compute the cosine similarity between all pairs of tweets in [20] and Hate Speech Tweets, and only the maximum cosine similarity is recorded. While the abusive pairs generate a positive cosine similarity score, the non-abusive pairs produce negative scores. Using these positive-negative similarity scores, they explores 3 ways of computing the final similarity score tweet in Hate Speech Tweets as:

• Average cosine similarity score

• Top rank cosine similarity score

• Weighted cosine similarity score

They use the final similarity score to indicate the category of the unlabeled tweet i where a positive score specifies a positive sample or offensive tweet, a negative score means a negative sample or non-offensive one.",Yes,"The authors sample a small portion of our dataset, about 300 tweets, for manual annotation.",~67K,75% (~50k),"17,130; 200 (100 sampled from offensive tweets and 100 from non-offensive ones)",N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
448,No,"(Islam et al., 2021)",An evolutionary approach to comparative analysis of detecting Bangla abusive text,Bulletin of Electrical Engineering and Informatics,Bangladesh,"Plenty of research has been done on detecting abusive text in the English language. Bangla abusive text detection has not been done to a great extent. In this experimental study, the authors have applied three distinct approaches to a comprehensive dataset to obtain a better outcome. In the first study, a large dataset collected from Facebook and YouTube has been utilized to detect abusive texts. After extensive pre-processing and feature extraction, a set of consciously selected supervised machine learning classifiers i.e. multinomial Naïve Bayes (MNB), multi layer perceptron (MLP), support vector machine (SVM), decision tree, random forrest, stochastic gradient descent (SGD), ridge, perceptron and k-nearest neighbors (k-NN) has been applied to determine the best result. The second experiment is conducted by constructing a balanced dataset by random under sampling the majority class and finally, a Bengali stemmer is employed on the dataset and then the final experiment is conducted. In all three experiments, SVM with the full dataset obtained the highest accuracy of 88%.",Yes,3,01.08.2021,16.11.2022,Yes,Yes,Abusive text,No,,No,,,,,Full Dataset; Balanced Dataset; Stemmed Dataset,3,3,"Facebook, YouTube",Abusiveness,Bengali,N/A,N/A,N/A,N/A,"The authors have selected some of the most controversial and viral Facebook pages and YouTube channels of Bangladesh's current time like Naila Nayem, Hero Alom, Shefuda, Ripon Video, Model Arif Khan and a few more. Those pages and channels belong to controversial Facebook celebrities, actors, and vulgar content creators. Public comments are scrapped without commenter's information to ensure privacy utilizing graph application program interface (API) and ""Youtube Comment Scraper"" for Facebook and YouTube, respectively. An irrelevant comment like sticker/emoticon, other language comments except Bangla has been removed from the dataset.",Yes,Public comments are scrapped without commenter's information to ensure privacy,manual,Not discussed,,Not discussed,,full/stemmed dataset: 12028 balanced dataset: 9760,80% (~9622; ~7808),20% (~2406; ~1952),N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
452,No,"(Jokić et al., 2021)",A Twitter Corpus and Lexicon for Abusive Speech Detection in Serbian,"3rd Conference on Language, Data and Knowledge (LDK 2021)",Serbia,"This paper presents the work on building AbCoSER, the first corpus of abusive speech in Serbian. The corpus consists of 6,436 manually annotated tweets, out of which 1,416 were labelled as tweets using some kind of abusive speech. Those 1,416 tweets were further sub-classified, for instance to those using vulgar, hate speech, derogatory language, etc. In this paper, the authors explain the process of data acquisition, annotation, and corpus construction. They also discuss the results of an initial analysis of the annotation quality. Finally, they present an abusive speech lexicon structure and its enrichment with abusive triggers extracted from the AbCoSER dataset.",Yes,4,30.08.2021,03.12.2022,Yes,Yes,"The concept of abusive speech, in the context of this paper, is an umbrella term for phenomena such as profanities, derogatory, and hate speech. One of the most cited definitions of hate speech comes from John T. Nockleby, who perceives hate speech as ""any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic"".",Yes,"1st level: abusive (TRUE) or non-abusive (FALSE) 2nd level: Profanity (PROF), Hate speech (HS), Derogatory speech (DS), Other (OTH)",No,,,,,AbCoSER (Abusive Corpus for serbian),1,1,Twitter,Abusiveness,Serbian,N/A,N/A,N/A,N/A,"The authors started with the list of 80 user accounts gathered via crowd-sourcing. To this list, they added various users accounts whose tweets were reported as hate speech on the h8index, an online platform for reporting hate speech, verbal violence, bullying, and discrimination on the Web. Initially, they gathered 450,000 tweets from the timeline of 120 user accounts via Twitter API that were further cleaned by removing tweets that were retweeted from other users timeline and tweets containing URLs, leaving 150,000 tweets in the list. Although each tweet has a language column, in the majority of cases language of Serbian tweets was marked as und - unidentified - since Twitter cannot reliably recognize the Serbian language. For example, out of 150,000 tweets, only 8,000 were marked as tweets in Serbian, while 120,000 were marked as und. Therefore, they could not use this feature to filter tweets written in Serbian and have to rely on manual annotation.",Not discussed,,manual,Yes,"Annotation for classification of tweets

The authors decided each tweet be annotated by two independent annotators. Ten annotators participated in the annotation and therefore the data set was split into five parts and each part was annotated by an annotator pair.

The annotation was performed considering the content of a whole tweet, as was the case for the majority of data sets. At the first level annotators marked a tweet as abusive (TRUE) or non-abusive (FALSE). At the second level annotators determined the category of abusive speech in tweets marked as abusive:

1\. Profanity (PROF), the tweet contains simplicity and vulgarity;

2\. Hate speech (HS), if a tweet contains an attack, disparagement, or promotion of hatred towards a group of people or members of that group on the basis of race, ethnicity, nationality, gender, religion, political orientation, sexual orientation;

3\. Derogatory speech (DS), a tweet is used to attack or humiliate an individual or group in a general sense, not in a way hate speech does;

4\. Other (OTH), abusive speech that does not belong to the above-mentioned categories e.g. ironic or sarcastic tweets.

An abusive tweet belongs to at least one of the categories from the second annotation level.

All annotators were provided with training and annotation guidelines containing examples similar to (133, 25]). For each of the category, annotators obtained its definition, some examples and an indicative list of trigger words characteristic to it, as described in [42].

Besides annotation guidelines, annotators received the decision list for abusive speech identification similar to the one used in [45], but upgraded and adopted for the general abusive speech. Since Twitter does not identify Serbian as a language successfully, and thus the language column of a tweet could not be relied upon, the annotators were given one more task - to check the language of a tweet and whether it could be interpreted. They needed to mark tweets with meaningless content, tweets written in a foreign language or multilingual tweets. After annotating the initial set of 200 tweets, an additional workshop with annotators was conducted to comment on the first annotation results and discuss discovered problems. Annotation was done online using Google sheets.

As annotation guidelines in the form of the decision tree are proven to be good a practice (45, 6, 23), the authors prepared the guidelines for annotators in the same format. One can see from the decision tree that a tweet marked as abusive has to be tested for each subcategory, since, as mentioned earlier, one tweet can belong to one or more subcategories. Annotators had a tab in their annotation interface with examples of all possible annotation combinations.

Annotation of abusive token spans for lexicon building

Following the Toxic Spans Detection task on Semeval 2021, and in line with our goal to enrich our lexicon of abusive speech with new entries and the usage examples 42, the additional manual annotation was conducted on 1,564 tweets that at least one of the annotators marked as abusive. This set of tweets was divided and shared among the already trained annotators with a task to detect triggers in each of the tweets. The annotators were given oral and written instructions together with examples of abusive speech categories and respective triggers as discussed in [42]. The purpose of identifying abusive triggers is to use them to enrich the lexicon of abusive words.",Yes,"In order to check how representative the data set is the authors sampled 200 tweets from it. Still, the ratio of tweets with abusive speech was just 12%. Therefore, the users' list was manually checked for the type of users and users were removed that are less likely to generate abusive speech such as:

1\. Public users, like telecommunication and similar companies as well as newspapers and news portals were removed from the list of users since one cannot expect that this type of users will generate abusive speech, as proved in 11.

2\. Fan pages and official pages of public persons, including politicians and sportsmen, or political parties were removed from the list for the same reason.

3\. Users that tweet in a foreign language.

4\. Users that do not generate abusive speech were detected by inspecting their timeline.

Thereafter, an initial list of a few seed words was identified, and Twitter was searched for occurrences of those words. They did not just add those tweets to our data set, they rather identified users that created those tweets and added them to the user list. The reason for such an approach was to retain the variety of offensive terms occurring in the collected tweets . Finally, their followers and those who reply to abusive tweets were added to the list as well. At the end of this step, the authors extracted the timeline of 111 users, and they come up with 320,440 tweets. The next step was to remove duplicates, empty tweets, retweets, tweets with URLs, and tweets that contained just mentions. The list was reduced to 194,348 tweets. From this corpus, they randomly sampled 6,500 tweets. In the next step, they identified tweets written potentially in English by filtering out tweets whose language was marked with ""en"" (112 tweets). This set was manually checked and 64 English tweets were removed. The remaining 48 tweets were wrongly marked as written in English, while actually written in Serbian. The resulting data set had 6,436 tweets and this set was used for annotation. Tweeter data differs significantly from other types of texts, e.g. books or newspaper articles, meaning that there are specific issues that have to be considered when processing such data. Some of them are:

1\. Spelling, grammar and typing errors and regional variations are more frequent2. Frequent use of out of vocabulary words or intentionally misspelled words (e.g. fejv, lajna, QURAZ)3. Excessive use of abbreviations, e.g. nznm (eng. I don't know), mum (eng. go fuck your mum), np (eng. no problem), jbt (eng. fuck you), jbg (eng. fuck it) etc.4. Equal use of Cyrillic and Latin script, omission of diacritics, and different Unicode characters5. Use of foreign language words and emoticons (e.g. :'-),: -P, : @))6. Iwitter-specific text: mentions, retweets and URLs as well as hashtags (e.g. #TLZP, #Utisak, #u6reci).",6436,N/A,N/A,All annotators were native Serbian speakers.,Yes,"The annotation scheme is accompanied by a detailed annotation guide resulted from the research of Fortuna et al. [16]. All annotators were provided with training and annotation guidelines containing examples similar to (33, 25]). For each of the category, annotators obtained its definition, some examples and an indicative list of trigger words characteristic to it, as described in [42]. Besides annotation guidelines, annotators received the decision list for abusive speech identification similar to the one used in 45], but upgraded and adopted for the general abusive speech.

As annotation guidelines in the form of the decision tree are proven to be good a practice, the authors prepared the guidelines for annotators in the same format (shown in Figure 2). One can see from the decision tree that a tweet marked as abusive has to be tested for each subcategory, since, as mentioned earlier, one tweet can belong to one or more subcategories.",Not discussed,,General,N/A,Targeted,"race, gender, sexuality, nationality, religion, other",Yes,No,,,No,,No,,,No,Human
457,No,"(Obadimu et al., 2021)",Developing a socio-computational approach to examine toxicity propagation and regulation in COVID-19 discourse on YouTube,Information Processing & Management,USA,"As the novel coronavirus (COVID-19) continues to ravage the world at an unprecedented rate, formal recommendations from medical experts are becoming muffled by the avalanche of toxic content posted on social media platforms. This high level of toxic content prevents the dissemination of important and time-sensitive information and jeopardizes the sense of community that online social networks (OSNs) seek to cultivate. In this article, the authors present techniques to analyze toxic content and actors that propagated it on YouTube during the initial months after COVID-19 information was made public. The dataset consists of 544 channels, 3,488 videos, 453,111 commenters, and 849,689 comments. They applied topic modeling based on Latent Dirichlet Allocation (LDA) to identify dominant topics and evolving trends within the comments on relevant videos. They conducted social network analysis (SNA) to detect influential commenters, and toxicity analysis to measure the health of the network. SNA allows them to identify the top toxic users in the network, which led to the creation of experiments simulating the impact of removal of these users on toxicity in the network. Through this work, they demonstrate not only how to identify toxic content related to COVID-19 on YouTube and the actors who propagated this toxicity, but also how social media companies and policy makers can use this work. This work is novel in that they devised a set of experiments in an attempt to show how if social media platforms eliminate certain toxic users, the overall health of the network can be improved by reducing the overall toxicity level.",Yes,16,01.09.2021,03.12.2022,Yes,Yes,"To set a context for our context for our work, we give an operational definition of toxicity as ""the usage of rude, disrespectful, or unreasonable language that will likely provoke or make another user leave a discussion."" (Obadimu, Mead, Hussain, et al., 2019).",No,,No,,,,,COVID-19 discourse dataset,1,1,YouTube,Toxicity,English,N/A,XX.01.2020 - XX.05.2020 (January 2020 through early May 2020),N/A,COVID-19,"The authors leveraged the YouTube search Data APIv3 to extract channels, videos, and comments using the following keywords: coronavirus, corona, virus, COVID19, COVID, and outbreak. The time frame of the dataset spans the period from January 2020 through early May 2020. To reduce noise in extracted data, several data processing steps were performed including data formatting, data standardization and data normalization. They used the Python programming language to transform the data into a standard and normalized format by dropping the missing values, ensuring that every column is assigned to the correct data type, etc. The dataset consisted of 544 channels, 3,488 videos, 453,111 commenters, and 849,689 comments (826,569 of which were unique comments). The comments were primarily from videos that were categorized as ""News & Politics"" (94%), followed by ""Entertainment"" (0.08%).",Not discussed,,automated,Yes,"The authors compute toxicity scores for each comment in the dataset. To accomplish this, they leveraged a classification tool called Perspective API, which was developed by Google's Project Jigsaw and 'Counter Abuse Technology' teams, and has shown to be highly effective in other case studies (Pavlopoulos et al., 2019; Han & Tsvetkov, 2020). This model uses a Convolutional Neural Network (CNN) trained with word vector inputs to determine whether a comment could be perceived as ""toxic"" to a discussion. The API returns a probability score between 0 and 1, with higher values indicating a greater likelihood of the toxicity label being applied to the comment. Since toxicity scores are based on a probability score of 0 to 1, the authors focused their analysis on toxic comments that are 0.5 or greater in order to gain a deeper understanding of high toxic content. This step in our methodology was executed in order to accomplish the primary goal of this study, which was to show how the reduction of the toxicity level on an OSN can improve the overall health of the network.",Not discussed,,"826,569 unique comments",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
458,No,"(Li et al., 2021)",COVID-HateBERT: a Pre-trained Language Model for COVID-19 related Hate Speech Detection,2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA),USA,"Existing methods only achieve high performance when the training and testing data come from the same data distribution. The models trained on the traditional hateful dataset cannot fit well on COVID-19 related dataset. Meanwhile, manually annotating the hate speech dataset for supervised learning is time-consuming. Here, the authors propose COVID-HateBERT, a pre-trained language model to detect hate speech on English Tweets to address this problem. They collect 200M English tweets based on COVID-19 related hateful keywords and hashtags. Then, they use a classifier to extract the 1.27M potential hateful tweets to re-train BERT-base. They evaluate their COVID-HateBERT on four benchmark datasets. The COVID-HateBERT achieves a 14.8%-23.8% higher macro average F1 score on traditional hate speech detection comparing to baseline methods and a 2.6%-6.73% higher macro average F1 score on COVID-19 related hate speech detection comparing to classifiers using BERT and BERTweet, which shows that COVID-HateBERT can generalize well on different datasets.",Yes,3,13.12.2021,03.12.2022,Yes,Yes,"Hate speech is commonly defined as languages that instigate hate or violence to a group of people, usually targeting their nationalities, race, gender, religion, sexual orientation, or other.",No,,No,,,,,"In-house COVID-19 dataset;

three publicly available Twitter hate datasets: Waseem & Hovy, HatEval 2019, COVID-HATE",4,1,Twitter,Hate Speech,English,N/A,01.01.2020 - 01.04.2021,N/A,COVID-19,"To collect tweets with potential hateful content related to COVID-19, first, the authors collected real-time tweets using Twitter Streaming API with two hashtags about COVID-19. We started with the essential hashtags ""coronavirus"" and ""Covid-19"". Then we found six hot topics with many discussions towards different groups or individuals, such as discussion about Asians, Trump, or ""Boomerremover"", which means old people who have a higher risk of being infected by a coronavirus. Other tweets might discuss Mask or Fauci. In each topic, they could find several hateful hashtags and other COVID-19 related hashtags. For example, ""Chinavirus"" is commonly used in tweets about Asians, while ""Trumpvirus"" is created for potential hate toward Trump. Next, they explored new hashtags in each topic by checking the frequency of new hashtags. For example, they began with hashtags ""Chinavirus"" and ""Chinesevirus"" in the Asian-hate topic and searched for other hashtags that frequently appeared with existing hashtags, such as ""wuhanvirus"" and ""kungflu"". Then they added them to the hashtags set and collected tweets that contained these hashtags. As a result, they obtained 41 hashtags about ten different types of hate towards individuals or groups and 80 COVID-19 related hashtags. (The 121 COVID-19 related hashtags are available in Table 1) Since Twitter would provide only one percent of real-time tweets, they could not get all these hashtags from real-time tweets. So they used a tool named ""snscrape"" to collect all the past tweets id related to hashtags and then used the Twitter official API to get the content of the tweets. Based on this method, they collected 200M tweets with 121 COVID-19 related hashtags from Jan 1, 2020, to Apr 1, 2021.",Not discussed,,manual,Yes,"These (selected) tweets totaling 1,679 are labeled as hate and non-hate. The annotation code considers both the context and target of a tweet, since hateful tweets may not have slurs, and hateful keywords do not necessarily make tweets hateful. For example, tweets that combine an Asian location or a person's name with a virus are labeled as hateful tweets. Three graduate students label hundreds of tweets from different subsets of 1,679 tweets for three rounds and develop additional annotation code after each round during the annotation process. An expert will make the final classification if tweets are labeled differently by the students. Finally, the in-house dataset contains 554 hateful tweets and 1,125 non-hateful tweets.",Yes,"The authors use a classifier to extract the potential hateful tweets from those 200M tweets to train the task-specific language model. The classifier, built by [10], was trained on their annotated dataset of 2,319 COVID-19 related hateful tweets. They represented tweets using linguistic features such as the number of characters and words, hashtags, and tweet embeddings (BERT). They (authors of [10]) trained Logistic Regression classifiers and conducted five-fold cross-validation on the three-class classification task.

The authors select the tweets with the ""Hate"" label and finally get 1.27M tweets. They then use these 1.27M hateful tweets to train their COVID-HateBERT.

They use an open-source tool Perspective to select the tweets whose score is greater than 0.8 (for annotation).",1679,"5-folds cross-validation, or use different datasets as the training set and testing set","5-folds cross-validation, or use different datasets as the training set and testing set",Three graduate students; An expert will make the final classification if tweets are labeled differently by the students.,Yes,"Three graduate students label hundreds of tweets from different subsets of 1,679 tweets for three rounds and develop additional annotation code after each round during the annotation process.",Not discussed,,Specific,"race, gender, nationality",Targeted,"nationality, race, gender, religion, sexuality, other",Yes,Yes,"Race, Ethnicity, Gender, Nationality","race, gender, nationality",Yes,"Asian, Immigrants, Women",No,,,No,Human
460,No,"(Huang et al., 2020)",Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition,Proceedings of the Twelfth Language Resources and Evaluation Conference,"USA, Canada","Existing research on fairness evaluation of document classification models mainly uses synthetic monolingual data without ground truth for author demographic attributes. In this work, the authors assemble and publish a multilingual Twitter corpus for the task of hate speech detection with inferred four author demographic factors: age, country, gender and race/ethnicity. The corpus covers five languages: English, Italian, Polish, Portuguese and Spanish. They evaluate the inferred demographic labels with a crowdsourcing platform, Figure Eight. To examine factors that can cause biases, they take an empirical analysis of demographic predictability on the English corpus. They measure the performance of four popular document classifiers and evaluate the fairness and bias of the baseline classifiers on the author-level demographic attributes.",Yes,57,11.05.2020,03.12.2022,Yes,Yes,"Hate speech. ""In this work, we assemble and publish a multilingual Twitter corpus for the task of hate speech detection with inferred four author demographic factors: age, country, gender and race/ethnicity. """,No,,Yes,Other,upon request,https://github.com/xiaoleihuang/Multilingual_Fairness_LREC  Email xiaolei.huang@memphis.edu to get anonymized text corpus.,Yes,Multilingual Twitter Corpus (assembled from 7 published Twitter hate speech Twitter datasets),1,1,Twitter,Hate Speech,"English, Italian, Polish, Portuguese, Spanish",N/A,N/A,N/A,N/A,"The authors assemble the annotated datasets for hate speech classification. To narrow down the data sources, they limit our dataset sources to the unique online social media site, Twitter. They have requested 16 published Twitter hate speech datasets, and finally obtained 7 of them in five languages. By using the Twitter streaming API, they collected the tweets annotated by hate speech labels and their corresponding user profiles in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 2019), Portuguese (Fortuna et al., 2019), and Spanish (Basile et al., 2019). They binarize all tweets' labels (indicating whether a tweet has indications of hate speech), allowing to merge the different label sets and reduce the data sparsity.",Yes,"To anonymize user information, the authors hash user and tweet ids and then replace hyperlinks, usernames, and hashtags with generic symbols (URL, USER, HASHTAG).  Privacy Considerations: “To facilitate the study of classification fairness, we will publicly distribute this anonymized corpus with the inferred demographic attributes including both original and binarized versions. To preserve user privacy, we will not publicize the personal profile information, including user ids, photos, geocoordinates as well as other user profile information, which were used to infer the demographic attributes. We will, however, provide inferred demographic attributes in their original formats from the Face++ and Google Maps based on per request to allow wider researchers and communities to replicate the methodology and probe more depth of fairness in document classification.”",automated,Yes,"The authors consider four user factors of age, race, gender and geographic location. For location, they inference two granularities, country and US region, but only experiment with the country attribute. While the demographic attributes can be inferred through tweets (Volkova et al., 2015Davidson et al., 2019), they intentionally exclude the contents from the tweets if they infer these user attributes, in order to make the evaluation of fairness more reliable and independent. If users were grouped based on attributes inferred from their text, then any differences in text classification across those groups could be related to the same text. Instead, they infer attributes from public user profile information (i.e., description, name and photo).

Age, Race, Gender: They infer these attributes from each user's profile image by using Face++ (https://www. faceplusplus.com/), a computer vision API that provides estimates of demographic characteristics. Empirical comparisons of facial recognition APIs have found that Face++ is the most accurate tool on Twitter data (Jung et al., 2018) and works comparatively better for darker skins (Buolamwini and Gebru, 2018). For the gender, they choose the binary categories (male/female) by the predicted probabilities. They map the racial outputs into four categories: Asian, Black, Latino and White. They only keep users that appear to be at least 13 years old, and they save the first result from the API if multiple faces are identified. They experiment and evaluate with binarization of race and age with roughly balanced distributions (white and nonwhite, &ltmedian vs. elder age) to consider a simplified setting across different languages, since race is harder to infer accurately.

Country: The country-level language variations can bring challenges that are worth to explore. They extract geolocation information from users whose profiles contained either numerical location coordinates or a well-formatted (matching a regular expression) location name. They fed the extracted values to the Google Maps API (https: //maps.googleapis.com) to obtain structured location information (city, state, country). They first count the main country source and then binarize the country to indicate if a user is in the main country or not. For example, the majority of users in the English are from the United States (US), therefore, they can binarize the country attributes to indicate if the users are in the US or not.",Not discussed,,"73163 (64,067 English + 3,810 Italian + Polish 86 + Portuguese 600 + Spanish 4,600)",70% (~51214),15% (~10974),N/A,Not discussed,,Not discussed,,Specific,"age, nationality, gender, race",Targeted,"age, nationality, gender, race",No,Yes,"Age, Country, Gender, Race","age, nationality, gender, race",No,,No,,,No,Human
462,No,"(Devyatkin et al., 2019)",EXTREMIST TEXT DETECTION IN SOCIAL WEB,IADIS International Conference Web Based Communities 2019 (part of MCCSIS 2019),Russia,"This paper considers the problem of extremist text detection in Russian social media. The authors propose models and methods for identification of extremist text in Russian, which apply deep linguistic parsing and statistical processing of texts. They also present the dataset of terrorist, religious hate, racism and other radical texts in Russian and results of experiments on this dataset. It was shown, that low-dimensional psycholinguistic and semantic features of texts allow detecting extremist texts with quite good performance while lexical features allow recognizing topics of the detected extremist texts.",Yes,1,16.07.2019,03.12.2022,Yes,Yes,"By extremist texts we mean written materials that foment hatred toward a particular social or ethnic group, race, religion; vindicate terrorism, propagate superiority of a particular social or ethnic group, race, religion, etc.",Yes,"extremist texts: Terrorism, Ideological texts, Religious hatred, Separatism, Nationalism, Aggression, Fascism",No,,,,,DATASET OF EXTREMIST TEXTS,1,1,"web-sites of outlawed in Russia organizations, political blogs, sites of opposition members, social networks about religion (for neutral category)","Extremism, Cyber Extremism",Russian,N/A,N/A,N/A,N/A,"The authors created a text corpus for the research. It includes not only extremist texts, but also topically similar texts that do not fall under the category of extremism (referred by the authors as neutral): political blogs, sites of opposition members, news, texts and posts from social networks about religion. When choosing texts for the corpus, they were guided by the law ""On Countering Extremist Activities"" and experts' opinion on what can be considered extremism and what cannot. The corpus includes 493 manually collected texts (650 000 words), 368 of them are extremist texts.",Not discussed,,manual,Yes,"The concept of extremism is broad and includes different types of offenses. Therefore, they classified all text on seven categories:

1\. Terrorism (27 texts, 3,296 words) - materials from web-sites of outlawed in Russia organizations (the Islamic State, Hizb ut-Tahrir, etc.), where their ideology is propagated.

2\. Ideological texts (26 texts, 21,131 words) affirms the superiority of some religion over others, or spreads false interpretations of sacred books.

3\. Religious hatred (55 texts, 16,697 words) - texts that calls for cruelty against representatives of other religions, forming a negative image of other religions, attributing dangerous intentions to persons of another religion.

4\. Separatism (7 texts, 852 words) - materials that disseminate the idea of separating some regions from the Russian Federation, and contain insults and threats against ethnic groups residing there.

5\. Nationalism (208 texts, 19,399 words) - texts that affirm the initial hostility of a certain ethnic group, call for the physical destruction of its representatives, require restrictions of their rights and freedoms in Russia.

6\. Aggression and calls for insurgency (43 texts, 6,757 words) - calls for unauthorized rallies and riots, deposing the government, threaten government officials and their families.

7\. Fascism (13 texts, 2,059 words) - texts that justify neo-fascism and genocide, discuss forbidden fascist books, etc.",Not discussed,,493 texts,5-fold cross-validation,5-fold cross-validation,N/A,Not discussed,,Not discussed,,Specific,"political, religion, race, nationality, other",Targeted,"race, religion",Yes,Yes,"Ideology, Religion, Ethnicity, etc  (see the 7 categories: Terrorism, Ideological texts, Religious hatred, Separatism, Nationalism, Aggression, Fascism)","political, religion, race, nationality, other",No,,No,,,No,Human
464,No,"(Chiril et al., 2020)",He said “who's gonna take care of your children when you are at ACL?”: Reported Sexist Acts are Not Sexist,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,France,"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. The authors propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet's vectorial representations (word embeddings, linguistic features, and various generalization strategies). The results are encouraging and constitute a first step towards offensive content moderation.",Yes,21,05.07.2020,03.12.2022,Yes,Yes,"Sexism is prejudice or discrimination based on a person's gender. It is based on the belief that one sex or gender is superior to another. Sexist hate speech is a message of inferiority usually directed against women at least in part because they are women, some authors refer to it as: ""words that wound"" (Matsuda et al., 1993Waldron, 2012Delgado et al., 2015). As defined by the Council of Europe, ""The aim of sexist hate speech is to humiliate or objectify women, to undervalue their skills and opinions, to destroy their reputation, to make them feel vulnerable and fearful, and to control and punish them for not following a certain behaviour"".

A misogynistic ideology does not necessarily rely on people's beliefs, values, and theories, and can be seen as a mechanism that has the role of upholding the social norms of patriarchies (i.e., represents the ""law enforcement"" branch of a patriarchal order) by differentiating between good women and bad women and punishing those who take (or attempt to take) a man's place in society. Considering these definitions, misogyny is a type of sexism.

We distinguish different types of sexist content depending on the impact on the addressee (called 'perlocutionary force'): sexist hate speech directly addressed to a target, sexist descriptive assertions not addressed to the target, or reported assertions that relate a story of sexism experienced by a woman.",Yes,"direct, descriptive, reporting, non-sexist, no decision",Yes,Github,,https://github.com/patriChiril/An-Annotated-Corpus-for-Sexism-Detection-in-French-Tweets,Yes,An Annotated Corpus for Sexism Detection in French Tweets,1,1,Twitter,Sexism,French,XX.10.2017 - XX.05.2018,N/A,N/A,N/A,"The corpus is new and contains French tweets collected between October 2017 and May 2018. In order to collect sexist and non sexist tweets, the authors followed Anzovino et al. (2018) approach using: (i) a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc., (ii) the names of women/men potentially victims or guilty of sexism (mainly politicians), (iii) specific hashtags to collect stories of sexism experiences: #balancetonporc, #sexisme, #sexiste, #SexismeOrdinaire, #EnsembleContreLeSexisme, #payetashnek, #payetontaf, etc. The tweets collected with these hashtags may contain reported sexist acts towards both men and women. Thus, they collected around 205, 000 tweets, among which about 70, 000 contain the specific hashtags.",Not discussed,,manual,Yes,"Given a tweet, annotation consists in assigning it one of the following five categories: direct, descriptive, reporting (the sexist content is a report of an experience or a denunciation of a sexist behaviour), non-sexist and no decision. A tweet is non sexist when it has no sexist content (it may contain a specific hashtag, but the content is not sexist). No decision refers to cases where the tweet lacks context, or when the sexist content is not in the text but only in a photo, video, or URL.

300 tweets have been used for the training of 5 annotators and then removed from the corpus. Then, 1,000 tweets have been annotated by all annotators so that the inter-annotator agreement could be computed. The authors noticed that the kappa scores between female annotators are very close to the one between male annotators. For these 1,000 tweets, the final labels have been assigned according to a majority vote.

Finally, a total of 11, 834 tweets have been annotated according to the guidelines after removing 1,053 tweets annotated as ""no decision"".",Not discussed,,"11, 834",9451,2383,Five annotators: master's degree students (3 female and 2 male) in Communication and Gender,Yes,"A total of 11, 834 tweets have been annotated according to the guidelines after removing 1,053 tweets annotated as ""no decision"".

Sexist content in directed assertions is explicitly addressed at a target, but contrary to other approaches cited above, the target can be a woman, a group of women or all women.

Descriptive assertions are not directed to an addressee: the target can be a woman, a group of women, or all women, it can be named but is not the addressee.

In reported assertions, the sexist content is a report of an experience or a denunciation of a sexist behaviour.

A tweet is non sexist when it has no sexist content.

No decision refers to cases where the tweet lacks context, or when the sexist content is not in the text but only in a photo, video, or URL.",Not discussed,,Specific,gender,Targeted,gender,Yes,Yes,Sexism,gender,Yes,Women,No,,,No,Human
465,No,"(Johnston & Weiss, 2017)",Identifying sunni extremist propaganda with deep learning,2017 IEEE Symposium Series on Computational Intelligence (SSCI),USA,"Sunni extremism poses a significant danger to society, yet it is relatively easy for these extremist organizations to spread jihadist propaganda and recruit new members via the Internet, Darknet, and social media. This paper discusses an approach that can assist with policing these sites, by automatically identifying a subset of web pages and social media content (or any text) that contains extremist content. The approach utilizes machine learning, specifically neural networks and deep learning, to classify text as containing “extremist” or “benign” (i.e., not extremist) content. This method is robust and can effectively learn to classify extremist multilingual text of varying length. This study also involved the construction of a high quality dataset for training and testing, put together by a team of 40 people (some with fluency in Arabic) who expended 9,500 hours of combined effort.",Yes,37,27.11.2017,03.12.2022,Yes,Yes,"Generally, something was considered Sunni extremist if it contained a clear call to violence in furtherance of extremist Sunni Islam, or asking others to support the same.",No,,Yes,Other,upon request,"Data will be shared, on request, for legitimate academic, governmental/security, or commercial purposes.",Yes,The dataset,1,1,"news articles, Wikipedia, paste site pages, terrorist magazines","Extremism, Sunni Extremist Propaganda","Arabic, English, Urdu, Pashto, Indonesian, Bengali, French, Xhosa, Persian, Other",N/A,N/A,N/A,N/A,"The dataset was principally composed of data from two ""paste"" sites, which are publicly known to be utilized by ISIL, AQ, and terrorist sympathizers to exchange and disseminate information and propaganda. The paste sites allow users to anonymously upload text and images and receive a unique URL for the material that the user can then disseminate to the target audience. Certain paste sites become favored by terrorists when administrators refuse to take down terrorist propaganda and communications. Each posting or ""paste"" was downloaded via a secure channel onto a server and saved as a text file.

To supplement the dataset, text from the English copies of Inspire, Dabig and Rumiyah (the first two being published by al-Qaeda and the latter being published by ISIL), as curated by the Clarion Project, were included in the extremist category. Further, existing datasets [7, 10] consisting of news articles on a range of subjects from tennis to the Syrian civil war were incorporated as benign material.

A great concern was that the algorithm would simply learn to recognize religious words, phrases, or grammatical constructions, and consequently not produce meaningful predictions. Accordingly, an English copy of the Quran was included in the benign category. The Quran was included as a benign document and preprocessed accordingly.",Not discussed,,manual,Yes,"Before creation of the dataset, a team of student researchers were recruited to assist with the manual classification of the multilingual web data. The recruitment was highly successful and a team of 40 personnel was formed. This team was responsible for making the ultimate decision on whether the materials reflected Sunni extremism. This team received some training by a former FBI employee with relevant counterterror experience, to help ensure consistency and correctness. Generally, something was considered Sunni extremist if it contained a clear call to violence in furtherance of extremist Sunni Islam, or asking others to support the same. Any materials specifically published by extremist groups, such as ISIL and AQ, were considered to be extremist by default. The team spent a total of about 9,500 person hours to collect and label the dataset.

Once data was collected, it was manually sorted into ""benign"" and ""extremist"" categories. Documents that were blank or contained incomprehensible (e.g. binary) content were excluded from the dataset. Any documents where the nature of the text was unclear were passed to the team of experts previously established. Any document that did not have articulable Sunni terrorist material was considered to be benign.",Not discussed,,"264,117 lines = 264,117 doc2vec vectors (13,458 files)",70% (~184882 vectors),20% (~52823 vectors),"A team of 40 personnel was formed, which included seven who were experts in Arabic and Farsi, and very knowledgeable about Islam and Middle Eastern culture",Not discussed,,Not discussed,,Specific,"religion, political, other",Targeted,religion,Yes,Yes,"Religion, etc (Sunni Extremist Propaganda)","religion, political, other",No,,No,,,No,Human
466,No,"(Alshehri et al., 2020)",Understanding and Detecting Dangerous Speech in Social Media,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","USA, Canada","Although several works have been performed on the related issue of detecting offensive and hateful language in online environments, dangerous speech such as physical threats has not previously been treated in any significant way. Motivated by these observations, the authors report their efforts to build a labeled dataset for dangerous speech: They manually curate a multi-dialectal dictionary of physical harm threats, use their lexicon to collect a large dataset of threatening speech from Arabic Twitter, and manually annotate a subset of the data for dangerous speech. They also exploit the dataset to develop highly effective models to detect dangerous content. Their best model performs at 59.60% macro F1, significantly outperforming a competitive baseline.",Yes,11,11.05.2020,03.12.2022,Yes,Yes,"We define dangerous language as a statement of an intention to inflict physical pain, injury, or damage on someone in retribution for something done or not. This definition excludes threats that do not reflect physical harm on the side of the receiver end of the threat. The definition also excludes “tongue in cheek” whose real intention is to tease.",No,,Yes,Github,,https://github.com/UBC-NLP/ara_dangspeech,No,Dangerous speech data; Offensive speech data (from Offensive Shared Task 2020),2,1,Twitter,"Dangerous speech, Threats",Arabic,Implicitly two weeks before 11.05.2020,N/A,N/A,N/A,"The authors came up with a list of 57 verbs in their basic form that can be used literally or metaphorically to indicate physical harm. The list covers the frequent verbs used in the threatening domain in Arabic. These verbs are used in one or more of the following varieties: Egyptian, Gulf, Levantine, Maghrebi, and MSA. Most of these verbs (n=50 out of 57) literally indicate physical harm. Examples are 'to stap' and 'to de-skin' (translation). The rest are used (sometimes metaphorically) to indicate threatening, such as '10 pluck' and  'to mark' usually with a body part such as ‘face' or 'head'. Finally, some of the verbs are used idiomatically, such as 'to drink someone's blood’ and ‘to erase/eliminate from the face of the earth'.

To be able to collect data, they used their manually curated list to construct threat phrases indicating physical harm such as ('I kill you') and ('He breaks him/it'). That is, each phrase consists of a physical harm verb, a singular or plural first or third person subject, and a plural or singular second or third person object. This gives the following pattern:

1st/3rd (SG/PL) + threat verb + 2nd/3rd (SG/PL)

Some of the phrases only differ on the basis of spelling due to dialectical variations. Manual search of some of the seed tokens in twitter suggests that patterns involving 3rd person subject are almost always not threats. Thus, the authors decided to limit the list of phrases to 'direct' dangerous threats, which are phrases involving a singular or plural first person subject and singular or plural second person object as follows:

Ist (SG/PL) + threat verb + 2nd (SG/PL)

Examples of these direct threats include ('We rape you') and ('I burn you'). Less dangerous threats such as (""We hurt you (all)"") and ('I push you') are also not considered. The motivation for not including these latter phrases even though they involve direct threats is that they indicate less danger and (more crucially) are more likely to be used metaphorically in Arabic. This resulted in a set of 286 direct and dangerous phrases, which constitute our list of 'dangerous' seeds.

They use the constructed 'dangerous' seed list to search Twitter using the REST API for two weeks resulting in a dataset of 2.8M tweets involving 'direct' threats as shown in Table 4l We then extract user ids from all users who contributed the REST API data (n = 399K users) and crawled their timelines (n = 705M tweets). They then acquire 107.5M tweets from the timelines, each of which carry one or more items from the 'dangerous' seed list. Combining these two datasets (the REST API dataset and dataset based on the timelines) results in a dataset consisting of 110.3M tweets. In this work, they focus on exploiting the REST API dataset exclusively, leaving the rest of the data to future research.",Not discussed,,manual,Yes,"The authors first randomly sample 1K tweets from the REST API dataset. Two of the authors annotated each tweet for being a threat ('dangerous') or not ('safe'). This sample annotation resulted in a Kappa (κ) score of 0.57, which is fair according to Lands and Koch's scale (Lands and Koch, 1977). The two annotators then held several discussion sessions to improve their mutual understanding of the problem and define some instructions as to how to label the data. The authors also added another random sample of 4K tweets (for a total size of 5K) to the annotation pool. After extensive revisions of the disagreement cases by the two annotators, the κ score for the whole dataset (5K) was found to be at 0.90. The annotated dataset has a total of 1, 375 tweets in the 'dangerous' class and 3, 636 in the 'non-dangerous class.",Yes,They first randomly sample 1K tweets from the REST API dataset and held discussions on instructions. They then added another random sample of 4K tweets for annotation.,"5, 011 tweets (then keep only tweets with at least two words, 4,445 tweets)",80% (~3556),10% (~445),Two of the authors,Yes,"The two annotators then held several discussion sessions to improve their mutual understanding of the problem and define some instructions as to how to label the data.

Their overall agreed-upon instructions for annotations include the following:

• Textual threats combined with pleasant emojis are not dangerous, as opposed to threat combined with less pleasant emojis.

• Mitigated threats with question marks or epistemic modals are dangerous unless they are combined with positive language or emojis.

• Threats related to sports are not dangerous. That is because it is common to use verbs like (""slaughter"") and (""rape"") among fans of rival teams to describe wins and losses.

• Ambiguous threats such as threats consisting of one word should be coded as 'dangerous’",Not discussed,,Specific,other,Non-targeted,N/A,Yes,No,,,No,,Yes,the receiver,To a person,No,Human
467,No,"(Sharif et al., 2020)",Detecting Suspicious Texts Using Machine Learning Techniques,Applied Sciences,"Bangladesh, Australia","People misuse the web media to disseminate malicious activity, perform the illegal movement, abuse other people, and publicize suspicious contents on the web. In this paper, a Machine Learning (ML)-based classification model is proposed (hereafter called STD) to classify Bengali text into non-suspicious and suspicious categories based on its original contents. A set of ML classifiers with various features has been used on the developed corpus, consisting of 7000 Bengali text documents where 5600 documents used for training and 1400 documents used for testing. The performance of the proposed system is compared with the human baseline and existing ML techniques. The SGD classifier ‘tf-idf’ with the combination of unigram and bigram features are used to achieve the highest accuracy of 84.57%.",Yes,23,18.09.2020,03.12.2022,Yes,Yes,"Analyzing the contents and properties of these definitions guided us to present a definition of suspicious Bengali text as follows: ""Suspicious Bengali texts are those texts which incite violence, encourage in terrorism, promote violent extremism, instigate political parties, excite people against a person or community based on some specific characteristics such as religious beliefs, minority, sexual orientation, race and physical disability.”",No,,No,,,,,Suspicious Bengali Text Dataset (SBTD),1,1,"Facebook, Online blogs, Websites, Newspapers",Suspicious texts,Bengali,N/A,N/A,N/A,N/A,"The authors have crawled a total of 7115 texts among them 3557 texts are S, and 3558 texts are NS. In the case of the suspicious class, 12.2% of source texts collected from the website (W), 12% data collected from the Facebook comment (FC), and 10.2% from the newspaper (N). Other sources such as Facebook posts (FP) and online blogs (OB) contributed 8.9% and 5.4% of text data. On the other hand, a significant portion of non-suspicious source texts collected from the newspapers (30.4%). A total of 7.8% of non-suspicious texts were collected from the OB, 5.6% from the W and 3.2% from the FC. A tiny portion of the texts was accumulated from various sources (such as novels and articles) in both classes. As the sources of the newspapers, the three most popular Bangladeshi newspapers are considered (such as the daily Jugantor, the daily Kaler Kontho, and the daily Prothom Alo) for accumulating the texts.",Not discussed,,manual,Yes,"Crowd-sourced data are initially labelled by five undergraduate students. The expert verifies the data labels. Majority voting will decide the initial label. If the non-suspicious count is greater than the suspicious count, then the initial label will be non-suspicious (0); otherwise, suspicious (1). After that, the expert will label the text as either non-suspicious or suspicious. If the initial label matches the expert label, then it will be the final label. When disagreement increased, the label marked with 'x', and the final label will be decided by a discussion between the experts and the annotators. If they agree on a label, it will be added to STD; otherwise, it will be discarded. It is noted that most of the disagreement was aroused for data of the suspicious class. Among 900 disagreements, only 5-7% disagreement occurs for non-suspicious classes. A small number of labels and their corresponding texts discarded from the crawled dataset due to the disagreement between experts and annotators. Precisely, 57 for the suspicious class and 58 for the non-suspicious class.",Not discussed,,7000,5600,1400,"Crowd-sourced data are initially labelled by five undergraduate students of Chittagong University of Engineering and Technology who have 8-12 months of experience in the BLP domain. They are also doing their undergraduate thesis on BLP and attended several seminars, webinars, and workshops on computational linguistics and NLP. The expert verifies the data labels. A professor or a PhD student having more than five years of experience or any researcher having vast experience in the BLP domain can be considered as an expert.",Not discussed,,Not discussed,,General,N/A,Targeted,"political, religion, sexuality, race, disability",Yes,No,,,No,,No,,,No,Human
468,No,"(Mullah & Zainon, 2022)",Improving detection accuracy of politically motivated cyber-hate using heterogeneous stacked ensemble (HSE) approach,Journal of Ambient Intelligence and Humanized Computing,"Malaysia, Nigeria","The researchers built a novel heterogeneous stacked ensemble (HSE) classifier for detecting politically motivated cyber-hate on Twitter. They constructed a heterogeneous stacked ensemble with eight baseline estimators. In the proposed methodology, they employed TF-IDF for feature vectorisation. They used Twitter API for data scraping to harvest tweets during a gubernatorial election in Nigeria for the training and evaluation of the stacked ensemble model. A total of 15,502 tweets were collected and after some preliminary cleaning, 5876 tweets were manually labelled as hate (1) or non-hate (0). The coded tweets contain 16.87% hate and 83.13% non-hate tweets. This article has three contributions – a critical review of literature on the detection of politically motivated cyber-hate, the building of a new dataset and the proposed stacked ensemble method. Two other public datasets (Kaggle and HASOC) were used to test the performance of their method. The F1-score metric was employed for comparison. The authors report that their method is better by 12% on the Kaggle and 4% on the HASOC datasets.",Yes,2,04.03.2022,03.12.2022,Yes,Yes,"Cyber-hate. "" In the past few decades, politically motivated cyber-hate has become an increasing concern for most countries across the globe (Ezeibe 2015). Electoral violence is considered a type of political unrest that is mainly characterized by its timing and intent (Fjelde 2020). Cyber-hate and cyberbullying are common electoral violence perpetrated during electioneer- ing on different SMPs (Adum et al. 2019).""",No,,Yes,Other,upon request,The dataset used in this study is available and can be accessed from the corresponding author on reasonable request.,Yes,The dataset (new); Two other public datasets (Kaggle and HASOC),3,1,Twitter,Hate Speech,English,N/A,N/A,N/A,"A gubernatorial election (September 19, 2020) in Edo State, Nigeria (hashtags: #EdoElection EdoDecides #APC #PDP #Obaseki #Godwin #Osagie #Ize #Iyamu)","The authors scraped tweets officially using Twitter application programming interfaces (APIs). For the collection of tweets that relate to political discourse in Nigeria, they used hashtags that are commonly employed to reference or link a discussion on Twitter within the election period. A gubernatorial election was conducted on September 19, 2020, in Edo State, Nigeria. Tweets with hashtags regarding this election were the focus of the data collection. The authors employed the following trending hashtags in our data scraping on Twitter: #EdoElection, EdoDecides, #APC, #PDP, #Obaseki, #Godwin, #Osagie, #Ize, #Iyamu. A total of 15,502 tweets were harvested based on the hashtags stated above. They also removed any duplicate tweets found. After the sorting, there were a total of 5876 tweets. They also carry out some preliminary cleaning of the tweets so that annotators can read with ease.",Not discussed,,manual,Yes,"The authors decided to use the binary classification. They applied the simple 'hard voting' principle to get the final label for each class. This means the majority label is final. To validate the work of the three annotators, they used Krippendorff's alpha coefficient.",Not discussed,,5876,Stratified 10-Fold cross-validation,Stratified 10-Fold cross-validation,N/A,Yes,“The instruction for the annotation was simple and clear.”,Not discussed,,Specific,political,Targeted,political,Yes,No,,,Yes,political-related (tweets that relate to political discourse in Nigeria),No,,,No,Human
471,No,"(Hahn et al., 2021)",Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),Germany,"Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, the authors identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. The authors observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.",Yes,0,06.08.2021,14.12.2022,Yes,Yes,"Profanity and hate speech often co-occur but are not equivalent, since not all hate speech is profane (e.g. implicit hate speech) and not all profanity is hateful (e.g. colloquialisms).",No,,No,,,,,"Fine-Tuning: German, English, French and Arabic portions of a large collection of tweets (www.archive.org/details/twitterstream);

Target Tasks:

1. distant task: DE-DT(GermEval-2019 subtask1), EN-DT(HASOC A), FR-DT(Charitidis et al. (2020)), AR-DT(Mubarak et al. (2017));
    
2. similar task: DE-ST(GermEval-2019 subtask2), EN-ST(HASOC B), AR-DT(Mubarak et al. (2017))
;",8,2,Twitter,"Hate Speech, Profanity","German, English, French, Arabic",N/A,2013 (for fine-tuning),N/A,N/A,"Word Lists: The minimal-pairs used in our experiments are derived from a German slur collection(www.hyperhero.com/de/insults.htm).

Fine-Tuning: The authors use the German, English, French and Arabic portions of a large collection of tweets(www.archive.org/details/twitterstream) collected between 2013-2018 to fine-tune BERT. For the German BERT model, all available German tweets are used, while the multilingual BERT is fine-tuned on a balanced corpus of 5M tweets per language. For validation during finetuning, they set aside 1k tweets per language.

Target Tasks: They test their sentence-level representations, which are used to train a neutral/ profane classifier on a subset of minimal pairs, on several hate speech benchmarks. For all four languages, they focus on a distant task DT (neutral/hate). For German, English and Arabic they additionally evaluate on a similar task ST (neutral profane), for which they removed additional classes (insult, abuse etc.) from the original finer-grained data labels and downsampled to the minority class (profane).

For German (DE), they use the test sets of GermEval-2019 (Struß et al., 2019) Subtask 1 (Other/Offense) and Subtask 2 (Other/Profanity) for DT and ST respectively. For English (EN), they use the HASOC (Mandl et al., 2019) Subtask A (NOT/HOF) and Subtask B (NOT/PRFN) for DT and ST respectively. French (FR) is tested on the hate speech portion (NonelHate) of the corpus created by Charitidis et al. (2020) for DT only, while Arabic (AR) is tested on Mubarak et al. (2017) for DT (Clean/Obscene+Offense) and ST (Clean/Obscene). As AR has no official train/test splits, they use the last 100 samples for testing. The training data of these corpora is not used.",Not discussed,,N/A,Yes,"Human Evaluation: To analyze the similarity and profanity of the substitutions, the authors perform a small human evaluation. Four annotators were asked to rate the similarity of profane words and their substitutions, and also to give a profanity score between 1 (not similar/profane) and 10 (very similar/profane) to words from a mixed list of slurs and substitutions.",Not discussed,,"1. distant task: DE-DT(3031), EN-DT(1153), FR-DT(6124), AR-DT(100);      2. similar task: DE-ST(222), EN-ST(186), AR-DT(24)",5-fold cross-validation,5-fold cross-validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,Yes,Human
472,No,"(Mussiraliyeva et al., 2021)",Bigram Based Deep Neural Network for Extremism Detection in Online User Generated Contents in the Kazakh Language,Advances in Computational Collective Intelligence,Kazakhstan,"This research examines the detection of extremist messages in online content in the Kazakh language. To do this, the authors have collected a corpus of extremist texts from open sources, developed a deep neural network based on bigrams for detecting extremist texts in the Kazakh language. The proposed model has shown high efficiency in comparison with classical methods of machine learning and deep learning.",Yes,1,27.09.2021,14.12.2022,Yes,Yes,Extremist,No,,No,,,,,the dataset,1,1,Vkontakte,Extremist,Kazakh,N/A,N/A,N/A,N/A,"The corpus is assembled on the basis of Kazakh-language posts of the Vkontakte social network platform. First of all, dictionaries of symbols denoting the positive and negative attitude of the author were compiled. In accordance with the written designation of emotions, a keyword search was performed and two collections were formed. These collections will be used for subsequent analysis of messages and identification of patterns of extremist texts. To form a collection of neutral messages, messages from news accounts of microblogs were taken.

The core of the software module is responsible for the interaction between user databases, text collection modules and their subsequent processing modules. Social system APIs return data via a web interface in JSON or XML format. To get the data from Vkontakte social network, the authors used VK API [20] that allows to get 1% of all data for research studies.",Not discussed,,N/A,Not discussed,,Not discussed,,N/A,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
473,No,"(Babakov et al., 2021)",Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company's Reputation,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,Russia,"Not all topics are equally “flammable” in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities. The authors define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. While toxicity in user-generated data is well-studied, they aim at defining a more fine-grained notion of inappropriateness. The core of inappropriateness is that it can harm the reputation of a speaker. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. They collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset. They also release pre-trained classification models trained on this data.",Yes,4,20.04.2021,14.12.2022,Yes,Yes,We define sensitive topic as a topic which has a high chance of yielding a discussion which can harm the speaker's reputation. …We define inappropriate message as a message on a sensitive topic which can frustrate the reader and/or harm the reputation of the speaker.,Yes,"divide the topics into three clusters: • Cluster 1: gambling, pornography, prostitution, slavery, suicide, social injustice, • Cluster 2: religion, terrorism, weapons, offline crime, online crime, politics, • Cluster 3: body shaming, health shaming, drugs, racism, sex minorities, sexism.",Yes,Github,,https://github.com/s-nlp/inappropriate-sensitive-topics,Yes,"two new datasets: (i) the dataset of sensitive topics and (ii) the appropriateness dataset;

a concatenation of two Russian Language Toxic Comments datasets released on Kaggle (Kaggle, 2019, 2020)",3,2,"2ch.hk, Otvet.Mail.ru","Inappropriate Messages, Sensitive Topics",Russian,N/A,N/A,N/A,N/A,"The authors retrieve the initial pool of texts from general sources with diverse topics, then filter them and hire crowd workers to label them for the presence of sensitive topics manually. They use the data from the following sources:

• 2ch.hk - a platform for communication in Russian similar to Reddit. The site is not moderated, suggesting a large amount of toxicity and controversythis makes it a practical resource for our purposes. They retrieve 4.7 million sentences from it.

• Otvet.Mail.ru - a question-answering platform that contains questions and answers of various categories and is also not moderated. They take 12 million sentences from it.",Not discussed,,"automated, manual",Yes,"Topic Labeling:

1.Crowdsourced Labeling:

The labeling is performed in a crowdsourcing platform Yandex. Toloka. It was preferred to other analogous platforms like Amazon Mechanical Turk because the majority of its workers are Russian native speakers.

The task of topic labeling is naturally represented as a multiple-choice task with the possibility to select more than one answer: the worker is shown the text and possible topics and is asked to choose one or more of them. They divide the topics into three clusters: • Cluster 1: gambling, pornography, prostitution, slavery, suicide, social injustice, • Cluster 2: religion, terrorism, weapons, offline crime, online crime, politics, • Cluster 3: body shaming, health shaming, drugs, racism, sex minorities, sexism. Cluster 1 is associated with undesirable behaviorcluster 2 deals with crimes, military actions, and their causescluster 3 is about the offense. However, this division is not strict and was performed to ease the labeling process.

Each cluster has a separate project in Yandex.Toloka. Every candidate text is passed to all three projects: they label each of them for all 18 topics.

Before labeling the examples, the authors ask users to perform training. It consists of 20 questions with pre-defined answers. To be admitted to labeling, a worker has to complete the training with at least 65% correct answers. In addition to that, they perform extra training during labeling. One of each ten questions given to a worker has a pre-defined correct answer. If a worker makes a mistake in this question, she is shown the correct answer with an explanation.

Likewise, they perform quality control using questions with pre-defined answers: one of ten questions given to the worker is used to control her performance. If the worker gives incorrect answers to more than 25% of control questions, she is banned from further labeling, and her latest answers are discarded. For the topic labeling task, the average performance of workers on control and training tasks was between 65 and 70%.

In addition to that, they control the speed of task accomplishment. If a user answers ten questions (one page of questions) in less than 20 seconds, this almost certainly indicates that she has not read the examples and selected random answers. Such workers are banned. To ensure the diversity of answers they allow one user to do at most 50 pages of tasks (500 tasks) per 12 hours.

Each sample is labeled in each project by 3 to 5 workers. They use dynamic overlap technique implemented in Toloka. An example is first labeled by the minimum number of workers. If they agree, their answer is considered truth. Otherwise, the example is given for extra labeling to more workers to clarify the true label. This allows separating the occasional user mistakes from inherently ambiguous examples.

They aggregate multiple answers into one score using the aggregation method by Dawid and Skene (1979). This is an iterative method that maximizes the probability of labeling taking into account the worker agreement, i.e. it trusts more the workers who agree with other workers often. The result of this algorithm is the score from 0 to 1 for each labeled example which is interpreted as the label confidence.

Besides the aggregation purposes, they use the confidence score as a measure of worker agreement. Since the low score of an example is the sign of either the ambiguity of this example or the low reliability of annotators who labeled it, they assume that the high confidence indicates that the task is interpreted by all workers in a similar way and does not contain inherent contradictions. The average confidence of labeling in their topic dataset is 0.995.

2.Automated Labeling:

After having collected almost 10,000 texts on sensitive topics, they were able to train a classifier that predicts the occurrence of a sensitive topic in the text. Although this classifier is not good enough to be used for real-world tasks, they suggest that samples classified as belonging to a sensitive topic with high confidence (more than 0.75 in the experiments) can be considered belonging to this topic. They perform an extra manual check by an expert (one of the authors) to eliminate mistakes. This method is also laborious, but it is an easier labeling scenario than the crowdsourcing task. Approving or rejecting a text as an entity of a single class is easier than classify it into one of six topics.

An alternative way of automated topic labeling is to take the data from specialized sources and select topic-attributed messages using a list of keywords inherent for a topic, i.e. words which definitely indicate the presence of a topic. This approach can give many false positives when applied to general texts because many keywords can have an idiomatic meaning not related to a sensitive topic. One such example can be the word ""addiction"" which can be used in entirely safe contexts, e.g. a phrase ""I'm addicted to chocolate"" should not be classified as belonging to the topic ""drugs"" However, when occurring in a specialized forum on addictions,10 this word almost certainly indicates this topic. They define a list of inherent keywords and select messages containing them from special resources related to a particular topic. They then manually check the collected samples.

The disadvantage of this approach is that they cannot handle multilabel samples. However, according to dataset statistics, only 15% of samples had more than one label. Given the limited time and budget, they decided to use this approach to further extend the dataset. The resulting sensitive topics dataset in the form we opensource it is the combination of all three approaches. Specific, nearly 11,000 samples were labeled in a fully manual manner either via crowdsourcing or by members of the authors’ team, the rest samples (nearly 14,500) were labeled via the described semi-automatic approaches.

Appropriateness Labeling:

Determining the appropriateness criteria explicitly turned out to be infeasible. Therefore, they rely on the inherent human intuition of appropriateness. They provide annotators with the following context: a chatbot created by a company produces a given phrase. They ask to indicate if this phrase can harm the reputation of the company. They also reinforce the annotators' understanding of appropriateness with the training examples. As in the topic labeling setup, here they ask the workers to complete the training before labeling the data. They also fine-tune annotators’ understanding of appropriateness with extra training during labeling.

Analogously to topic labeling, the appropriateness labeling is performed via Yandex.Toloka crowdsourcing platform. The crowdsourcing setup repeats the one they used in the topic labeling project. They perform training and quality control analogously to topic labeling. The primary sources of the samples passed to appropriateness labeling are the same as in the topic labeling setup (2ch.hk and Otvet. Mail.ru websites). Before handing texts to workers, they filter them as described in Topic Labeling and also perform extra filtering. They filter out all messages containing obscene language and explicit toxicity. (see sampling strategy for more details about filtering and pre-selection)

The further labeling process is performed analogously to topic labeling. They use the same training and quality control procedures and define the number of workers per example dynamically. To get the final answer, we use the same DawidSkene aggregation method. It aggregates the labels given by workers (0 and 1, which state for ""appropriate"" and ""inappropriate"") into a single score from 0 to 1. We interpret this score as the appropriateness level, where the score in the interval [0, 0.2] indicates appropriate sentences, the score in [0.8, 1] means that the sentence is inappropriate, and other scores indicate ambiguous examples.",Yes,"Topic Labeling:

To pre-select the data for topic labeling, the authors manually create large sets of keywords for each sensitive topic. They first select a small set of words associated with a topic and then extract semantically close words using pre-trained word embeddings from Rus Vectôres(https://rusvectores.org/ru/associates/) and further extend the keyword list (this can be done multiple times). In addition to that, for some topics they use existing lists of associated slang on topical websites, e.g. drugs and weapons.'

Appropriateness Labeling:

Before handing texts to workers, they filter them as described in Topic Labeling and also perform extra filtering. They filter out all messages containing obscene language and explicit toxicity. They identify toxicity with a BERT-based classifier for toxicity detection. They fine-tune ruBERT model (Kuratov and Arkhipov, 2019) on a concatenation of two Russian Language Toxic Comments datasets released on Kaggle (Kaggle, 2019, 2020). They filter out sentences which were classified as toxic with the confidence greater than 0.75. As mentioned above, toxicity is beyond the scope of our work, because it has been researched before. Therefore, they make sure that messages which can be automatically recognized as toxic are not included in this dataset.

Inappropriate messages in their formulation concern one of the sensitive topics. Therefore, they pre-select data for labeling by automatically classifying them with sensitive topics. They select the data for labeling in the following proportion:

• 1/3 of samples which belong to one or more sensitive topic with high confidence (0.75), • 1/3 of samples classified as sensitive with medium confidence (0.3 c &lt0.75). This is necessary in case if multilabel classifier or crowd workers captured uncertain details of sensitive topics, • 1/3 random samples - these are used to make the selection robust to classifier errors.","25,679 (Topic dataset, within which 9,946 samples were manually labelled, the others were labelled via automated approaches); 82,063 (Appropriateness dataset, within which 8,687 samples have manually assigned topic labels, the others were labelled via automated approaches)","85% (~21827 in topic dataset, ~69754 in appropriateness dataset)","15% (~3852 in topic dataset, ~12309 in appropriateness dataset)",Crowdsourcing workers on Yandex.Toloka and members of the authors’ team,Not discussed,,Not discussed,,Specific,"race, gender, sexuality, religion, political, body, other",Non-targeted,N/A,Yes,Yes,"Race, Gender, Sexuality, Religion, politics, etc (Cluster 1: gambling, pornography, prostitution, slavery, suicide, social injustice, • Cluster 2: religion, terrorism, weapons, offline crime, online crime, politics, • Cluster 3: body shaming, health shaming, drugs, racism, sex minorities, sexism)","race, gender, sexuality, religion, political, body, other",No,,No,,,No,Human
474,No,"(Beddiar et al., 2021)",Data expansion using back translation and paraphrasing for hate speech detection,Online Social Networks and Media,Finland,"Although, automatic detection of offensive content using deep learning approaches seems to provide encouraging results, training deep learning-based models requires large amounts of high-quality labeled data, which is often missing. In this regard, the authors present in this paper a new deep learning-based method that fuses a Back Translation method, and a Paraphrasing technique for data augmentation. Their pipeline investigates different word-embedding-based architectures for classification of hate speech. The back translation technique relies on an encoder–decoder architecture pre-trained on a large corpus and mostly used for machine translation. In addition, paraphrasing exploits the transformer model and the mixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are compared to seek enhanced classification results. They evaluate their proposal on five publicly available datasets; namely, AskFm corpus, Formspring dataset, Warner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The performance of the proposal together with comparison to some related state-of-art results demonstrate the effectiveness and soundness of their proposal.",Yes,23,23.06.2021,14.12.2022,Yes,Yes,"One predominant kind of online hate speech is cyberbullying. The latter refers to intentional, cruel, and repeated behavior among peers, using electronic media and communication technologies.",No,,No,,,,,"five publicly available datasets: AskFm, Formspring, OLID 2019, Warner and Waseem, and Wikipedia toxic comments dataset, and their expanded versions",10,5,"AskFm, FormSpring, Twitter, Wikipedia","Cyberbullying, Hate Speech, Offensiveness, Toxicity",English,N/A,N/A,N/A,N/A,"The authors utilized five publicly available datasets: AskFm, Formspring, OLID 2019, Warner and Waseem, and Wikipedia toxic comments dataset.

This paper advocates a combination of back translation and paraphrasing techniques for data augmentation procedure.

Back translation augmentation relies on translating text data to another language and then translating it back to the original language. This technique allows generating textual data of distinct wording to original text while preserving the original context and meaning. The authors exploit the back translation technique provided in two previous works to generate new sentences close in meaning to the original sentences. This technique was used for neural machine translation at a large scale and yielded promising results. It is based on a pre-trained transformer model, where six blocks for both encoder and decoder were used. Back-translated sentences based on sampling and noised beam search were generated by translating original data into German and back into English. The model was trained on data from the WMT'18 English-German news translation task [431 where the Moses tokenizer was used in line with a joint source and target Byte-Pair Encoding [44]. To measure the closeness between the reference human translation and the machine translation, BLEU metric was used. Synthetic data is then concatenated to original sentences to create larger datasets.

Paraphrasing using back translation can yield useful equivalent representations of the original text as well. It aims at rewriting the original text in different words without changing its meaning by passing through an intermediate language. This can be used for many purposes such as text simplification, document summarization, data augmentation and information retrieval. The authors exploit machine translation models to translate English text into French and back to English (round-trip translation). The French-English translation model is based on a mixture of experts as provided by a previous work. The generated paraphrases are then concatenated to the original sentences to create an initial expanded dataset. Finally, the data constructed from the back translation augmentation and the data generated from the paraphrasing augmentation are concatenated to obtain a final expanded dataset.",Not discussed,,N/A,Not discussed,,Not discussed,,AskFM (10k--;188.7K)Formspring (12k--;261K)OLID (13k--;265.2K)Warner and Waseem (1.8k--;41k)Wikipedia Toxic (9k--;172k)  \*format: (original --expanded),70% (of both original and artificially generated datasets),30% (of both original and artificially generated datasets),N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
476,No,"(Wu & Bhandary, 2020)",Detection of Hate Speech in Videos Using Machine Learning,2020 International Conference on Computational Science and Computational Intelligence (CSCI),USA,This research deals with classification of videos into normal or hateful categories based on the spoken content of the videos. The video dataset is built using a crawler to search and download videos based on offensive words that are specified as keywords. The audio is extracted from the videos and is converted into textual format using a Speech-to-Text converter to obtain a transcript of the videos. Experiments are conducted by training four models with three different feature sets extracted from the dataset. The models are evaluated by computing the specified evaluation metrics. The evaluated metrics indicate that Random Forrest Classifier model delivers the best results in classifying videos.,Yes,6,16.12.2020,14.12.2022,Yes,Yes,"Hate speech can be defined as speech used to express hate towards a person or a group of people based on characteristics such as race, religion, ethnicity, gender, nationality, disability and sexual orientation. It can be expressed as speech, writing, gesture or display that attacks individuals because of the group they belong to.",Yes,"Each of the videos is manually classified as normal, racist or sexist.",No,,,,,The video dataset,1,1,YouTube,"Hate Speech, Racism, Sexism",English,N/A,N/A,N/A,N/A,"The authors propose a hate speech detection system that focuses on spoken content of the videos. This system is designed using the following steps: 1. Extract audio from video 2. Convert audio into text format 3. Train machine learning models over text-based features and classify videos as normal or hateful

The dataset was manually collected from YouTube. Videos containing normal speech as well as videos containing offensive terms were selected to form the dataset.

The authors focused on videos with racist and sexist speech. To construct the dataset, they have implemented a crawler that searches for videos on YouTube and downloads them. To help in searching for the videos, they used the YouTube Data API. YouTube Data API contains libraries to search, delete, upload video content and so on. The API provides the search by keyword feature which searches and returns the videos whose video title, channel name or description contain the given keyword. They used multiple offensive words such as racist rants, racial slur, sexist comments, sexism and so on, as keywords to the API which would return the video title as well as the unique video id.

Since the YouTube Data API does not have any provisions to download the videos, the Pytube library was used to download videos in mp4 format. Pytube is a python-based library which is specifically used to download videos from YouTube. The unique video ids generated, are passed to the downloader function and the video is downloaded. Each of the videos is manually classified as normal, racist or sexist.

To extract the audio content from the videos, they make use of the FFmpeg API. The FFmpeg API is a multimedia framework that allows users to encode, decode and convert media between different formats. Using this API, they can convert the videos into any audio format. They are primarily focusing on text-based features to train the machine learning models. Thus, once the audio is extracted, it must be converted into textual format. This can be done using the Speech-to-Text conversion APIs and frameworks that are readily available. For the purpose of their approach, they use the Google Cloud Speech-to-Text API. The Google Cloud Speech-to-Text API makes use of Neural Network models that enables users to convert audio files into textual scripts. The API requires the audio files to be of FLAC or LINEAR format. Thus, the mp4 videos are initially converted to FLAC format. Since the API requires the audio files to have only mono channel, these FLAC formatted audios with stereo channel are converted into mono channel.",Not discussed,,manual,Not discussed,,Not discussed,,300 videos,70% (~210),30% (~90),N/A,Not discussed,,Not discussed,,Specific,"race, gender, ",Targeted,"race, religion, gender, nationality, disability, sexuality",Yes,Yes,"Race, Sexism","race, gender, ",No,,No,,,No,Human
477,No,"(Bellan & Strapparava, 2018)",Detecting Inappropriate Comments to News,AI\*IA 2018 – Advances in Artificial Intelligence,Italy,"The computational analysis of inappropriate comments, posted in response to professional news-papers, is not well investigated yet. The most predictive linguistic and cognitive features were seldom been addressed, and inappropriateness was not investigated deeply. After collecting a new dataset of inappropriate comments, three classic machine learning models were tested over two possible representations for the data to fed in: normal and distorted. Text distortion technique, thanks to its ability to mask thematic information, enhanced classification performance resulting in the valuable ground in which extract features from. Lexicon based features showed to be the most valuable characteristics to consider. Logistic regression turned out to be the most efficient algorithm.",Yes,3,09.11.2018,14.12.2022,Yes,Yes,"Inappropriate comments, defined as deliberately offensive, off-topic, troll-like, or direct attacks based on religious, sexual, racial, gender, or ethnic posts…",No,,No,,,,,The Guardian news comments dataset (new); the cross-domain dataset (Twitter dataset from Davidson et al. and from Golbeck et al.),2,1,"The Guardian website, Twitter",Inappropriate Comments,English,01.11.2017 - 31.01.2018,01.11.2017 - 31.01.2018,N/A,N/A,"The authors opted for The Guardian to collect news and comments for investigating the phenomenon. The dataset was extracted from the collection of comments crawled on the newspaper website in a period of time starting from the beginning of November 2017 until the end of January 2018. They collected articles and comments posted among public available sections of the journal, checking for new comments daily. Because moderators delete inappropriate messages, a continuous check of its presence allowed them to label it in two distinct categories: appropriate comment and inappropriate comment. So, there was no need of any further operations in labeling the data, as well avoiding the annotator agreement issue highlighted by Wassem. The dataset was extracted from the collection of comments grabbed, and then balanced by keeping all the posts removed by the moderators, while the appropriate comments were randomly selected. Thus, they ended up with 1386 samples in total, 693 inappropriate and 693 appropriate comments.

As an additional experiment, they also tested the best models in a different context to check if their classifiers were able to generalize across domains. Because the context plays a major role in the scope of posting and in language's style of the message, they opted for the Twitter setting. They built up the cross-domain dataset by concatenating samples from the work of Davidson et al. [6] to those from Golbeck et al. [11] because both represent some forms of inappropriateness. In the Davidson's dataset the 5% of samples were representative of hate speech, while in the Golbeck's dataset the 15% of texts were examples of on-line harassment. The same policy applied for extraction of the first dataset, were performed hereso they ended with a balanced dataset for cross-domain experiments.",Not discussed,,manual,Not discussed,,Not discussed,,1386 comments,N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Targeted,"religion, sexuality, race, gender",Yes,No,,,No,,No,,,No,Human
479,No,"(Mouheb et al., 2018)",Detection of Offensive Messages in Arabic Social Media Communications,2018 International Conference on Innovations in Information Technology (IIT),United Arab Emirates,"This paper proposes a scheme for the detection of cyberbullying in Arabic social media streams. The proposed scheme detects cyberbullying comments based on a corpus of bullying and aggressive keywords. In addition, the bullying comments are classified according to their strength into three classes, namely mild, medium, and strong, using a weighted function. The authors evaluate the proposed scheme using real dataset, collected from Youtube and Twitter. The experiments show that the proposed scheme can accurately identify most of the bullying comments.",Yes,13,18.11.2018,14.12.2022,Yes,Yes,"Cyberbullying involves many forms of content, including posting aggressive and hurtful messages, rumors, pejorative comments, as well as disclosure of personal information, images or videos as a way of defaming, humiliating and making fun of the victims. In contrast to traditional bullying, cyberbullying does not only occur in particular places and times such as school hours, but instead can occur anytime and anywhere, even at home, where children should feel most safe.",Yes,Three classes of bullying based on the weights: Mild/Medium/Strong Bullying,No,,,,,1) YouTube Dataset; 2) Twitter Dataset,2,2,"YouTube, Twitter",Cyberbullying,Arabic,N/A,N/A,"Arab Gulf countries (For the Twitter dataset, the authors collected comments based on geographical location)",N/A,"(1) YouTube Dataset: The authors collected textual Arabic comments posted on YouTube videos along with their corresponding responses. More specifically, they chose videos with controversial and offensive topics such as race and culture. Comments on these videos are usually a rich source for bullying and offensive comments from users expressing their personal opinions about the subject of the videos. Moreover, since most of YouTube comments are not moderated, the comments, though offensive, remain available for reviewers on the social network. Using the YouTube Data API, they collected over 50,000 comments and answers. The collected comments were stored in CSV files, which provides an easy queryable format using python APIs. They create a CSV file including all comments of a YouTube video obtained through its URL. Although they target YouTube videos with Arabic content, few of the posted comments contain text in languages other than Arabic, mainly English and French languages. Other few comments contained text read in Arabic language but written using English letters and numbers. These comments were filtered in the preprocessing step.
(2) Twitter Dataset: They also experiment with another dataset of Arabic comments collected from Twitter social network. The social network makes 1% of its tweets publicly available for download. Using Twitter Streaming API, they collected comments based on geographical location (Arab Gulf countries) since they target Arabic language comments. They also gathered comments using hashtags related to offensive topics that are more likely to include bullying words. However, to validate our experiments, they also crawl a random set of tweets which is less likely to contain any offensive word. In total, this dataset contains 42,138 comments.",Not discussed,,automated,Yes,"The scheme to detect cyberbullying is based on word spotting. To this end, the authors use a list of offensive keywords that are most commonly used among Arab youth. Using the collected dataset, they manually examined a sample of around 2,000 comments to extract a list of bullying keywords. They selected words that are either offensive verbs, offensive adjectives or other aggressive words. Since the collected comments are related to controversial topics, they have seen that they indeed contain a number of offensive and bullying words. In total, they selected 264 offensive/offensive keywords that we labeled as indicative of cyberbullying communication. Each offensive word d in the keyword file is assigned a weight w, ranging from 1 to 41 being slightly offensive and 4 being very offensive. These weights will be used to identify the strength and the class of a cyberbullying comment.

After cleaning and preprocessing the comments, they break each comment into separate words. If a word is found to be in the list of offensive keywords, the comment is labeled as bullying. Additionally, they identify the strength of each bullying comment by assigning it a weight. The weight of a bullying comment is calculated as the sum of the weights of each offensive word found in the comment.

Additionally, in case an offensive word is repeated many times in a row or contains repetitive letters, they multiply the weight of the word by two factors, r1 and r2 as follows: • If a word is repeated three times or more in a row, they multiply the weight by a factor of r1=1.7 • If a word is repeated twice, they multiply the weight by a factor of r1=1.5.

If a word is only repeated once, they check if there are any repetitions in the letters of the word. In such case, they multiply the weight of the word by a factor r2, as follows: • If a certain letter is repeated twice, we multiply the weight by a factor r2=1.2. • If a certain letter is repeated three times, we multiply the weight by a factor r2=1.25. • If a certain letter is repeated four times or more, we multiply the weight by a factor r2=1.3. • If a word is not repeated nor it includes any letter repetitions, we consider its original weight.

After detecting the cyberbullying comments, they classify them according to their strength based on the weights calculated in the previous subsection. They define three classes of bullying as follows: • Mild Bullying if its weight falls within the range [1-5]. • Medium Bullying if its weight falls within the range [6-10]. • Strong Bullying if its weight is 11 or beyond.",Not discussed,,"~50,000 comments (YouTube Dataset); 42,138 comments (Twitter Dataset)",N/A,N/A,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
481,No,"(Xu et al., 2013)",An Examination of Regret in Bullying Tweets,Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,USA,"Social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. In this paper, the authors construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. They then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, they propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret.",Yes,47,09.06.2013,14.12.2022,Yes,Yes,Bullying tweets; Regret,Yes,"Author’s Role (in bullying): Accuser, Bully, Reporter, Victim or Other whether the tweet was deleted whether the tweet is teasing (written jokingly).",No,,,,,corpus of bullying tweets,1,1,Twitter,"Cyberbullying, Regret",English,31.07.2012 - 31.10.2012,31.07.2012 - 31.10.2012,N/A,N/A,"The authors adopt the procedure used in (Xu et al., 2012) to obtain bullying traces; each identified trace contains at least one bullying related keyword and passes a bullying-or-not text classifier. The data was collected in realtime using the Twitter streaming API; once a tweet is collected, they query its url (https://twitter.com/ USERID/status /TWEETID) at regular intervals and infer its status from the resulting http response code. They interpret an HTTP 200 response as an indication a tweet still exists and an HTTP 404 response, which indicates the tweet is unavailable, as indicating deletion. A user changing their privacy settings can also result in an HTTP 403 response; they do not consider this to be a deletion. Other response codes, which appear quite rarely, are treated as anomalies and ignored. All non HTTP 200 responses are retried twice to ensure they are not transient oddities. To determine when a tweet is deleted, the authors attempted to access each tweet at time points T_i = 5 × 4^i minutes for i = 0, 1...7 after the creation time. These roughly correspond to periods of 5 minutes, 20 minutes, 1.5 hours, 6 hours, 1 day, 4 days, 2 weeks, and 2 months. While they assume that user deletion is the main cause of a tweet becoming unavailable, other causes are possible such as the censorship of illegal contents by Twitter (Twitter, 2012). The sample data was collected from July 31 through October 31, 2012 and contains 522,984 bullying traces. Because of intermittent network and computer issues, several multiple day data gaps exist in the data. To combat this, they filter our data to include only tweets of unambiguous status. If any check within the 20480 minutes (about two weeks) interval returns an HTTP 404 code, the tweet is no longer accessible and they consider it deleted. If the 20480 minute or 81920 minute check returns an HTTP 200 response, that tweet is still accessible and they consider it surviving. The union of the surviving and deleted groups formed their cleaned dataset, containing 311,237 tweets in total.",Yes,"Anonymization was done following the preprocessing procedure in (Xu et al., 2012)",automated,Yes,"Author's Role: Participants in a bullying episode assume well-defined roles which dramatically affect the viewpoint of the author describing the event. The authors trained a text classifier to determine author role (Xu et al., 2012), and used it to label each bullying trace in the cleaned corpus by author role: Accuser, Bully, Reporter, Victim or Other.

Teasing: Many bullying traces are written jokingly. They built a text classifier to identify teasing bullying traces (Xu et al., 2012) and applied it to the cleaned corpus.",Not discussed,,"311,237 tweets",10-fold cross validation (with an inner 5-fold cross validation on the training portion),10-fold cross validation,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
483,No,"(Rizwan et al., 2020)",Hate-Speech and Offensive Language Detection in Roman Urdu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),Pakistan,"In this study, the authors: (1) Present a lexicon of hateful words in Roman Urdu (RU), (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. The authors conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",Yes,26,16.11.2020,14.12.2022,Yes,Yes,"Hate Speech; Offensive Content. ""On one hand, this has resulted in exchanges of ideas and fostered relationships, while on the other, it is exploited to spread, incite, promote, or justify hatred, violence, and discrimination against users based on their gender, religion, race, affiliation with certain groups, and views related to certain events or subjects (e.g., politics) through hateful, offensive, derogatory, or obscene language.""",Yes,"coarse-grained classification: Hate-Offensive content, Normal content fine-grained classification: Abusive/Offensive, Sexism, Religious Hate, Profane,  Normal",Yes,Github,,https://github.com/haroonshakeel/roman_urdu_hate_speech,Yes,Roman Urdu Hate-Speech and Offensive Language Detection (RUHSOLD); corpus of random and hate-speech tweets (~4.7m),2,2,Twitter,"Hate Speech, Offensiveness",Urdu,N/A,N/A,N/A,N/A,"The authors have constructed our own lexicon of hateful words (by searching for such keywords online and interviewing people). This lexicon consists of abusive and derogatory terms along with slurs or terms pertaining to religious hate and sexist language. Using this lexicon along with a separate collection of RU common words, they search and collect 20, 000 tweets and perform a manual preliminary analysis to find new slang, abuses, and identify frequently occurring common terms. The choice to add common RU words is made in order to extract random inoffensive tweets and the tweets that are offensive but do not contain any offensive words. They discard words or terms for which the number of extractable tweets are too few. Using this updated lexicon we search and collect 50, 000 new tweets.",Not discussed,,manual,Yes,"RUHSOLD is manually labeled by three independent annotators. During the annotation process, all conflicts are resolved by a majority vote among three annotators. Tweets on which a consensus cannot be reached or that are reckoned to provide insufficient information for labeling are discarded and replaced by new randomly sampled tweets from the data collection. We develop the gold-standard for two sub-tasks. First sub-task is based on binary labels of Hate-Offensive content and Normal content (i.e., inoffensive language). These labels are self-explanatory. The authors refer to this sub-task as ""coarse-grained classification"". Second sub-task defines Hate-Offensive content with four labels at a granular level. These labels are the most relevant for the demographic of users who converse in RU and are defined in related literature. They refer to this sub-task as ""fine-grained classification"". The objective behind creating two gold-standards is to enable the researchers to evaluate the hate-speech detection approaches on both easier (coarse-grained) and challenging (fine-grained scenarios).",Yes,"From this updated tweet base (50,000 tweets), around 10, 000 tweets are randomly sampled for annotations. To avoid issues related to user distribution bias as highlighted by Arango et al. (2019), they restrict a maximum of 120 tweets per user.","10,012 tweets","7,209","2,003",N/A,Yes,"(in fine-grained classification)

All labels and their definitions are summarized as follows:

Abusive/Offensive: Profanity, strongly impolite, rude or vulgar language expressed with fighting or hurtful words in order to insult a targeted individual or group.

Sexism: Language used to express hatred towards a targeted individual or group based on gender or sexual orientation.

Religious Hate: Language used to express hatred towards a targeted individual or group based on their religious beliefs or lack of any religious beliefs and the use of religion to incite violence or propagate hatred against a targeted individuals or group.

Profane: The use of vulgar, foul or obscene language without an intended target.

Normal: This contains text that don't fall into the above categories.",Not discussed,,Specific,"gender, religion, other",Targeted,"gender, religion, race, political",Yes,Yes,"Gender, Religion, other","gender, religion, other",No,,No,,,No,Human
485,No,"(Chang & Danescu-Niculescu-Mizil, 2019)",Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),USA,"Online discussions often derail into toxic exchanges between participants. In this work, the authors: • introduce the first model for forecasting conversational events that can capture the dynamics of a conversation as it develops; • build two diverse datasets (one entirely new, one extending prior work) for the task of forecasting derailment of online conversations; • compare the performance of our model against the current state-of-the-art, and evaluate its ability to provide early warning signs.",Yes,35,03.11.2019,14.12.2022,Yes,Yes,Conversation derailment,Yes,"(post-hoc annotation) whether or not the conversation contains a personal attack whether a conversation eventually had a comment removed by a moderator for violation of Rule 2 ""Don't be rude or hostile to other users""",Yes,Website,,https://convokit.cornell.edu/,Yes,Derailment Datasets: Wikipedia data; Reddit CMV data,2,2,"Wikipedia, Reddit",Conversation Derailment,English,N/A,N/A,N/A,N/A,"Wikipedia data. Zhang et al.'s 'Conversations Gone Awry' dataset consists of 1,270 conversations that took place between Wikipedia editors on publicly accessible talk pages. The conversations are sourced from the WikiConv dataset (Hua et al., 2018) and labeled by crowdworkers as either containing a personal attack from within (i.e., hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. A series of controls are implemented to prevent models from picking up on trivial correlations. To prevent models from capturing topic-specific information (e.g., political conversations are more likely to derail), each attack-containing conversation is paired with a clean conversation from the same talk page, where the talk page serves as a proxy for topic.3 To force models to actually capture conversational dynamics rather than detecting already-existing toxicity, human annotations are used to ensure that all comments preceding a personal attack are civil. To the ends of more effective model training, the authors of this research elected to expand the 'Conversations Gone Awry' dataset, using the original annotation procedure. Since they found that the original data skewed towards shorter conversations, they focused this crowdsourcing run on longer conversations: ones with 4 or more comments preceding the attack. Through this additional crowdsourcing, they expand the dataset to 4,188 conversations. They perform an 80-20-20 train/dev/test split, ensuring that paired conversations end up in the same split in order to preserve the topic control. Finally, they randomly sample another 1 million conversations from WikiCon to use for the unsupervised pre-training of the generative component.

Reddit CMV data. The CMV dataset is constructed from conversations collected via the Reddit API. In contrast to the Wikipedia-based dataset, they explicitly avoid the use of post-hoc annotation. Instead, they use as their label whether a conversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don't be rude or hostile to other users"". Though the lack of post-hoc annotation limits the degree to which they can impose controls on the data (e.g., some conversations may contain toxic comments not flagged by the moderators), they do reproduce as many of the Wikipedia data's controls as they can. Namely, they replicate the topic control pairing by choosing pairs of positive and negative examples that belong to the same top-level post, following Tan et al. (2016)and enforce that the removed comment was made by a user who was previously involved in the conversation. This process results in 6,842 conversations, to which they again apply a pair-preserving 80-20-20 split. Finally, they gather over 600,000 conversations that do not include any removed comment, for unsupervised pre-training.",Not discussed,,manual,Yes,"Wikipedia data: the authors of this research elected to expand the 'Conversations Gone Awry' dataset, using the original annotation procedure. Since they found that the original data skewed towards shorter conversations, they focused this crowdsourcing run on longer conversations: ones with 4 or more comments preceding the attack. Through this additional crowdsourcing, they expand the dataset to 4,188 conversations.

Reddit CMV data: they use as their label whether a conversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don't be rude or hostile to other users"".",Not discussed,,"4,188 conversations (Wikipedia data); 6,842 conversations (Reddit CMV data)",80% (~3350; ~4674),20% (~838; ~1368),N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,Yes,personal attack in a conversation,To a person,No,Human
486,No,"(Ashraf, Zubiaga, & Gelbukh, 2021)",Abusive language detection in youtube comments leveraging replies as conversational context,PeerJ Computer Science,"Mexico, UK","They introduce a new publicly available annotated dataset for abusive language detection in short texts. The dataset includes comments from YouTube, along with contextual information: replies, video, video title, and the original description. The comments in the dataset are labelled as abusive or not and are classified by topic: politics, religion, and other. In particular, they discuss our refined annotation guidelines for such classification. They report a number of strong baselines on this dataset for the tasks of abusive language detection and topic classification, using a number of classifiers and text representations. They show that taking into account the conversational context, namely, replies, greatly improves the classification results as compared with using only linguistic features of the comments. They also study how the classification accuracy depends on the topic of the comment.",Yes,11,08.10.2021,01.01.2023,Yes,Yes,"Abuse. ""Abuse is any form of expression that (1) addresses another person, group or community, (2) is derogatory, sexist, vulgar or profane, and (3) refers to human flaws, intends to offend a person or a group, or implies condescension or victim-blaming.""",Yes,"politics, religion, other",Yes,Github,,https://github.com/Noman712/contextual-abusive-language-detection/tree/main/dataset,Yes,Context-Aware Abusive Language Detection in YouTube Comments (CAALDYC),1,1,YouTube,Abusive Language,English,XX.12.2019-XX.01.2020,XX.XX.2016-XX.XX.2017,N/A,N/A,"First, they manually selected 29 YouTube videos based on topics: politics, religion, and other. These are videos published by popular sources like BBC, CNN, or TV shows like Saturday Night Live and, due to their popularity, have a large number of comments and replies. They collected all the data that are related to these videos such as ID, title, comments, replies of comments, likes, date, and time. Initially, their data collection led to more than 160,000 comments associated with these videos, retrieved through the YouTube data API (https://developers.google.com/youtube/v3; last visited: 28-01-2021). Moreover, we have included all the replies to the comments. The extracted comments and replies of each video were stored in separate CSV files in chronological order. Next, we converted the dataset into a single CSV file containing the columns: video-URL, title, comment, replies, date, and time. Data was grouped and sorted together based on the date and time of the video. Finally, comments that do not have replies were removed for two reasons: (i) they were not providing extra information about comments, (ii) to avoid difficulties in the annotation process because they were not fulfilling our annotation requirements. After removing these comments, we were able to extract 18,794 comments that have replies. As a result, we only selected comments which were in English language.",Not discussed,,manual,Yes,"Since this research focuses on context-aware abusive-language detection, annotators were asked to assign one of the two labels (abusive or non-abusive) by taking into account the additional information of the comments. In addition to being annotated with abusive language detection labels, comments in the dataset are classified into topics: politics, religion, and other. Some of the comments intersect in topics; however, for the sake of simplicity, they ask the annotators to choose the most relevant topic, which is, of course, an oversimplification. ",Yes,Convenience sampling,"2,304",N/A,N/A,"Three annotators (A1, A2, and A3). They all have a good command of the English language and are experienced in social media and NLP research. These annotators are from a computer science background and have a minimum qualification of a Master’s degree; one of the annotators is an author of this paper, and two others are from Chang Gung University, Taoyuan, Taiwan. Two annotators are male, and one is female. They are from the same continent: Asia, and have the same faith: Islam.",Yes,"Abuse is any form of expression that (1) addresses another person, group or community, (2) is derogatory, sexist, vulgar or profane, and (3) refers to human flaws, intends to offend a person or a group, or implies condescension or victim-blaming.",Not discussed,,Specific,"political, religion, other",Targeted,gender,Yes,Yes,"Politics, Religion, Other","political, religion, other",No,,No,,,No,Human
487,No,"(Alraddadi, & Ghembaza, 2021)",Anti-Islamic Arabic Text Categorization using Text Mining and Sentiment Analysis Techniques,International Journal of Advanced Computer Science and Applications,Saudi Arabia,"The aim of this research is to detect and classify websites based on their content if it encourages spreading hate speech toward Islam and Muslims, or Islamophobia using sentiment analysis and web text mining techniques. In this research, a large dataset corpus has been collected, to identify and classify anti-Islamic online content. The target is to automatically detect the content of those websites that are hostile to Islam and transmitting extremist ideas against it. The main purpose is to reduce the spread of those webpages that give the wrong idea about Islam. The proper dataset is collected from different sources, and the two datasets for the Arabic language (balanced and unbalanced) have been produced. The framework of the proposed approach has been described. The approach used in this framework is based on the supervised Machine Learning (ML) approach using Support Vector Machines (SVM) and Multinomial Naive Bayes (MNB) models as classifiers, and Term Frequency-Inverse Document Frequency (TF-IDF) as feature extraction. Different experiments including word level and tri-gram level on the two datasets have been conducted and compared to the obtained results. The experimental results show that the supervised ML approach using word level is the finest approach for both datasets that produce high accuracy with 97% applied on the balanced Arabic dataset using the SVM algorithm with TF-IDF as feature extraction. Finally, an interactive web- application prototype has been developed and built in order to detect and classify toxic language such as anti-Islamic online text- content.",Yes,0,XX.01.2021,27.12.2022,Yes,Yes,"Anti-Islam and anti-Muslims. Anti-Islam and anti-Muslims can be expressed as the hatred toward the Islam and Muslims, especially as a political force that promotes terrorism. In particular, it is a criticism of Islam, its actions, its nature, and what it teaches people (Froio, 2018). 
Islamophobia. Anti-Islam may involve Islamophobia, which is the fear and hatred against the Islamic religion and Muslims.",Yes,Islamophobia,No,,,,,Datasets,2,2,"Yahoo!, Google search","Anti-Islam, Anti-Muslim, Islamophobia",Arabic,28.02.2021 - 15.04.2021,N/A,N/A,N/A,"The data was gathered from the Internet using Yahoo and Google search engines. Furthermore, the MSA text content was gathered, which is the formal Arabic language instead of the informal Arabic dialects. The collection of the Arabic data started from the end of February 2021 until the mid of April 2021.
The main keywords used to collect data in Arabic language هعاداة الإسلام، هناهضة الإسلام، الإساءة للزسىل، كزه الإسلام، هحاربة :were الإسلام، الإنفصالية الإسلاهىية، اضطهاد الوزأة، الشوىلية الإسلاهىية والتطزف الإسلاهىٌ.
These keywords helped us to reduce the amount of search, in order to find the desired content, due to the huge number of articles that talk about Islam in good or in bad ways. Two datasets for the Arabic language were produced.
The collected data is organized into an excel spreadsheet using a web-scraping tool called Octoparse. This tool takes the URL of the webpage to extract data from, then selects the target data to be extracted, and runs the scraping to get the data as CSV, Excel, Application Programming Interface (API), or save them to a database. The extracted data contain the title, the content, the URL, the date, and label them as an anti- Islamic content or not. During the process of collecting data, one challenge was the extraction and the retrieval of the blocked webpages containing extremist ideas or false information about Islam from Saudi Arabia search engines.",Not discussed,,N/A,Not discussed,,,,6142; 8510,5957,2553,N/A,Not discussed,,Not discussed,,Specific,religion,Targeted,religion,Yes,Yes,Islamic,religion,No,,No,,,No,Human
489,No,"(Calabrese et al., 2021)",AAA: Fair Evaluation for Abuse Detection Systems Wanted,13th ACM Web Science Conference 2021,"UK, Italy","This research aims to develop abuse detection systems that can be used to alert and support human moderators of online communities. They introduce Adversarial Attacks against Abuse (AAA), a new evaluation strategy and associated metric that better captures a model’s performance on certain classes of hard-to-classify micro posts, and for example penalises systems which are biased on low-level lexical features. It does so by adversarially modifying the model developer’s training and test data to generate plausible test samples dynamically. They make AAA available as an easy-to-use tool. They show its effectiveness in error analysis by comparing the AAA performance of several state-of-the-art models on multiple datasets. This work will inform the development of detection systems and contribute to the fight against abusive language online.",Yes,5,22.06.2021,26.12.2022,Yes,Yes,"Abuse. ""We use abuse as an umbrella term covering any kind of harmful content on the Web, as this is accepted practice in the field (Waseem, Davidson, Warmsley, and Weber, 2017; Vidgen, Harris, Nguyen, Tromble, Hale, and Margetts, 2019).",No,,Yes,Github,,https://github.com/Ago3/Adversifier,Yes,AAA,2,1,Twitter,Abuse,English,N/A,N/A,N/A,N/A,"They retrieved a public archive of tweets4 and filtered out all the unique English tweets that did not contain a pair of quotation marks, obtaining around 925,000 in- stances; then, we processed tweets by removing all quoted material and substituting it with a special &lt;mask; token.",Not discussed,,manual,Yes,"Three annotators each processed 3,000 of the top 9,000 most relevant tweets. Annotators were asked to decide whether it was clear that the candidate template did not express agreement with the quoted text, for all possible messages that could be embedded inside of it – if there was a reasonable counterexample the tweet had to be discarded. Approved tweets were lightly edited to remove sensitive or too specific information, e.g. URLs and user handles, together with vulgar or derogatory terms5. In the second stage, each of the annotators validated the accepted candidate templates produced by the other two annotators. They were able to either discard or slightly revise them. In the third and final stage a fourth annotator validated the templates, marking each item as either valid or invalid.",Yes,convenience sampling,9000,N/A,N/A,Not discussed,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,Yes,Human
493,No,"(Köffer, Riehle, Höhenberger, & Becker, 2018)",Discussing the Value of Automatic Hate Speech Detection in Online Debates,Multikonferenz Wirtschaftsinformatik 2018,Germany,This study discusses the potential value of automatic analytics of German texts to detect hate speech. They build upon previous research for English texts and demonstrate the transferability to German. The paper discusses the results with respect to the potential for media organizations and considerations about moderation techniques and algorithmic transparency.,Yes,38,09.03.2018,05.01.2023,Yes,Yes,"Hate speech. ""In Germany, the amount of abusive content on the Internet during the refugee crisis has sparked a national debate on how to deal with online hate speech.""",No,,No,,,,,Dataset,1,1,"Alles Schall und Rauch, Cicero, Compact, Contra Magazin, Epoch Times, Focus, Freie Welt, Junge Freiheit, NEOPresse, Rheinische Post, Tagesspiegel, Welt, Zeit",Hateful Comments,German,N/A,N/A,N/A,Refugee crisis in Germany in 2015/16,"They collected user-generated comments that were publicly available on news platforms on the Internet. The extraction of data included mainstream journalistic news websites as well as websites of so-called alternative media. They used web scraping technologies to collect the comments. The dataset comprises 13 platforms. In total, 376,143 comments and 21,740 articles have been collected.",Not discussed,,manual,Yes,"They collected ratings via an online survey. They used a binary categorization, so that study participants rated comments as “hate” or “no hate”. In addition, study participants could also decide to skip a comment if they were unsure whether it contained hate or not.",Yes,Study participants rated randomly selected comments on the project website. The selection of comments ensured that we got a similar amount of labeled data for each platform.,11973,N/A,N/A,"Out of the 247 participants, 169 did provide their age and gender; the remaining 78 users submitted neither age nor gender. 65.0% male and 35% female, 33% under 25 (24.7% male, 8.2% female), 41.4% aged between 25-30 (28.6% male, 12.7% female), 12.9% aged between 30-35 (5.2% male, 7.7% female), and 12.7% aged over 35 (6.5% male, 6.3% female).",No,,Not discussed,,Specific,other,Targeted,"nationality, political",Yes,No,,,Yes,Refugee,No,,,No,Human
494,No,"(Alatawi, Alhothali, & Moria, 2021)",Detecting White Supremacist Hate Speech Using Domain Specific Word Embedding With Deep Learning and BERT,IEEE Access,Saudi Arabia,"This research investigates the feasibility of automatically detecting white supremacist hate speech on Twitter using deep learning and natural language processing techniques. Two deep learning models are investigated in this research. The first approach utilizes a bidirectional Long Short-Term Memory (BiLSTM) model along with domain-specific word embeddings extracted from white supremacist corpus to capture the semantic of white supremacist slangs and coded words. The second approach utilizes one of the most recent language models, which is Bidirectional Encoder Representations from Transformers (BERT). The BiLSTM model achieved 0.75 F1-score and BERT reached a 0.80 F1-score. Both models are tested on a balanced dataset combined from Twitter and a Stormfront dataset compiled from white supremacist forum.",Yes,36,26.07.2021,05.01.2023,Yes,Yes,"Hate speech. ""Hate speech is any expression that encourages, promotes, or justifies violence, hatred, or dis- crimination against a person or group of individuals based on characteristics such as color, gender, race, sexual orienta- tion, nationality, religion, or other attributes (Weber, 2009).""",No,,Yes,Github,,https://github.com/Hind-Saleh-Alatawi/WhiteSupremacistDataset,Yes,"TWITTER WHITE SUPREMACY DATASET
",2,1,"Twitter, Stormfront",White Supremacist Hate Speech,English,N/A,N/A,North America,N/A,"To create the domain-specific word embedding, they first collected a corpus consisting of 1,041,576 tweets. The tweets were obtained from known white supremacist hashtags such as #white_privilege, and #it_is_ok_to_be_white. They also collected content from accounts that identified themselves as white supremacists explicitly (e.g., Whit***er) or implicitly (e.g., Na***st), and/or shared supportive phrases for white supremacy in hashtags or tweets encouraging or promoting racial or religious hatred against others.",No,,manual,Yes,"The annotation procedure initially consisted of four labels: explicit white supremacy (EWS), implicit white supremacy (IWS), other hate speech (O), and neutral(N).",Yes,Random sampling,"1,999","1,200",400,Three judges,Yes,"Explicit white supremacy content refers to hate speech/tweets that express either racial or religious hatred towards others or claims of being undermined by other racial or religious groups (e.g., ‘‘These people do not want solution, They only want you dead White man’’. Implicit white supremacy content refers to expressing racial or religious hatred either indirectly or implicitly without using explicit hate terms (e.g., ‘‘we own our diversity, leave our country’’). Other hate speech is any hateful text other than white supremacist hate text, such as misogyny (i.e., hatred of women), homophobia (i.e., hatred of LGBT people), or sexism (i.e., discrimination based on gender). Neutral text, on the other hand, is any content that expresses positive subjective content (e.g., ‘‘Always brother’’), factual text (e.g., weather situation), or any other content not intended to promote or encourage hatred. Neu- tral also includes textual information that is challenging and cannot be annotated as hate speech or non-hate speech due to ambiguous intentions or contexts, e.g., ‘‘to be against immigration does not mean to kill people’’ or ‘‘die for them’’. Also, any factual text that includes hate terms with no hate intent is considered as Neutral (e.g., ‘‘Christchurch mosque shooter to be sentenced on August 24’’).
",Not discussed,,Specific,"race, religion",Targeted,"gender, race, sexuality, nationality, religion, other",Yes,Yes,"Race, Religion","race, religion",No,,No,,,No,Human
495,No,"(Sajid, Hassan, Ali, & Gillani, 2020)",Roman Urdu Multi-Class Offensive Text Detection using Hybrid Features and SVM,2020 IEEE 23rd International Multitopic Conference (INMIC),Pakistan,"They propose a new technique that automatically detects Roman Urdu comments from YouTube videos in five classes. These classes, including, Religious Hate, Violence Promotion, Extremist (Racist), Threat/Fear, and Neutral. They have generated a dataset by scrapping Roman Urdu comments from YouTube videos and labeled by the language experts. They have considered N-grams and TF-IDF values for feature extraction followed by SVM classification. Some classes have relatively fewer instances, and they employed SMOTE for class-balancing.",Yes,5,20.01.2020,05.01.2023,Yes,Yes,"Hate speech. ""According to UN hate speech is: any kind of communication in speech, writing or behavior that attacks or uses abusive or discriminatory language which refers to a group or single person based on religion, ethnicity, nationality, race, color, gender or other identity factors (Guterres, 2019).""",Yes,"Religious Hate, Violence Promotion, Extremist (Racist), Threat/Fear, Neutral",No,,,,,Dataset,1,1,YouTube,Hate Speech,Urdu,N/A,N/A,N/A,Not discussed,"For this research, they obtained a new Roman Urdu dataset for hate speech detection from YouTube comments. The first step is to crawl YouTube comments data; for this purpose, they developed a YouTube comments crawler. They query different keywords and find videos that have comments containing offensive in Roman Urdu. They also query many offensive Roman Urdu words on YouTube to find a video that has comments related to offensive keywords. They scrape about 16806 comments data from YouTube.",No,,manual,No,,No,,16300,12225,4075,Three annotators,Yes,an essential guide,Not discussed,,Specific,"religion, race",Targeted,"religion, race, nationality, gender, other ",Yes,Yes,"Religion, Race","religion, race",No,,No,,,No,Human
496,No,"(Ayo, Folorunso, Ibharalu, & Osinuga, 2020)",Hate speech detection in Twitter using hybrid embeddings and improved cuckoo search-based neural networks,International Journal of Intelligent Computing and Cybernetics,Nigeria,"Design/methodology/approach – This study proposes a hybrid embedding enhanced with a topic inference method and an improved cuckoo search neural network for hate speech detection in Twitter data. The proposed method uses a hybrid embedding technique that includes Term Frequency-Inverse Document Frequency (TF-IDF) for word-level feature extraction and Long Short Term Memory (LSTM) which is a variant of recurrent neural networks architecture for sentence-level feature extraction. The extracted features from the hybrid embeddings then serve as input into the improved cuckoo search neural network for the prediction of a tweet as hate speech, offensive language or neither.
Findings – The proposed method showed better results when tested on the collected Twitter datasets compared to other related methods. In order to validate the performances of the proposed method, t-test and post hoc multiple comparisons were used to compare the significance and means of the proposed method with other related methods for hate speech detection. Furthermore, Paired Sample t-Test was also conducted to validate the performances of the proposed method with other related methods.",Yes,11,03.11.2020,06.01.2023,Yes,Yes,"Hate speech. ""It’s a kind of abusive language that shows a clear malicious intention of being spiteful, causing harm, or inciting hatred (Nockleby, 2000; Foster, 2020).""",Yes,"Hate speech, Offensive language, neither",No,,,,,Dataset,1,1,Twitter,Hate Speech,English,N/A,N/A,N/A,Not discussed,"The Twitter Streaming API (Davidson et al., 2017) was used to randomly collect the tweets used in the evaluation through the BirdDog access level. These tweets are made up of a random sample of 25,000 English Twitter’s public live stream extracted through the BirdDog API and containing 3,639 terms from the hate speech lexicon compiled by Hatebase.org. ",No,,manual,Yes,"Crowdsource of users was used to manually label the dataset into hate speech, offensive language and neither.",Yes,"Random sampling 25,000 English Twitter’s public live stream extracted through the BirdDog API ",25000,20000,5000,Not discussed,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
497,No,"(Alshaalan & Al-Khalifa, 2020)",Hate Speech Detection in Saudi Twittersphere: A Deep Learning Approach,Proceedings of the Fifth Arabic Natural Language Processing Workshop,Saudi Arabia,"This paper aims to investigate several neural network models based on Convolutional Neural Network (CNN) and Recurrent Neural Networks (RNN) to detect hate speech in Arabic tweets. It also evaluates the recent language representation model BERT on the task of Arabic hate speech detection. To conduct the experiments, they firstly built a new hate speech dataset that contains 9,316 annotated tweets. Then, they conducted a set of experiments on two datasets to evaluate four models: CNN, GRU, CNN+GRU and BERT. The experimental results on the dataset and an out-domain dataset show that the CNN model gives the best performance with an F1-score of 0.79 and AUROC of 0.89.",Yes,10,12.12.2020,05.01.2023,Yes,Yes,"Hate speech. ""While there is no formal definition of hate speech, there is a general agreement among scholars and service providers to define it as any language that attack a person or a group based on some characteristic such as race, color, ethnicity, gender, religion, or other characteristic (Schmidt & Wiegand, 2017).""",Yes,"racist, religious, ideological",Yes,Github,,https://github.com/raghadsh/Arabic-Hate-speech,Yes,General hate speech dataset (GHSD),2,1,Twitter,Hate Speech,Arabic,XX.03.2018-XX.08.2018,N/A,Saudi Twittersphere,N/A,"They used the standard Twitter search/streaming API with tweepy python library to collect data. The data was collected in a span of 6 months, from March 2018 to August 2018 using a keyword-based and thread-based search. In the keyword-based approach, they included in the search query a total of 164 impartial unique terms (excluding the various morphological variations of the same term) that refer to groups, or to people belonging to groups that are likely to be targeted by hate speech. For the thread-based search, they included in the search query different hashtag that discusses controversial topics which are considered to be strong indicators of hate speech. They monitored Twitter trends during the period of the data collection and ended up with a total of 10 hashtags used for data retrieval.",No,,manual,Yes,"To obtain the annotations for the dataset, Figure Eight crowdsourcing platform was utilized to recruit both crowed and internal workers (volunteers and freelancers). Before submitting the task to the annotators, they generated an annotation guideline for hate speech annotation with the help of experts in interreligious dialogue, Islamic jurisprudence and media studies. The guideline is available online on GitHub4. The entire annotation process was completed in a course of three weeks, starting from 1 to 23 September 2019. The dataset was submitted to the annotators in different batches. The first batch (1,000 tweets) was annotated by crowed workers. The second batch (4,000 tweets) was annotated by 15 different Saudi annotators. The third and final batch (5,000 tweets) was annotated by three freelancers who are familiar with Saudi dialect.",Not discussed,,"8,964","6,425","2,539","Crowed and internal workers (volunteers and freelancers), including 15 different Saudi annotators and three freelancers who are familiar with Saudi dialect.",Yes,The guideline is available online on GitHub (https://github.com/raghadsh/Arabic-Hate-speech/blob/master/Annotation%20Guidlines.pdf).,Not discussed,,Specific,"race, religion, political",Targeted,"race, gender, religion, other",Yes,Yes,"Race, Religion, Ideology","race, religion, political",No,,No,,,No,Human
498,No,"(Qian, ElSherief, Belding, & Wang, 2019)",Learning to Decipher Hate Symbols,arXiv preprint ,USA,"In this paper, they propose a novel task of deciphering hate symbols. To do this, we leverage the Urban Dictionary and collected a new, symbol-rich Twitter corpus of hate speech. They investigate neural network latent context models for deciphering hate symbols. More specifically, they study Sequence-to-Sequence models and show how they are able to crack the cyphers based on context. Furthermore, they propose a novel Variational Decipher and show how it can generalize better to unseen hate symbols in a more challenging testing setting.",Yes,9,04.04.2019,06.01.2023,Yes,Yes,Hate speech.,No,,No,,,,,UD,1,1,"Urban Dictionary, Twitter",Hate Speech,English,01.01.2011-31.12.2017,N/A,N/A,N/A,"They first collect hate symbols and the correspond- ing definitions from the Urban Dictionary. Each term with one of the following hashtags: #hate, #racism, #racist, #sexism, #sexist, #nazi is selected as a candidate and added to the set S0. They collected a total of 1,590 terms. Next, they expand this set by different surface forms using the Urban Dictionary API. For each term si in set S0, they obtain a set of terms Ri that have the same meaning as si but with different surface forms.
For each of the hate symbols, they collect all tweets from 2011-01-01 to 2017-12-31 that contain exactly the same surface form of hate symbol in the text. Since they only focus on hate speech, they train an SVM (Cortes and Vapnik, 1995) classifier to filter the collected tweets. ",No,,N/A,Not discussed,,,,18667,16227,2440,N/A,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,No,No,,,No,,No,,,No,Human
499,No,"(Mathur, Shah, Sawhney, & Mahata, 2018)",Detecting Offensive Tweets in Hindi-English Code-Switched Language,Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media,"India, USA","The paper focuses on the classification of offensive tweets written in Hinglish language, which is a portmanteau of the Indic language Hindi with the Roman script. The paper introduces a novel tweet dataset, titled Hindi-English Offen- sive Tweet (HEOT) dataset, consisting of tweets in Hindi-English code switched language split into three classes: non- offensive, abusive and hate-speech. They approach the problem of classification of the tweets in HEOT dataset using transfer learning wherein the proposed model employing Convolutional Neural Networks is pre-trained on tweets in English followed by retraining on Hinglish tweets.",Yes,114,20.07.2018,01.01.2023,Yes,Yes,"Offensive speech. ""Offensive text can be broadly classified as abusive and hate speech on the basis of the context and target of the offence.""",Yes,"Hate speech, abusive speech",No,,,,,Hindi-English Offen- sive Tweet (HEOT),2,1,Twitter,Offensive Speech,Hindi-English,XX.11.2017-XX.12.2017,N/A,India,N/A,"Dataset HEOT was created using the Twitter Streaming API by selecting tweets in Hindi-English code switched language by data mining specific profane words in Hinglish language. The tweets were collected during the months of November-December 2017 and were crowd-sourced to ten NLP researchers for annotation and verification. The data repository thus created consists of 3679 tweets out of which the count of non-offensive, abusive and hate-inducing tweets is 1414, 1942 and 323 respectively and categorized similarly to the previous dataset.",Not discussed,,manual,Not discussed,,Not discussed,,3679,14188,4000,Ten NLP researchers,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
500,No,"(Park, et al., 2021)",Detecting community sensitive norm violations in online conversations,arXiv preprint ,USA,"They introduce a new dataset focusing on a more complete spectrum of community norms and their violations in the local conversational and global community contexts. They introduce a series of models that use this data to develop context- and community-sensitive norm violation detection, showing that these changes give high performance.",Yes,2,09.10.2021,01.01.2023,Yes,Yes,"Norm violations. ""…violating a variety of community norms beyond the traditional hate speech and incivility, such as spamming or violating community format/topics.""",Yes,"general incivility, trolling, harassment, hate speech, spam",Yes,Github,,https://github.com/chan0park/NormVio,Yes,NORMVIO Dataset,1,1,Reddit,Norm Violations,English,N/A,N/A,N/A,N/A,"They collected our initial data via the Reddit API, which provides a list of moderators and their comments for each subreddit. For each of the top 100K most popular subreddits, they identified the most recent 500 comments from each moderator and retrieved comments that moderators posted in response to a removed comment (henceforth, moderation comments).
Moderation comments often provide useful signals for inferring which community norm was violated. From the full set of moderation comments, they selected those that contain a phrase explicitly stating the rule number (e.g. “this comment violates Rule 2”) or the exact text of one of its subreddit’s rules (e.g. “don’t be rude”).
They then fetch the entire conversation thread for this set of moderation comments: the original post and all parent comments prior to the moderator’s comment. They also fetched the norm-violating comment that was removed by moderators, by searching archived comments via the Pushshift API (Baumgartner et al., 2020).5
The final dataset is comprised of 20K conversations that have the last comment removed by one of the moderators of the community. Following the approach of Chang and Danescu-Niculescu-Mizil (2019), they include 32K paired unmoderated conversations as a control set. Each moderated conversation is matched with up to two unmoderated conversations from the same post and with most",Yes,"They anonymize individual usernames and personal identifiers of posters and moderators. Finally, along with their data release, they provide guidelines to the users who wish to delete their comments from the Pushshift dump.","automated, manual",Not discussed,,Not discussed,,"52,012","41,667","5,131",Not discussed,Not discussed,,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
502,No,"(Waseem & Hovy, 2016)",Hateful symbols or hateful people? predictive features for hate speech detection on twitter,Proceedings of the NAACL student research workshop,Denmark,They provide a list of criteria founded in critical race theory and use them to annotate a publicly available corpus of more than 16k tweets. They analyze the impact of various extra-linguistic features in conjunction with character n-grams for hate-speech detection. They also present a dictionary based on the most indicative words in our data.,Yes,1371,17.06.2016,02.02.2023,Yes,Yes,"A tweet is offensive if it: 1. uses a sexist or racial slur. 2. attacks a minority. 3. seeks to silence a minority. 4. criticizes a minority (without a well-founded argument). 5. promotes, but does not directly use, hate speech or violent crime. 6. criticizes a minority and uses a straw man argument. 7. blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims. 8. shows support of problematic hashtags. E.g. “#BanIslam”, “#whoriental”, “#whitegenocide”. 9. negatively stereotypes a minority. 10. defends xenophobia or sexism. 11. contains a screen name that is offensive, as per the previous criteria, the tweet is ambiguous (at best), and the tweet is on a topic that satisfies any of the above criteria.",Yes,"Sexist, racist",Yes,Github,,http://github.com/zeerakw/hatespeech,Yes,Corpus,1,1,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"They bootstrapped the corpus collection, by performing an initial manual search of common slurs and terms used pertaining to religious, sexual, gender, and ethnic minorities. In the results, they identified frequently occurring terms in tweets that contain hate speech and references to specific entities, such as the term “#MKR”, the hashtag for the Australian TV show My Kitchen Rules, which often prompts sexist tweets directed at the female participants2. In addition, they identified a small number of prolific users from these searches.
Based on this sample, they used the public Twitter search API to collect the entire corpus, filtering for tweets not written in English.",Not discussed,,manual,Yes,"A tweet is offensive if it
1. uses a sexist or racial slur.
2. attacks a minority.
3. seeks to silence a minority.
4. criticizes a minority (without a well-founded argument).
5. promotes, but does not directly use, hate speech or violent crime.
6. criticizes a minority and uses a straw man argument.
7. blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims.
8. shows support of problematic hashtags. E.g. “#BanIslam”, “#whoriental”, “#whitegenocide”
9. negatively stereotypes a minority.
10. defends xenophobia or sexism.
11. contains a screen name that is offensive, as per the previous criteria, the tweet is ambiguous (at best), and the tweet is on a topic that satisfies any of the above criteria.",,,"16,914",N/A,N/A,The authors and an outside annotator (a 25-year-old woman studying gender studies and a non- activist feminist).,Not discussed,,Not discussed,,Specific,"gender, race",Targeted,"race, gender, nationality, religion",Yes,Yes,"Sexism, Racism","gender, race",No,,No,,,No,Human
503,No,"(Davidson et al., 2017)",Automated Hate Speech Detection and the Problem of Offensive Language.,Proceedings of the international AAAI conference on web and social media ,"USA, Qatar","They used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. They use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. They train a multi-class classifier to distinguish between these different categories. A close analysis of the predictions and the errors shows when they can reliably separate hate speech from other offensive language and when this differentiation is more difficult. They find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.
",Yes,1962,XX.05.2017,02.02.2023,Yes,Yes," Hate speech. ""We define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.""",No,,Yes,Github,,https://github.com/t-davidson/ hate- speech- and- offensive- language,Yes,Hatebase lexicon,2,1,Twitter,Hate Speech,English,N/A,N/A,N/A,N/A,"They begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API they searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. They extracted the time-line for each user, resulting in a set of 85.4 million tweets. ",Not discussed,,manual,Yes,"From this corpus they then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. They use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.
",Yes,random sampling,"24,802",N/A,N/A,CrowdFlower (CF) workers,Yes,They were provided with our definition along with a paragraph explaining it in further detail.,Not discussed,,General,N/A,Non-targeted,N/A,Yes,No,,,No,,No,,,No,Human
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,